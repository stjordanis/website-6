{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Full Stack Deep Learning Our mission is to help you go from a promising ML experiment to a shipped product, with real-world impact. Current Course We are teaching a major update of the course Spring 2021 as an official UC Berkeley course and as an online course, with all lectures and labs available for free. \ud83d\ude80Spring 2021 Online Course\ud83d\ude80 About this course There are many great courses to learn how to train deep neural networks. However, training the model is just one part of shipping a deep learning project. This course teaches full-stack production deep learning: Formulating the problem and estimating project cost Finding, cleaning, labeling, and augmenting data Picking the right framework and compute infrastructure Troubleshooting training and ensuring reproducibility Deploying the model at scale Who is this for The course is aimed at people who already know the basics of deep learning and want to understand the rest of the process of creating production deep learning systems. You will get the most out of this course if you have: At least one-year experience programming in Python. At least one deep learning course (at a university or online). Experience with code versioning, Unix environments, and software engineering. While we cover the basics of deep learning (backpropagation, convolutional neural networks, recurrent neural networks, transformers, etc), we expect these lectures to be mostly review. Instructors Sergey Karayev is Head of STEM AI at Turnitin. He co-founded Gradescope after getting a PhD in Computer Vision at UC Berkeley. Josh Tobin is Founder and CEO of a stealth startup. He worked as a Research Scientist at OpenAI after getting a PhD in Robotics at UC Berkeley. Pieter Abbeel is Professor at UC Berkeley. He co-founded Covariant.ai, Berkeley Open Arms, and Gradescope. Our March 2019 Bootcamp Course Offerings \ud83d\ude80Spring 2021\ud83d\ude80 Online Course Spring 2021 CSE 194-080 - UC Berkeley Undergraduate course Spring 2020 CSEP 590C - University of Washington Professional Master's Program course \u2728Fall 2019\u2728 nicely formatted videos and notes from our weekend bootcamp March 2019 raw videos from our bootcamp August 2018 bootcamp","title":"Home"},{"location":"#current-course","text":"We are teaching a major update of the course Spring 2021 as an official UC Berkeley course and as an online course, with all lectures and labs available for free. \ud83d\ude80Spring 2021 Online Course\ud83d\ude80","title":"Current Course"},{"location":"#about-this-course","text":"There are many great courses to learn how to train deep neural networks. However, training the model is just one part of shipping a deep learning project. This course teaches full-stack production deep learning: Formulating the problem and estimating project cost Finding, cleaning, labeling, and augmenting data Picking the right framework and compute infrastructure Troubleshooting training and ensuring reproducibility Deploying the model at scale","title":"About this course"},{"location":"#who-is-this-for","text":"The course is aimed at people who already know the basics of deep learning and want to understand the rest of the process of creating production deep learning systems. You will get the most out of this course if you have: At least one-year experience programming in Python. At least one deep learning course (at a university or online). Experience with code versioning, Unix environments, and software engineering. While we cover the basics of deep learning (backpropagation, convolutional neural networks, recurrent neural networks, transformers, etc), we expect these lectures to be mostly review.","title":"Who is this for"},{"location":"#instructors","text":"Sergey Karayev is Head of STEM AI at Turnitin. He co-founded Gradescope after getting a PhD in Computer Vision at UC Berkeley. Josh Tobin is Founder and CEO of a stealth startup. He worked as a Research Scientist at OpenAI after getting a PhD in Robotics at UC Berkeley. Pieter Abbeel is Professor at UC Berkeley. He co-founded Covariant.ai, Berkeley Open Arms, and Gradescope. Our March 2019 Bootcamp","title":"Instructors"},{"location":"#course-offerings","text":"\ud83d\ude80Spring 2021\ud83d\ude80 Online Course Spring 2021 CSE 194-080 - UC Berkeley Undergraduate course Spring 2020 CSEP 590C - University of Washington Professional Master's Program course \u2728Fall 2019\u2728 nicely formatted videos and notes from our weekend bootcamp March 2019 raw videos from our bootcamp August 2018 bootcamp","title":"Course Offerings"},{"location":"spring2021/","text":"Full Stack Deep Learning - Spring 2021 We've updated and improved our materials and can't wait to share them with you! Every Monday, we will post videos of our lectures and lab sessions. You can follow along on our Twitter or YouTube , or sign up via email below. Synchronous Online Course We also offered a paid option for those who wanted weekly assignments, capstone project, Slack discussion, and certificate of completion. This synchronous option is now full, but you can enter your email above to be the first to hear about future offerings. Those who are enrolled, please see details . Week 1: Fundamentals The week of February 1, we do a blitz review of the fundamentals of deep learning, and introduce the codebase we will be working on in labs for the remainder of the class. Lecture 1: DL Fundamentals Notebook: Coding a neural net from scratch Lab 1: Setup and Intro Reading: How the backpropagation algorithm works Week 2: CNNs The week of February 8, we cover CNNs and Computer Vision Applications, and introduce a CNN in lab. Lecture 2A: CNNs Lecture 2B: Computer Vision Applications Lab 2: CNNs Reading: A brief introduction to Neural Style Transfer Improving the way neural networks learn Week 3: RNNs The week of February 15, we cover RNNs and applications in Natural Language Processing, and start doing sequence processing in lab. Lecture 3: RNNs Lab 3: RNNs Reading: The Unreasonable Effectiveness of Recurrent Neural Networks Attention Craving RNNS: Building Up To Transformer Networks Week 4: Transformers The week of February 22, we talk about the successes of transfer learning and the Transformer architecture, and start using it in lab. Lecture 4: Transfer Learning and Transformers Lab 4: Transformers Reading: Transformers from Scratch Week 5: ML Projects The week of March 1, our synchronous online course begins with the first \"Full Stack\" lecture: Setting up ML Projects. Lecture 5: Setting up ML Projects (\ud83d\udc48 with detailed notes) Reading: Rules of Machine Learning ML Yearning (and subscribe to Andrew Ng's newsletter ) Those in the syncronous online course will have their first weekly assignment: Assignment 1 , available on Gradescope. Week 6: Infra & Tooling The week of March 7, we tour the landscape of infrastructure and tooling for deep learning. Lecture 6: Infrastructure & Tooling (\ud83d\udc48 with detailed notes) Reading: Machine Learning: The High-Interest Credit Card of Technical Debt Those in the syncronous online course will have to work on Assignment 2 . Week 7: Troubleshooting The week of March 14, we talk about how to best troubleshoot training. In lab, we learn to manage experiments. Lecture 7: Troubleshooting DNNs (\ud83d\udc48 with detailed notes) Lab 5: Experiment Management Reading: Why is machine learning hard? Those in the syncronous online course will have to work on Assignment 3 . Week 8: Data The week of March 21, we talk about Data Management, and label some data in lab. Lecture 8: Data Management (\ud83d\udc48 with detailed notes) Lab 6: Data Labeling Reading: Emerging architectures for modern data infrastructure Those in the syncronous online course will have to work on Assignment 4 . Week 9: Ethics The week of March 28, we discuss ethical considerations. In lab, we move from lines to paragraphs. Lecture 9: AI Ethics (\ud83d\udc48 with detailed notes) Lab 7: Paragraph Recognition Those in the synchronous online course will have to submit their project proposals . Week 10: Testing The week of April 5, we talk about Testing and Explainability, and set up Continuous Integration in lab. Lecture 10: Testing & Explainability (\ud83d\udc48 with detailed notes) Lab 8: Testing & CI Those in the synchronous online course will work on their projects. Week 11: Deployment The week of April 12, we cover Deployment and Monitoring, and deploy our model to AWS Lambda in lab. Lecture 11: Deployment & Monitoring (\ud83d\udc48 with detailed notes) Lab 9: Web Deployment Those in the synchronous online course will work on their projects. Week 12: Research The week of April 19, we talk research, and set up robust monitoring for our model. Lecture 12: Research Directions (\ud83d\udc48 with detailed notes) Lab 10: Monitoring Those in the synchronous online course will work on their projects. \u2728Week 13: Teams\u2728 The week of April 26, we discuss ML roles and team structures, as well as big companies vs startups. Lecture 13: ML Teams & Startups (\ud83d\udc48 with detailed notes) Panel Discussion: Do I need a PhD to work in ML? Week 14-16: Projects Those in the synchronous online course will submit 5-minute videos of their projects and associated write-ups by May 15. The week of May 17, we will award the best projects and host them on this site. Other Resources Fast.ai is a great free two-course sequence aimed at first getting hackers to train state-of-the-art models as quickly as possible, and only afterward delving into how things work under the hood. Highly recommended for anyone. Dive Into Deep Learning is a great free textbook with Jupyter notebooks for every part of deep learning. NYU\u2019s Deep Learning course has excellent PyTorch breakdowns of everything important going on in deep learning. Stanford\u2019s ML Systems Design course has lectures that parallel those in this course. The Batch by Andrew Ng is a great weekly update on progress in the deep learning world. /r/MachineLearning/ is the best community for staying up to date with the latest developments.","title":"Spring 2021 Schedule"},{"location":"spring2021/#full-stack-deep-learning-spring-2021","text":"We've updated and improved our materials and can't wait to share them with you! Every Monday, we will post videos of our lectures and lab sessions. You can follow along on our Twitter or YouTube , or sign up via email below. Synchronous Online Course We also offered a paid option for those who wanted weekly assignments, capstone project, Slack discussion, and certificate of completion. This synchronous option is now full, but you can enter your email above to be the first to hear about future offerings. Those who are enrolled, please see details .","title":"Full Stack Deep Learning - Spring 2021"},{"location":"spring2021/#week-1-fundamentals","text":"The week of February 1, we do a blitz review of the fundamentals of deep learning, and introduce the codebase we will be working on in labs for the remainder of the class. Lecture 1: DL Fundamentals Notebook: Coding a neural net from scratch Lab 1: Setup and Intro Reading: How the backpropagation algorithm works","title":"Week 1: Fundamentals"},{"location":"spring2021/#week-2-cnns","text":"The week of February 8, we cover CNNs and Computer Vision Applications, and introduce a CNN in lab. Lecture 2A: CNNs Lecture 2B: Computer Vision Applications Lab 2: CNNs Reading: A brief introduction to Neural Style Transfer Improving the way neural networks learn","title":"Week 2: CNNs"},{"location":"spring2021/#week-3-rnns","text":"The week of February 15, we cover RNNs and applications in Natural Language Processing, and start doing sequence processing in lab. Lecture 3: RNNs Lab 3: RNNs Reading: The Unreasonable Effectiveness of Recurrent Neural Networks Attention Craving RNNS: Building Up To Transformer Networks","title":"Week 3: RNNs"},{"location":"spring2021/#week-4-transformers","text":"The week of February 22, we talk about the successes of transfer learning and the Transformer architecture, and start using it in lab. Lecture 4: Transfer Learning and Transformers Lab 4: Transformers Reading: Transformers from Scratch","title":"Week 4: Transformers"},{"location":"spring2021/#week-5-ml-projects","text":"The week of March 1, our synchronous online course begins with the first \"Full Stack\" lecture: Setting up ML Projects. Lecture 5: Setting up ML Projects (\ud83d\udc48 with detailed notes) Reading: Rules of Machine Learning ML Yearning (and subscribe to Andrew Ng's newsletter ) Those in the syncronous online course will have their first weekly assignment: Assignment 1 , available on Gradescope.","title":"Week 5: ML Projects"},{"location":"spring2021/#week-6-infra-tooling","text":"The week of March 7, we tour the landscape of infrastructure and tooling for deep learning. Lecture 6: Infrastructure & Tooling (\ud83d\udc48 with detailed notes) Reading: Machine Learning: The High-Interest Credit Card of Technical Debt Those in the syncronous online course will have to work on Assignment 2 .","title":"Week 6: Infra &amp; Tooling"},{"location":"spring2021/#week-7-troubleshooting","text":"The week of March 14, we talk about how to best troubleshoot training. In lab, we learn to manage experiments. Lecture 7: Troubleshooting DNNs (\ud83d\udc48 with detailed notes) Lab 5: Experiment Management Reading: Why is machine learning hard? Those in the syncronous online course will have to work on Assignment 3 .","title":"Week 7: Troubleshooting"},{"location":"spring2021/#week-8-data","text":"The week of March 21, we talk about Data Management, and label some data in lab. Lecture 8: Data Management (\ud83d\udc48 with detailed notes) Lab 6: Data Labeling Reading: Emerging architectures for modern data infrastructure Those in the syncronous online course will have to work on Assignment 4 .","title":"Week 8: Data"},{"location":"spring2021/#week-9-ethics","text":"The week of March 28, we discuss ethical considerations. In lab, we move from lines to paragraphs. Lecture 9: AI Ethics (\ud83d\udc48 with detailed notes) Lab 7: Paragraph Recognition Those in the synchronous online course will have to submit their project proposals .","title":"Week 9: Ethics"},{"location":"spring2021/#week-10-testing","text":"The week of April 5, we talk about Testing and Explainability, and set up Continuous Integration in lab. Lecture 10: Testing & Explainability (\ud83d\udc48 with detailed notes) Lab 8: Testing & CI Those in the synchronous online course will work on their projects.","title":"Week 10: Testing"},{"location":"spring2021/#week-11-deployment","text":"The week of April 12, we cover Deployment and Monitoring, and deploy our model to AWS Lambda in lab. Lecture 11: Deployment & Monitoring (\ud83d\udc48 with detailed notes) Lab 9: Web Deployment Those in the synchronous online course will work on their projects.","title":"Week 11: Deployment"},{"location":"spring2021/#week-12-research","text":"The week of April 19, we talk research, and set up robust monitoring for our model. Lecture 12: Research Directions (\ud83d\udc48 with detailed notes) Lab 10: Monitoring Those in the synchronous online course will work on their projects.","title":"Week 12: Research"},{"location":"spring2021/#week-13-teams","text":"The week of April 26, we discuss ML roles and team structures, as well as big companies vs startups. Lecture 13: ML Teams & Startups (\ud83d\udc48 with detailed notes) Panel Discussion: Do I need a PhD to work in ML?","title":"\u2728Week 13: Teams\u2728"},{"location":"spring2021/#week-14-16-projects","text":"Those in the synchronous online course will submit 5-minute videos of their projects and associated write-ups by May 15. The week of May 17, we will award the best projects and host them on this site.","title":"Week 14-16: Projects"},{"location":"spring2021/#other-resources","text":"Fast.ai is a great free two-course sequence aimed at first getting hackers to train state-of-the-art models as quickly as possible, and only afterward delving into how things work under the hood. Highly recommended for anyone. Dive Into Deep Learning is a great free textbook with Jupyter notebooks for every part of deep learning. NYU\u2019s Deep Learning course has excellent PyTorch breakdowns of everything important going on in deep learning. Stanford\u2019s ML Systems Design course has lectures that parallel those in this course. The Batch by Andrew Ng is a great weekly update on progress in the deep learning world. /r/MachineLearning/ is the best community for staying up to date with the latest developments.","title":"Other Resources"},{"location":"spring2021/lab-1/","text":"Lab 1: Setup and Introduction Video In this video, we introduce the lab throughout the course. We formulate the problem, provide the codebase structure, and train a simple Multilayer Perceptron on the MNIST dataset. 4:11 - Understand the problem and path to solution 5:54 - Set up the computing environment 12:54 - Review the codebase 24:55 - Train the MLP model on MNIST Slides PDF Download Follow Along GitHub Readme","title":"Lab 1: Setup and Introduction"},{"location":"spring2021/lab-1/#lab-1-setup-and-introduction","text":"","title":"Lab 1: Setup and Introduction"},{"location":"spring2021/lab-1/#video","text":"In this video, we introduce the lab throughout the course. We formulate the problem, provide the codebase structure, and train a simple Multilayer Perceptron on the MNIST dataset. 4:11 - Understand the problem and path to solution 5:54 - Set up the computing environment 12:54 - Review the codebase 24:55 - Train the MLP model on MNIST","title":"Video"},{"location":"spring2021/lab-1/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lab-1/#follow-along","text":"GitHub Readme","title":"Follow Along"},{"location":"spring2021/lab-10/","text":"\u2728Lab 10: Monitoring\u2728 Video No slides Follow along Readme Notes TODO","title":"\u2728Lab 10: Monitoring\u2728"},{"location":"spring2021/lab-10/#lab-10-monitoring","text":"","title":"\u2728Lab 10: Monitoring\u2728"},{"location":"spring2021/lab-10/#video","text":"No slides","title":"Video"},{"location":"spring2021/lab-10/#follow-along","text":"Readme","title":"Follow along"},{"location":"spring2021/lab-10/#notes","text":"TODO","title":"Notes"},{"location":"spring2021/lab-2/","text":"Lab 2: CNNs and Synthetic Data Video No slides. Follow Along GitHub Readme In this lab, you train a single-line ConvNet predictor on the EMNIST dataset and then synthetically generate your own data. 00:00 - Introduction 05:23 - Look at the EMNIST dataset 09:52 - Train a base ConvNet model 12:43 - Examine the ConvNet code 17:33 - Lab 2 homework 19:35 - Make a synthetic dataset of EMNIST lines","title":"Lab 2: CNNs and Synthetic Data"},{"location":"spring2021/lab-2/#lab-2-cnns-and-synthetic-data","text":"","title":"Lab 2: CNNs and Synthetic Data"},{"location":"spring2021/lab-2/#video","text":"No slides.","title":"Video"},{"location":"spring2021/lab-2/#follow-along","text":"GitHub Readme In this lab, you train a single-line ConvNet predictor on the EMNIST dataset and then synthetically generate your own data. 00:00 - Introduction 05:23 - Look at the EMNIST dataset 09:52 - Train a base ConvNet model 12:43 - Examine the ConvNet code 17:33 - Lab 2 homework 19:35 - Make a synthetic dataset of EMNIST lines","title":"Follow Along"},{"location":"spring2021/lab-3/","text":"Lab 3: RNNs Video No slides Follow along Readme Notes 00:00 - Introduction. 01:59 - Introduce LineCNNSimple, a model that can read multiple characters in an image. 15:52 - Make this model more efficient with LineCNN, which uses a fully convolutional network. 18:18 - Upgrade the model further into LitModelCTC, which uses a CTC (Connectionist Temporal Classification) loss. 23:29 - Finalize your model, LineCNNLSTM, by adding an LSTM layer on top of CNN. 27:34 - Lab 3 homework.","title":"Lab 3: RNNs"},{"location":"spring2021/lab-3/#lab-3-rnns","text":"","title":"Lab 3: RNNs"},{"location":"spring2021/lab-3/#video","text":"No slides","title":"Video"},{"location":"spring2021/lab-3/#follow-along","text":"Readme","title":"Follow along"},{"location":"spring2021/lab-3/#notes","text":"00:00 - Introduction. 01:59 - Introduce LineCNNSimple, a model that can read multiple characters in an image. 15:52 - Make this model more efficient with LineCNN, which uses a fully convolutional network. 18:18 - Upgrade the model further into LitModelCTC, which uses a CTC (Connectionist Temporal Classification) loss. 23:29 - Finalize your model, LineCNNLSTM, by adding an LSTM layer on top of CNN. 27:34 - Lab 3 homework.","title":"Notes"},{"location":"spring2021/lab-4/","text":"Lab 4: Transformers Video No slides Follow along Readme Notes In this lab, you use the LineCNN + LSTM model with CTC loss from lab 3 as an \"encoder\" of the image, and then send it through Transformer decoder layers. 00:00 - Introduction 01:43 - LineCNNTransformer class 04:50 - TransformerLitModel 06:51 - Code to make predictions 08:50 - Training guidelines","title":"Lab 4: Transformers"},{"location":"spring2021/lab-4/#lab-4-transformers","text":"","title":"Lab 4: Transformers"},{"location":"spring2021/lab-4/#video","text":"No slides","title":"Video"},{"location":"spring2021/lab-4/#follow-along","text":"Readme","title":"Follow along"},{"location":"spring2021/lab-4/#notes","text":"In this lab, you use the LineCNN + LSTM model with CTC loss from lab 3 as an \"encoder\" of the image, and then send it through Transformer decoder layers. 00:00 - Introduction 01:43 - LineCNNTransformer class 04:50 - TransformerLitModel 06:51 - Code to make predictions 08:50 - Training guidelines","title":"Notes"},{"location":"spring2021/lab-5/","text":"Lab 5: Experiment Management Video No slides Follow along Readme Notes In this lab, we'll use Weights and Biases to manage experiments for our handwriting recognition model. 00:00 - Introduction 00:56 - IAMLines Dataset 05:29 - Make EMNISTLines more like IAMLines 09:57 - Set up Weights and Biases 13:42 - Run experiments on Weights and Biases 22:58 - Configure W&B sweeps to search for hyper-parameters","title":"Lab 5: Experiment Management"},{"location":"spring2021/lab-5/#lab-5-experiment-management","text":"","title":"Lab 5: Experiment Management"},{"location":"spring2021/lab-5/#video","text":"No slides","title":"Video"},{"location":"spring2021/lab-5/#follow-along","text":"Readme","title":"Follow along"},{"location":"spring2021/lab-5/#notes","text":"In this lab, we'll use Weights and Biases to manage experiments for our handwriting recognition model. 00:00 - Introduction 00:56 - IAMLines Dataset 05:29 - Make EMNISTLines more like IAMLines 09:57 - Set up Weights and Biases 13:42 - Run experiments on Weights and Biases 22:58 - Configure W&B sweeps to search for hyper-parameters","title":"Notes"},{"location":"spring2021/lab-6/","text":"Lab 6: Data Labeling Video Follow along Readme","title":"Lab 6: Data Labeling"},{"location":"spring2021/lab-6/#lab-6-data-labeling","text":"","title":"Lab 6: Data Labeling"},{"location":"spring2021/lab-6/#video","text":"","title":"Video"},{"location":"spring2021/lab-6/#follow-along","text":"Readme","title":"Follow along"},{"location":"spring2021/lab-7/","text":"Lab 7: Paragraph Recognition Video Follow along Readme","title":"Lab 7: Paragraph Recognition"},{"location":"spring2021/lab-7/#lab-7-paragraph-recognition","text":"","title":"Lab 7: Paragraph Recognition"},{"location":"spring2021/lab-7/#video","text":"","title":"Video"},{"location":"spring2021/lab-7/#follow-along","text":"Readme","title":"Follow along"},{"location":"spring2021/lab-8/","text":"Lab 8: Testing & CI Video Follow along Readme","title":"Lab 8: Testing & CI"},{"location":"spring2021/lab-8/#lab-8-testing-ci","text":"","title":"Lab 8: Testing &amp; CI"},{"location":"spring2021/lab-8/#video","text":"","title":"Video"},{"location":"spring2021/lab-8/#follow-along","text":"Readme","title":"Follow along"},{"location":"spring2021/lab-9/","text":"\u2728Lab 9: Web Deployment\u2728 Video Follow along Readme","title":"\u2728Lab 9: Web Deployment\u2728"},{"location":"spring2021/lab-9/#lab-9-web-deployment","text":"","title":"\u2728Lab 9: Web Deployment\u2728"},{"location":"spring2021/lab-9/#video","text":"","title":"Video"},{"location":"spring2021/lab-9/#follow-along","text":"Readme","title":"Follow along"},{"location":"spring2021/lecture-1/","text":"Lecture 1: DL Fundamentals Video Slides PDF Download Notes In this video, we discuss the fundamentals of deep learning. We will cover artificial neural networks, the universal approximation theorem, three major types of learning problems, the empirical risk minimization problem, the idea behind gradient descent, the practice of back-propagation, the core neural architectures, and the rise of GPUs. This should be a review for most of you; if not, then briefly go through this online book -neuralnetworksanddeeplearning.com. 1:25\u200b - Neural Networks 6:48\u200b - Universality 8:48\u200b - Learning Problems 16:17\u200b - Empirical Risk Minimization / Loss Functions 19:55\u200b - Gradient Descent 23:57\u200b - Backpropagation / Automatic Differentiation 26:09\u200b - Architectural Considerations 29:01\u200b - CUDA / Cores of Compute","title":"Lecture 1: DL Fundamentals"},{"location":"spring2021/lecture-1/#lecture-1-dl-fundamentals","text":"","title":"Lecture 1: DL Fundamentals"},{"location":"spring2021/lecture-1/#video","text":"","title":"Video"},{"location":"spring2021/lecture-1/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lecture-1/#notes","text":"In this video, we discuss the fundamentals of deep learning. We will cover artificial neural networks, the universal approximation theorem, three major types of learning problems, the empirical risk minimization problem, the idea behind gradient descent, the practice of back-propagation, the core neural architectures, and the rise of GPUs. This should be a review for most of you; if not, then briefly go through this online book -neuralnetworksanddeeplearning.com. 1:25\u200b - Neural Networks 6:48\u200b - Universality 8:48\u200b - Learning Problems 16:17\u200b - Empirical Risk Minimization / Loss Functions 19:55\u200b - Gradient Descent 23:57\u200b - Backpropagation / Automatic Differentiation 26:09\u200b - Architectural Considerations 29:01\u200b - CUDA / Cores of Compute","title":"Notes"},{"location":"spring2021/lecture-10/","text":"Lecture 10: Testing & Explainability Video Slides Download slides as PDF Notes Notes were taken by James Le and Vishnu Rachakonda Download notes as PDF 1 - What\u2019s Wrong With Black-Box Predictions? What does it mean when we have a good test set performance? If the test data and production data come from the same distribution , then in expectation , the performance of your model on your evaluation metrics will be the same. Let\u2019s unpack the bolded assumptions: In the real world, the production distribution does not always match the offline distribution . You could have data drift, data shift, or even malicious users trying to attack your model. Expected performance does not tell the whole story . For instance, if you are working on long-tail data distribution, then the sample of data that you use to evaluate the model offline might not tell you much about the tail of that distribution - meaning that your test set score can be misleading. On top of that, if you evaluate your model with a single metric across your entire dataset, that does not mean your model is actually performing well against all the slices of data that might be important. The performance of your model is not equal to the performance of your machine learning system . There are other things (that can go wrong with the ML system) that do not have anything to do with the model. Finally, the test set performance only tells you about the metrics that you are evaluating . In the real world, you are probably not optimizing the exact metrics you care about deep down. How bad is this problem? This is a quote from a former ML engineer at an autonomous vehicle company: \u201c I think the single biggest thing holding back the autonomous vehicle industry today is that, even if we had a car that worked, no one would know, because no one is confident that they know how to evaluate it properly. \u201d We believe that there is a similar sentiment to lesser degrees in other fields of machine learning, where the evaluation is the biggest bottleneck . The goal of this lecture is to introduce concepts and methods to help you, your team, and your users: Understand at a deeper level how well your model is performing. Become more confident in your model\u2019s ability to perform well in production. Understand the model\u2019s performance envelope (where you should expect it to perform well and where not). 2 - Software Testing Types of Tests There are three basic types of software tests: Unit tests that test the functionality of a single piece of code (an assertion on a single function or a single class) in isolation. Integration tests that test how two or more units perform when used together (e.g., test if a model works well with a pre-processing function). End-to-end tests that test how the entire software system performs when all units are put together (e.g., test on realistic inputs from a real user). Testing is a broad field, so you will likely encounter various other kinds of tests as well. Best Practices Here are a couple of \u201cuncontroversial\u201d testing best practices: Automate your tests: You have tests that run by themselves (typically via a CI/CD system) without a user committing an action. There should be no ambiguity on whether your system performs up to standard on the tests that are being run. Make sure your tests are reliable, run fast, and go through the same code review process as the rest of your code: The number of tests grows in proportion to the size of your codebase. If your tests are unreliable, then people will start ignoring them. If your tests are slow, then you won\u2019t want to run them frequently during development. If your tests do not undergo the code review process, they will have bugs, and it\u2019s better not to have them at all. Enforce that tests must pass before merging into the main branch: This is a good norm for teams with more than one person. This is a forcing function to make sure that everyone is committed to writing good tests and can also be helpful for regulatory concerns. When you find new production bugs, convert them to tests: This ensures that someone does not accidentally reintroduce those bugs in the future. Follow the testing pyramid: Introduced by Google , it says that you should write a lot more unit tests than integration tests and a lot more integration tests than end-to-end tests. Compared to end-to-end tests, unit tests are faster, more reliable, and better at isolating failures. The rule of thumb that Google recommends (as a rough split) is 70% unit tests, 20% integration tests, and 10% end-to-end tests. Next up, let\u2019s discuss a few \u201ccontroversial\u201d testing best practices: Solitary tests : The distinction between a solitary test and a sociable test is that - solitary testing does not rely on real data from other units, while sociable testing makes the implicit assumption that other modules are working. Test coverage : You get a test coverage score for your codebase, which tells you what percentage of lines of code in your codebase is called by at least one test. Test coverage gives you a single metric that quantifies the quality of your testing suite. However, test coverage does not measure the right things (in particular, test quality). Test-driven development : In principle, you want to create your tests before you write your code. These tests serve as the specification of how the code functions. There are not many people who religiously stick to this methodology of development, but TDD is a valuable tool nonetheless. Testing In Production The traditional view is that the goal of testing is to prevent shipping bugs into production. Therefore, by definition, you must do your testing offline before your system goes into production. However, there are two caveats: Informal surveys reveal that the percentage of bugs found by automated tests is surprisingly low. On top of that, modern service-oriented distributed systems (which are deployed in most software engineering organizations nowadays) are particularly hard to test. The interactions between the components can get tricky. Here is our philosophy for testing in production: Bugs are inevitable, so you might as well set up the system so that users can help you find them. There are a few strategies to test in production: Canary deployment : Do not roll out the new software version to all the users right away. Instead, just roll it out to a small percentage of your users and separately monitor that group\u2019s behavior. A/B testing: You can run a more principled statistical test if you have particular metrics that you care about: one for the old version of the code that is currently running and another for the new version that you are trying to test. Real user monitoring: Rather than looking at aggregate metrics (i.e., click-through rate), try to follow the journey that an actual user takes through your application and build a sense of how users experience the changes. Exploratory testing: Testing in production is not something that you want to automate fully. It should involve a bit of exploration (individual users or granular metrics). Continuous Integration and Continuous Delivery CI/CD platforms automate the tests that you run by hooking into your code repo. When you trigger some actions to take place (pushing new code, merging new code into a branch, submitting a pull request), CI/CD platforms kick off a job that is responsible for packaging your code, running all your tests, producing a report that tells you how well your code performs on your tests, and gatekeeping whether your new code can make it to the next stage. Tactically, you can define these jobs as commands in a Docker container and store the results for later review. SaaS solutions for continuous integration include CircleCI and Travis CI . Most of them do not have GPUs available. If you are just getting started, the default recommendation is GitHub Actions , which is super easy to integrate. Jenkins and Buildkite are manual options for running continuous integration on your own hardware, in the cloud, or something in between. There is a lot more flexibility about the types of jobs you can run through the systems (meaning you can use your GPUs). The tradeoff is that they are harder to set up. 3 - Testing Machine Learning Systems There are several core differences between traditional software systems and ML systems that add complexity to testing ML systems: Software consists of only code, but ML combines code and data. Software is written by humans to solve a problem, while ML is compiled by optimizers to satisfy a proxy metric. Software is prone to loud failures, while ML is prone to silent failures. Software tends to be relatively static (in principle), while ML is constantly changing. Due to such differences, here are common mistakes that teams make while testing ML systems: Think the ML system is just a model and only test that model. Not test the data. Not build a granular enough understanding of the performance of the model before deploying it. Not measure the relationship between model performance metrics and business metrics. Rely too much on automated testing. Think offline testing is enough, and therefore, not monitor or test in production. Above is the diagram of how you can think of your entire production ML system that straddles across the offline and online environments: Sitting in the middle is your ML model - an artifact created by your training process, which takes in an input and produces an output. The training system takes code and data as inputs and produces the trained model as the output. The prediction system takes in and pre-processes the raw data, loads the trained ML model, loads the model weights, calls model.predict() on the data, post-processes the outputs, and returns the predictions. Once you deploy your prediction system to the online environment, the serving system takes in requests from users, scales up and down to meet the traffic demands, and produces predictions back to those users. The whole ML system closes the loop by collecting production data (both the predictions that the model produces and additional feedback from users, business metrics, or labelers) and sending them back to the offline environment. The labeling system takes the raw data seen in production, helps you get inputs from labelers, and provides labels for that data. The storage and pre-processing system stores and pre-processes the labeled data before passing it back to the training system. One way to think about how to test ML systems the right way is to think about the tests that you can run for each system component and across the border of these components. Infrastructure Tests Infrastructure tests are unit tests for your training system . They help you avoid bugs in the training pipeline. You can unit test your training code like any other code. Another common practice is to add single-batch or single-epoch tests that check performance after an abbreviated training run on a tiny dataset, which catches obvious regressions to your training code. Tactically, you should run infrastructure tests frequently during the development process. Training Tests Training tests are integration tests between your data system and your training system. They make sure that training jobs are reproducible. You can pull a fixed dataset and run a full or abbreviated training run on it. Then, you want to check and ensure that the model performance on the newly trained model remains consistent with the reference performance. Another option is to pull a sliding window of data (maybe a new window for every few days) and run training tests on that window. Tactically, you should run training tests periodically, ideally nightly for frequently changing codebase. Functionality Tests Functionality tests are unit tests for your prediction system . They help you avoid regressions in code that makes up your prediction infrastructure. You can unit test your prediction code like any other code. Specifically for the ML system, you can load a pre-trained model and test its predictions on a few key examples . Tactically, you should run functionality tests frequently during the development process. Evaluation Tests Evaluation tests are integration tests between your training system and your prediction system. They make sure that a newly trained model is ready to go into production. These make up the bulk of what\u2019s unique about testing ML systems. At a high level, you want to evaluate your model on all of the metrics , datasets , and slices that you care about. Then, you want to compare the new model to the old and baseline models. Finally, you want to understand the performance envelope of the new model. Operationally, you should run evaluation tests whenever you have a new candidate model considered for production. It is important to note that evaluation tests are more than just the validation score. They look at all the metrics that you care about : Model metrics : precision, recall, accuracy, L2, etc. Behavioral metrics : The goal of behavioral tests is to ensure the model has the invariances we expect. There are three types of behavioral tests: (1) invariance tests to assert that the change in inputs shouldn\u2019t affect outputs, (2) directional tests to assert that the change in inputs should affect outputs, and (3) minimum functionality tests to ensure that certain inputs and outputs should always produce a given result. Behavioral testing metrics are primarily used in NLP applications and proposed in the Beyond Accuracy paper by Ribeiro et al. (2020) . Robustness metrics : The goal of robustness tests is to understand the model\u2019s performance envelope (i.e., where you should expect the model to fail). You can examine feature importance, sensitivity to staleness, sensitivity to data drift, and correlation between model performance and business metrics. In general, robustness tests are still under-rated. Privacy and fairness metrics : The goal of privacy and fairness tests is to distinguish whether your model might be biased against specific classes. Helpful resources are Google\u2019s Fairness Indicators and the Fairness Definitions Explained paper by Verma and Rubin (2018) . Simulation metrics : The goal of simulation tests is to understand how the model performance could affect the rest of the system. These are useful when your model affects the real world (for systems such as autonomous vehicles, robotics, recommendation systems, etc.). Simulation tests are hard to do well because they require a model of how the world works and a dataset of different scenarios. Instead of simply evaluating the aforementioned metrics on your entire dataset in aggregate, you should also evaluate these metrics on multiple slices of data . A slice is a mapping of your data to a specific category. A natural question that arises is how to pick those slices. Tools like What-If and SliceFinder help surface the slices where the model performance might be of particular interest. Finally, evaluation tests help you maintain evaluation datasets for all of the distinct data distributions you need to measure. Your main validation or test set should mirror your production distribution as closely as possible as a matter of principle. When should you add new evaluation datasets? When you collect datasets to specify specific edge cases. When you run your production model on multiple data modalities. When you augment your training set with data not found in production (synthetic data). The report produced by the evaluation system entails the metrics broken down against each of the data slices. How can you decide whether the evaluation passes or fails? At a high level, you want to compare the new model to the previous model and another fixed older model . Tactically, you can (1) set thresholds on the differences between the new and the old models for most metrics, (2) set thresholds on the differences between data slices, and (3) set thresholds against the fixed older model to prevent slower performance \u201cleaks.\u201d Shadow Tests Shadow tests are integration tests between your prediction system and your serving system . They help you catch production bugs before those bugs meet users. In many settings, models (which are built in frameworks such as sklearn, Pytorch, TensorFlow, etc.) are developed in isolation from the existing software system. For example, a model to flag inappropriate tweets may be developed in TensorFlow on a static set of data, not directly in the streaming environment of the broader software architecture. Because the prediction system and the serving system are developed in different settings with different assumptions and environments, there are many opportunities for bugs to creep in. These bugs can be tricky to catch prior to integration, so shadow tests can help identify them beforehand. Firstly, shadow tests help you detect bugs in the production deployment . In the code path you're using to build the production model, maybe there's some bug there. You want to make sure that you catch that before users see that bug. Secondly, shadow tests also help you detect inconsistencies between the offline model and the online model . There\u2019s a translation step in the training pipeline in many companies - going from the offline trained model to the online production model (the model itself, the preprocessing pipeline, etc.). A common bug source in production ML systems happens because of the inconsistencies cropping up in that translation step. A good health check ensures that your actual production model is producing the exact predictions on a fixed set of data as the model you have running on your laptop. Thirdly, shadow tests help you detect issues that don't appear on the data you have offline but appear on production data . How do we design shadow tests? These can require a significant amount of infrastructure, as they are dependent on actual model integration opportunities being available. Typical shadow tests involve testing the performance of a candidate model on real data without returning or acting on the output . For example, a company may integrate and run a new model alongside the previous model without returning the output to the user. Analyzing the consistency of the predictions between the two models can help spot important differences before they impact production performance. Another option is to gather production data, save it offline, and test the model\u2019s performance on the fresh data offline . Overall, evaluating the distribution of model predictions in offline vs. online settings, candidate vs. production, or any similar setting of a model update before deploying a new model can help you avoid bugs. A/B Tests Shadow tests evaluate the prediction performance of a model as part of the broader software architecture, but not the impact on users . A/B tests fill this role . A/B tests are a common practice in software engineering, especially in web systems. A/B testing is defined as \u201ca randomized experimentation process wherein two or more versions of a variable (web page, page element, etc.) are shown to different segments of website visitors at the same time to determine which version leaves the maximum impact and drive business metrics.\u201d[1] In model evaluation, A/B tests determine the impact of different model predictions on user and business metrics. One common way of A/B testing models is to \u201ccanary\u201d data or return predictions on a small portion of the data (i.e., 1% or 10%) to the relevant users. The remaining data acts as a control and functions under existing system behavior (i.e., an old model or even no model). Evaluating the difference in metrics between the two groups can determine the relative impact of your model. This simple baseline can work well. Adding more statistically principled splits , which is common in A/B testing, can be a good idea. Labeling Tests Machine learning models operate in a GIGO paradigm: garbage in, garbage out. To prevent poor quality labels from cropping up and corrupting the model, you need to unit test the labeling systems and procedures. You should start by training, certifying, and evaluating individual labelers, who each play a crucial role in the quality of the labels. A simple and common label quality test is to spot check labels as they come in by opening up 100 or 1000 labels from a batch and evaluating them yourself to understand their quality. Using a performant model\u2019s guidance, you can make this process more efficient and only look at labels where the model and the labeler disagree. Another test can be to aggregate labels of multiple labels and measure agreement across labels . The higher the agreement, the better quality the labels are. Using metrics of agreement, you can assign \u201ctrust scores\u201d to labelers based on their performance relative to other labelers and weigh the labels accordingly. Expectation Tests Expectation tests address the data preprocessing and storage system . Essentially, they are unit tests for your data. They are designed to catch data quality issues and bad data before they make their way into the pipeline. The typical way that expectation tests operate is rule- or threshold-based . At each step of the data processing pipeline, the output should conform to a specific format that matches a rule or specific format. If the rule or threshold does not pass, then that stage of the expectation test and the data pipeline\u2019s related step fails. Such tests are frequently run with batch data pipeline jobs. Great Expectations is an open-source library gaining popularity for running expectation tests. The library allows you to set hard rules for the kinds of values or behaviors (i.e., statistics) you expect from your data. How do you set the rules and thresholds for expectation tests? Most expectation tests are set manually. A more sophisticated option is to profile a high-quality sample of your data and set thresholds accordingly. In practice, to avoid false alarms from overly sensitive tests, a combination of both approaches is needed. Challenges and Recommendations Operationalizing ML Tests Running tests is an excellent idea in theory but can pose many practical challenges for data science and ML teams. The first challenge is often organizational . In contrast to software engineering teams for whom testing is table stakes, data science teams often struggle to implement testing and code review norms. The second challenge is infrastructural. Most CI/CD platforms don\u2019t support GPUs, data integrations, or other required elements of testing ML systems effectively or efficiently. The third challenge is tooling , which has not yet been standardized for operations like comparing model performance and slicing datasets. Finally, decision-making for ML test performance is hard. What is \u201cgood enough\u201d test performance is often highly contextual, which is a challenge that varies across ML systems and teams. Let\u2019s boil all these lessons for testing down into a clear set of recommendations specific to ML systems: Test each part of the ML system, not just the model. You build the machine that builds the model, not just the model! Test code, data, and model performance, not just code. Testing model performance is an art, not a science. There is a considerable amount of intuition that guides testing ML systems. Thus, the fundamental goal of testing model performance is to build a granular understanding of how well your model performs and where you don\u2019t expect it to perform well. Using this intuition derived from testing, you can make better decisions about productionizing your model effectively. Build up to this gradually! You don\u2019t need to do everything detailed in this lecture, and certainly not all at once. Start with: Infrastructure tests Evaluation tests Expectation tests 4 - Explainable and Interpretable AI Definitions What do explainable and interpretable AI, buzzwords you\u2019ve undoubtedly heard before, actually mean? Let\u2019s start by outlining some more fundamental terms about the problem space: Domain predictability : the degree to which it is possible to detect data outside the model\u2019s domain of competence. Interpretability : the degree to which a human can consistently predict the model\u2019s result ( Kim et al., 2016 ). Explainability : the degree to which a human can understand the cause of a decision ( Miller, 2017 ). We\u2019ll walk through four different methods of making models interpretable and explainable: Use an interpretable family of models. Distill the complex model to an interpretable one. Understand the contribution of features to the prediction. Understand the contribution of training data points to the prediction. Use An Interpretable Family of Models Examples of interpretable families of models are simple, familiar models like linear regression, logistic regression, generalized linear models, and decision trees . If you understand the math of these models, it\u2019s pretty easy to understand why a model made the decision it did. Because of the reasonably elementary math, these models are interpretable and explainable. However, they are not very powerful. Another class of models that are interpretable is attention models . Examining where a model is \u201clooking\u201d helps us anticipate a model\u2019s prediction, thus making them interpretable. However, attention maps are not particularly explainable. They do not produce complete explanations for a model\u2019s output, just a directional explanation. Furthermore, attention maps are not reliable explanations. Attention maps tell us only where a model is looking, not why it is looking there. Frequently, models focus exclusively on an image\u2019s salient region without an underlying reasoning that relates to the task at hand. In the sample below, the attention model is \u201clooking\u201d at the salient region for classification, which has a very different meaning in each context. The conflation of attention with explanation is a critical pitfall to avoid. Distill A Complex To An Interpretable One Instead of restricting models to only interpretable families, we can fit a more complex model and interpret its decision using another model from an interpretable family. The trick is to train this additional model, referred to as a surrogate model , on the raw data and the complex model\u2019s predictions. The surrogate model\u2019s corresponding interpretation can be used as a proxy for understanding the complex model. This technique is quite simple and fairly general to apply. In practice, however, two concerns manifest. If the surrogate itself performs well on the predictions, why not try to directly apply it rather than the more complex model? If it doesn\u2019t perform well, how do we know that it genuinely represents the complex model\u2019s behavior? Another category of surrogate models is local surrogate models (LIME) . Rather than apply the surrogate model in a global context on all the data, LIME models focus on a single point to generate an explanation for. A perturbation is applied to the point, resulting in a local neighborhood of perturbed data points. On top of these perturbed data points, a surrogate model is trained to map the points to the original predictions from the complex model. If the surrogate model classifies similarly to the complex model, the surrogate can be considered a proxy for interpretation purposes. This method is used widely, as it works for all data types (including images and text). However, defining the right perturbations and ensuring the stability of the explanations is challenging. Understand The Contribution of Features To The Prediction Better understanding each feature\u2019s role in making a prediction is another option for interpretable and explainable ML. Data visualization is one such option, with plots like partial dependence plots and individual conditional expectation plots. A numerical method is permutation feature importance , which selects a feature, randomizes its order in the dataset, and sees how that affects performance. While this method is very easy and widely used, it doesn\u2019t work for high-dimensional data or cases where there is feature interdependence. A more principled approach to explaining the contribution of individual features is SHAP ( Shapley Additive Explanations ). At a high level, SHAP scores test how much changes in a single feature impact the output of a classifier when controlling for the values of the other features. This is a reliable method to apply, as it works on a variety of data and is mathematically principled. However, it can be tricky to implement and doesn\u2019t provide explanations. Gradient-based saliency maps are a popular method for explanations and interpretations. This intuitive method selects an input, performs a forward pass, computes the gradient with respect to the pixels, and visualizes the gradients. Essentially, how much does a unit change in the value of the input\u2019s pixels affect the prediction of the model? This is a straightforward and common method. Similar to the challenge with attention, the explanations may not be correct, and the overall method is fragile and sensitive to small changes. Understand The Contribution of Training Data Points To The Prediction Instead of focusing on features and their explicit relevance to the prediction, we can also take a hard look at the training data points themselves. Prototypes and criticisms are one such approach, though it is less applicable to deep learning. In this method, prototypes are clusters of data that explain much of the variance in the model. Criticisms are data points not explained by the prototypes. Another approach is to look specifically at \u201c influential instances \u201d or data points that cause major changes in the model\u2019s predictions when removed from the data set. Do You Need \"Explainability\"? A good question to ask yourself whether or not \u201cexplainable AI\u201d is a real need for your applications. There are a couple of cases where this question can be useful: Regulators demand it. In this case, there\u2019s not much you can do besides produce some kind of explainable model. However, it can be helpful to ask for clarification on what explainability is judged as. Users demand it. In some cases, users themselves may want trust or explainability in the system. Investigate the necessity for the explainability and trust to come directly from the model itself. Can good product design inspire trust more effectively? For example, allowing doctors to simply override models can reduce the immediate need for explainability. A big associated concern is how often users interact with the model. Infrequent interactions likely require explainable AI, as humans do not get a chance to build their feel for the system. More frequent interactions allow for the simpler objective of interpretability. Deployment demands it. Sometimes, ML stakeholders may demand explainability as a component of ensuring confidence in ML system deployment. In this context, explainability is the wrong objective; domain predictability is the real aim . Rather than full-on explainability, interpretability can be helpful for deployment, especially visualizations for debugging. At present, true explainability for deep learning models is not possible . Current explanation methods are not faithful to the original model performance; it can be easy to cherry-pick specific examples that can overstate explainability. Furthermore, these methods tend to be unreliable and highly sensitive to the input. Finally, as described in the attention section, the full explanation is often not available to modern explainability methods. Because of these reasons, explainability is not practically feasible for deep learning models (as of 2021). Read Cynthia Rudin\u2019s 2019 paper for more detail. Caveats For Explainable and Interpretable AI If you genuinely need to explain your model\u2019s predictions, use an interpretable model family (read more here ). Don\u2019t try to force-fit deep learning explainability methods; they produce cool results but are not reliable enough for production use cases. Specific interpretability methods like LIME and SHAP are instrumental in helping users reach interpretability thresholds faster. Finally, the visualization for interpretability can be pretty useful for debugging. 5 - Resources ML Test Score Paper Behavioral testing paper Jeremy Jordan\u2019s effective testing Robustness Gym Made with ML\u2019s guide to testing Eugene Yan\u2019s practical guide to maintaining machine learning Chip Huyen\u2019s CS329 lecture on evaluating models Interpretable ML Book [1] https://vwo.com/ab-testing-2/","title":"Lecture 10: Testing & Explainability"},{"location":"spring2021/lecture-10/#lecture-10-testing-explainability","text":"","title":"Lecture 10: Testing &amp; Explainability"},{"location":"spring2021/lecture-10/#video","text":"","title":"Video"},{"location":"spring2021/lecture-10/#slides","text":"Download slides as PDF","title":"Slides"},{"location":"spring2021/lecture-10/#notes","text":"Notes were taken by James Le and Vishnu Rachakonda Download notes as PDF","title":"Notes"},{"location":"spring2021/lecture-10/#1-whats-wrong-with-black-box-predictions","text":"What does it mean when we have a good test set performance? If the test data and production data come from the same distribution , then in expectation , the performance of your model on your evaluation metrics will be the same. Let\u2019s unpack the bolded assumptions: In the real world, the production distribution does not always match the offline distribution . You could have data drift, data shift, or even malicious users trying to attack your model. Expected performance does not tell the whole story . For instance, if you are working on long-tail data distribution, then the sample of data that you use to evaluate the model offline might not tell you much about the tail of that distribution - meaning that your test set score can be misleading. On top of that, if you evaluate your model with a single metric across your entire dataset, that does not mean your model is actually performing well against all the slices of data that might be important. The performance of your model is not equal to the performance of your machine learning system . There are other things (that can go wrong with the ML system) that do not have anything to do with the model. Finally, the test set performance only tells you about the metrics that you are evaluating . In the real world, you are probably not optimizing the exact metrics you care about deep down. How bad is this problem? This is a quote from a former ML engineer at an autonomous vehicle company: \u201c I think the single biggest thing holding back the autonomous vehicle industry today is that, even if we had a car that worked, no one would know, because no one is confident that they know how to evaluate it properly. \u201d We believe that there is a similar sentiment to lesser degrees in other fields of machine learning, where the evaluation is the biggest bottleneck . The goal of this lecture is to introduce concepts and methods to help you, your team, and your users: Understand at a deeper level how well your model is performing. Become more confident in your model\u2019s ability to perform well in production. Understand the model\u2019s performance envelope (where you should expect it to perform well and where not).","title":"1 - What\u2019s Wrong With Black-Box Predictions?"},{"location":"spring2021/lecture-10/#2-software-testing","text":"","title":"2 - Software Testing"},{"location":"spring2021/lecture-10/#types-of-tests","text":"There are three basic types of software tests: Unit tests that test the functionality of a single piece of code (an assertion on a single function or a single class) in isolation. Integration tests that test how two or more units perform when used together (e.g., test if a model works well with a pre-processing function). End-to-end tests that test how the entire software system performs when all units are put together (e.g., test on realistic inputs from a real user). Testing is a broad field, so you will likely encounter various other kinds of tests as well.","title":"Types of Tests"},{"location":"spring2021/lecture-10/#best-practices","text":"Here are a couple of \u201cuncontroversial\u201d testing best practices: Automate your tests: You have tests that run by themselves (typically via a CI/CD system) without a user committing an action. There should be no ambiguity on whether your system performs up to standard on the tests that are being run. Make sure your tests are reliable, run fast, and go through the same code review process as the rest of your code: The number of tests grows in proportion to the size of your codebase. If your tests are unreliable, then people will start ignoring them. If your tests are slow, then you won\u2019t want to run them frequently during development. If your tests do not undergo the code review process, they will have bugs, and it\u2019s better not to have them at all. Enforce that tests must pass before merging into the main branch: This is a good norm for teams with more than one person. This is a forcing function to make sure that everyone is committed to writing good tests and can also be helpful for regulatory concerns. When you find new production bugs, convert them to tests: This ensures that someone does not accidentally reintroduce those bugs in the future. Follow the testing pyramid: Introduced by Google , it says that you should write a lot more unit tests than integration tests and a lot more integration tests than end-to-end tests. Compared to end-to-end tests, unit tests are faster, more reliable, and better at isolating failures. The rule of thumb that Google recommends (as a rough split) is 70% unit tests, 20% integration tests, and 10% end-to-end tests. Next up, let\u2019s discuss a few \u201ccontroversial\u201d testing best practices: Solitary tests : The distinction between a solitary test and a sociable test is that - solitary testing does not rely on real data from other units, while sociable testing makes the implicit assumption that other modules are working. Test coverage : You get a test coverage score for your codebase, which tells you what percentage of lines of code in your codebase is called by at least one test. Test coverage gives you a single metric that quantifies the quality of your testing suite. However, test coverage does not measure the right things (in particular, test quality). Test-driven development : In principle, you want to create your tests before you write your code. These tests serve as the specification of how the code functions. There are not many people who religiously stick to this methodology of development, but TDD is a valuable tool nonetheless.","title":"Best Practices"},{"location":"spring2021/lecture-10/#testing-in-production","text":"The traditional view is that the goal of testing is to prevent shipping bugs into production. Therefore, by definition, you must do your testing offline before your system goes into production. However, there are two caveats: Informal surveys reveal that the percentage of bugs found by automated tests is surprisingly low. On top of that, modern service-oriented distributed systems (which are deployed in most software engineering organizations nowadays) are particularly hard to test. The interactions between the components can get tricky. Here is our philosophy for testing in production: Bugs are inevitable, so you might as well set up the system so that users can help you find them. There are a few strategies to test in production: Canary deployment : Do not roll out the new software version to all the users right away. Instead, just roll it out to a small percentage of your users and separately monitor that group\u2019s behavior. A/B testing: You can run a more principled statistical test if you have particular metrics that you care about: one for the old version of the code that is currently running and another for the new version that you are trying to test. Real user monitoring: Rather than looking at aggregate metrics (i.e., click-through rate), try to follow the journey that an actual user takes through your application and build a sense of how users experience the changes. Exploratory testing: Testing in production is not something that you want to automate fully. It should involve a bit of exploration (individual users or granular metrics).","title":"Testing In Production"},{"location":"spring2021/lecture-10/#continuous-integration-and-continuous-delivery","text":"CI/CD platforms automate the tests that you run by hooking into your code repo. When you trigger some actions to take place (pushing new code, merging new code into a branch, submitting a pull request), CI/CD platforms kick off a job that is responsible for packaging your code, running all your tests, producing a report that tells you how well your code performs on your tests, and gatekeeping whether your new code can make it to the next stage. Tactically, you can define these jobs as commands in a Docker container and store the results for later review. SaaS solutions for continuous integration include CircleCI and Travis CI . Most of them do not have GPUs available. If you are just getting started, the default recommendation is GitHub Actions , which is super easy to integrate. Jenkins and Buildkite are manual options for running continuous integration on your own hardware, in the cloud, or something in between. There is a lot more flexibility about the types of jobs you can run through the systems (meaning you can use your GPUs). The tradeoff is that they are harder to set up.","title":"Continuous Integration and Continuous Delivery"},{"location":"spring2021/lecture-10/#3-testing-machine-learning-systems","text":"There are several core differences between traditional software systems and ML systems that add complexity to testing ML systems: Software consists of only code, but ML combines code and data. Software is written by humans to solve a problem, while ML is compiled by optimizers to satisfy a proxy metric. Software is prone to loud failures, while ML is prone to silent failures. Software tends to be relatively static (in principle), while ML is constantly changing. Due to such differences, here are common mistakes that teams make while testing ML systems: Think the ML system is just a model and only test that model. Not test the data. Not build a granular enough understanding of the performance of the model before deploying it. Not measure the relationship between model performance metrics and business metrics. Rely too much on automated testing. Think offline testing is enough, and therefore, not monitor or test in production. Above is the diagram of how you can think of your entire production ML system that straddles across the offline and online environments: Sitting in the middle is your ML model - an artifact created by your training process, which takes in an input and produces an output. The training system takes code and data as inputs and produces the trained model as the output. The prediction system takes in and pre-processes the raw data, loads the trained ML model, loads the model weights, calls model.predict() on the data, post-processes the outputs, and returns the predictions. Once you deploy your prediction system to the online environment, the serving system takes in requests from users, scales up and down to meet the traffic demands, and produces predictions back to those users. The whole ML system closes the loop by collecting production data (both the predictions that the model produces and additional feedback from users, business metrics, or labelers) and sending them back to the offline environment. The labeling system takes the raw data seen in production, helps you get inputs from labelers, and provides labels for that data. The storage and pre-processing system stores and pre-processes the labeled data before passing it back to the training system. One way to think about how to test ML systems the right way is to think about the tests that you can run for each system component and across the border of these components.","title":"3 - Testing Machine Learning Systems"},{"location":"spring2021/lecture-10/#infrastructure-tests","text":"Infrastructure tests are unit tests for your training system . They help you avoid bugs in the training pipeline. You can unit test your training code like any other code. Another common practice is to add single-batch or single-epoch tests that check performance after an abbreviated training run on a tiny dataset, which catches obvious regressions to your training code. Tactically, you should run infrastructure tests frequently during the development process.","title":"Infrastructure Tests"},{"location":"spring2021/lecture-10/#training-tests","text":"Training tests are integration tests between your data system and your training system. They make sure that training jobs are reproducible. You can pull a fixed dataset and run a full or abbreviated training run on it. Then, you want to check and ensure that the model performance on the newly trained model remains consistent with the reference performance. Another option is to pull a sliding window of data (maybe a new window for every few days) and run training tests on that window. Tactically, you should run training tests periodically, ideally nightly for frequently changing codebase.","title":"Training Tests"},{"location":"spring2021/lecture-10/#functionality-tests","text":"Functionality tests are unit tests for your prediction system . They help you avoid regressions in code that makes up your prediction infrastructure. You can unit test your prediction code like any other code. Specifically for the ML system, you can load a pre-trained model and test its predictions on a few key examples . Tactically, you should run functionality tests frequently during the development process.","title":"Functionality Tests"},{"location":"spring2021/lecture-10/#evaluation-tests","text":"Evaluation tests are integration tests between your training system and your prediction system. They make sure that a newly trained model is ready to go into production. These make up the bulk of what\u2019s unique about testing ML systems. At a high level, you want to evaluate your model on all of the metrics , datasets , and slices that you care about. Then, you want to compare the new model to the old and baseline models. Finally, you want to understand the performance envelope of the new model. Operationally, you should run evaluation tests whenever you have a new candidate model considered for production. It is important to note that evaluation tests are more than just the validation score. They look at all the metrics that you care about : Model metrics : precision, recall, accuracy, L2, etc. Behavioral metrics : The goal of behavioral tests is to ensure the model has the invariances we expect. There are three types of behavioral tests: (1) invariance tests to assert that the change in inputs shouldn\u2019t affect outputs, (2) directional tests to assert that the change in inputs should affect outputs, and (3) minimum functionality tests to ensure that certain inputs and outputs should always produce a given result. Behavioral testing metrics are primarily used in NLP applications and proposed in the Beyond Accuracy paper by Ribeiro et al. (2020) . Robustness metrics : The goal of robustness tests is to understand the model\u2019s performance envelope (i.e., where you should expect the model to fail). You can examine feature importance, sensitivity to staleness, sensitivity to data drift, and correlation between model performance and business metrics. In general, robustness tests are still under-rated. Privacy and fairness metrics : The goal of privacy and fairness tests is to distinguish whether your model might be biased against specific classes. Helpful resources are Google\u2019s Fairness Indicators and the Fairness Definitions Explained paper by Verma and Rubin (2018) . Simulation metrics : The goal of simulation tests is to understand how the model performance could affect the rest of the system. These are useful when your model affects the real world (for systems such as autonomous vehicles, robotics, recommendation systems, etc.). Simulation tests are hard to do well because they require a model of how the world works and a dataset of different scenarios. Instead of simply evaluating the aforementioned metrics on your entire dataset in aggregate, you should also evaluate these metrics on multiple slices of data . A slice is a mapping of your data to a specific category. A natural question that arises is how to pick those slices. Tools like What-If and SliceFinder help surface the slices where the model performance might be of particular interest. Finally, evaluation tests help you maintain evaluation datasets for all of the distinct data distributions you need to measure. Your main validation or test set should mirror your production distribution as closely as possible as a matter of principle. When should you add new evaluation datasets? When you collect datasets to specify specific edge cases. When you run your production model on multiple data modalities. When you augment your training set with data not found in production (synthetic data). The report produced by the evaluation system entails the metrics broken down against each of the data slices. How can you decide whether the evaluation passes or fails? At a high level, you want to compare the new model to the previous model and another fixed older model . Tactically, you can (1) set thresholds on the differences between the new and the old models for most metrics, (2) set thresholds on the differences between data slices, and (3) set thresholds against the fixed older model to prevent slower performance \u201cleaks.\u201d","title":"Evaluation Tests"},{"location":"spring2021/lecture-10/#shadow-tests","text":"Shadow tests are integration tests between your prediction system and your serving system . They help you catch production bugs before those bugs meet users. In many settings, models (which are built in frameworks such as sklearn, Pytorch, TensorFlow, etc.) are developed in isolation from the existing software system. For example, a model to flag inappropriate tweets may be developed in TensorFlow on a static set of data, not directly in the streaming environment of the broader software architecture. Because the prediction system and the serving system are developed in different settings with different assumptions and environments, there are many opportunities for bugs to creep in. These bugs can be tricky to catch prior to integration, so shadow tests can help identify them beforehand. Firstly, shadow tests help you detect bugs in the production deployment . In the code path you're using to build the production model, maybe there's some bug there. You want to make sure that you catch that before users see that bug. Secondly, shadow tests also help you detect inconsistencies between the offline model and the online model . There\u2019s a translation step in the training pipeline in many companies - going from the offline trained model to the online production model (the model itself, the preprocessing pipeline, etc.). A common bug source in production ML systems happens because of the inconsistencies cropping up in that translation step. A good health check ensures that your actual production model is producing the exact predictions on a fixed set of data as the model you have running on your laptop. Thirdly, shadow tests help you detect issues that don't appear on the data you have offline but appear on production data . How do we design shadow tests? These can require a significant amount of infrastructure, as they are dependent on actual model integration opportunities being available. Typical shadow tests involve testing the performance of a candidate model on real data without returning or acting on the output . For example, a company may integrate and run a new model alongside the previous model without returning the output to the user. Analyzing the consistency of the predictions between the two models can help spot important differences before they impact production performance. Another option is to gather production data, save it offline, and test the model\u2019s performance on the fresh data offline . Overall, evaluating the distribution of model predictions in offline vs. online settings, candidate vs. production, or any similar setting of a model update before deploying a new model can help you avoid bugs.","title":"Shadow Tests"},{"location":"spring2021/lecture-10/#ab-tests","text":"Shadow tests evaluate the prediction performance of a model as part of the broader software architecture, but not the impact on users . A/B tests fill this role . A/B tests are a common practice in software engineering, especially in web systems. A/B testing is defined as \u201ca randomized experimentation process wherein two or more versions of a variable (web page, page element, etc.) are shown to different segments of website visitors at the same time to determine which version leaves the maximum impact and drive business metrics.\u201d[1] In model evaluation, A/B tests determine the impact of different model predictions on user and business metrics. One common way of A/B testing models is to \u201ccanary\u201d data or return predictions on a small portion of the data (i.e., 1% or 10%) to the relevant users. The remaining data acts as a control and functions under existing system behavior (i.e., an old model or even no model). Evaluating the difference in metrics between the two groups can determine the relative impact of your model. This simple baseline can work well. Adding more statistically principled splits , which is common in A/B testing, can be a good idea.","title":"A/B Tests"},{"location":"spring2021/lecture-10/#labeling-tests","text":"Machine learning models operate in a GIGO paradigm: garbage in, garbage out. To prevent poor quality labels from cropping up and corrupting the model, you need to unit test the labeling systems and procedures. You should start by training, certifying, and evaluating individual labelers, who each play a crucial role in the quality of the labels. A simple and common label quality test is to spot check labels as they come in by opening up 100 or 1000 labels from a batch and evaluating them yourself to understand their quality. Using a performant model\u2019s guidance, you can make this process more efficient and only look at labels where the model and the labeler disagree. Another test can be to aggregate labels of multiple labels and measure agreement across labels . The higher the agreement, the better quality the labels are. Using metrics of agreement, you can assign \u201ctrust scores\u201d to labelers based on their performance relative to other labelers and weigh the labels accordingly.","title":"Labeling Tests"},{"location":"spring2021/lecture-10/#expectation-tests","text":"Expectation tests address the data preprocessing and storage system . Essentially, they are unit tests for your data. They are designed to catch data quality issues and bad data before they make their way into the pipeline. The typical way that expectation tests operate is rule- or threshold-based . At each step of the data processing pipeline, the output should conform to a specific format that matches a rule or specific format. If the rule or threshold does not pass, then that stage of the expectation test and the data pipeline\u2019s related step fails. Such tests are frequently run with batch data pipeline jobs. Great Expectations is an open-source library gaining popularity for running expectation tests. The library allows you to set hard rules for the kinds of values or behaviors (i.e., statistics) you expect from your data. How do you set the rules and thresholds for expectation tests? Most expectation tests are set manually. A more sophisticated option is to profile a high-quality sample of your data and set thresholds accordingly. In practice, to avoid false alarms from overly sensitive tests, a combination of both approaches is needed.","title":"Expectation Tests"},{"location":"spring2021/lecture-10/#challenges-and-recommendations-operationalizing-ml-tests","text":"Running tests is an excellent idea in theory but can pose many practical challenges for data science and ML teams. The first challenge is often organizational . In contrast to software engineering teams for whom testing is table stakes, data science teams often struggle to implement testing and code review norms. The second challenge is infrastructural. Most CI/CD platforms don\u2019t support GPUs, data integrations, or other required elements of testing ML systems effectively or efficiently. The third challenge is tooling , which has not yet been standardized for operations like comparing model performance and slicing datasets. Finally, decision-making for ML test performance is hard. What is \u201cgood enough\u201d test performance is often highly contextual, which is a challenge that varies across ML systems and teams. Let\u2019s boil all these lessons for testing down into a clear set of recommendations specific to ML systems: Test each part of the ML system, not just the model. You build the machine that builds the model, not just the model! Test code, data, and model performance, not just code. Testing model performance is an art, not a science. There is a considerable amount of intuition that guides testing ML systems. Thus, the fundamental goal of testing model performance is to build a granular understanding of how well your model performs and where you don\u2019t expect it to perform well. Using this intuition derived from testing, you can make better decisions about productionizing your model effectively. Build up to this gradually! You don\u2019t need to do everything detailed in this lecture, and certainly not all at once. Start with: Infrastructure tests Evaluation tests Expectation tests","title":"Challenges and Recommendations Operationalizing ML Tests"},{"location":"spring2021/lecture-10/#4-explainable-and-interpretable-ai","text":"","title":"4 - Explainable and Interpretable AI"},{"location":"spring2021/lecture-10/#definitions","text":"What do explainable and interpretable AI, buzzwords you\u2019ve undoubtedly heard before, actually mean? Let\u2019s start by outlining some more fundamental terms about the problem space: Domain predictability : the degree to which it is possible to detect data outside the model\u2019s domain of competence. Interpretability : the degree to which a human can consistently predict the model\u2019s result ( Kim et al., 2016 ). Explainability : the degree to which a human can understand the cause of a decision ( Miller, 2017 ). We\u2019ll walk through four different methods of making models interpretable and explainable: Use an interpretable family of models. Distill the complex model to an interpretable one. Understand the contribution of features to the prediction. Understand the contribution of training data points to the prediction.","title":"Definitions"},{"location":"spring2021/lecture-10/#use-an-interpretable-family-of-models","text":"Examples of interpretable families of models are simple, familiar models like linear regression, logistic regression, generalized linear models, and decision trees . If you understand the math of these models, it\u2019s pretty easy to understand why a model made the decision it did. Because of the reasonably elementary math, these models are interpretable and explainable. However, they are not very powerful. Another class of models that are interpretable is attention models . Examining where a model is \u201clooking\u201d helps us anticipate a model\u2019s prediction, thus making them interpretable. However, attention maps are not particularly explainable. They do not produce complete explanations for a model\u2019s output, just a directional explanation. Furthermore, attention maps are not reliable explanations. Attention maps tell us only where a model is looking, not why it is looking there. Frequently, models focus exclusively on an image\u2019s salient region without an underlying reasoning that relates to the task at hand. In the sample below, the attention model is \u201clooking\u201d at the salient region for classification, which has a very different meaning in each context. The conflation of attention with explanation is a critical pitfall to avoid.","title":"Use An Interpretable Family of Models"},{"location":"spring2021/lecture-10/#distill-a-complex-to-an-interpretable-one","text":"Instead of restricting models to only interpretable families, we can fit a more complex model and interpret its decision using another model from an interpretable family. The trick is to train this additional model, referred to as a surrogate model , on the raw data and the complex model\u2019s predictions. The surrogate model\u2019s corresponding interpretation can be used as a proxy for understanding the complex model. This technique is quite simple and fairly general to apply. In practice, however, two concerns manifest. If the surrogate itself performs well on the predictions, why not try to directly apply it rather than the more complex model? If it doesn\u2019t perform well, how do we know that it genuinely represents the complex model\u2019s behavior? Another category of surrogate models is local surrogate models (LIME) . Rather than apply the surrogate model in a global context on all the data, LIME models focus on a single point to generate an explanation for. A perturbation is applied to the point, resulting in a local neighborhood of perturbed data points. On top of these perturbed data points, a surrogate model is trained to map the points to the original predictions from the complex model. If the surrogate model classifies similarly to the complex model, the surrogate can be considered a proxy for interpretation purposes. This method is used widely, as it works for all data types (including images and text). However, defining the right perturbations and ensuring the stability of the explanations is challenging.","title":"Distill A Complex To An Interpretable One"},{"location":"spring2021/lecture-10/#understand-the-contribution-of-features-to-the-prediction","text":"Better understanding each feature\u2019s role in making a prediction is another option for interpretable and explainable ML. Data visualization is one such option, with plots like partial dependence plots and individual conditional expectation plots. A numerical method is permutation feature importance , which selects a feature, randomizes its order in the dataset, and sees how that affects performance. While this method is very easy and widely used, it doesn\u2019t work for high-dimensional data or cases where there is feature interdependence. A more principled approach to explaining the contribution of individual features is SHAP ( Shapley Additive Explanations ). At a high level, SHAP scores test how much changes in a single feature impact the output of a classifier when controlling for the values of the other features. This is a reliable method to apply, as it works on a variety of data and is mathematically principled. However, it can be tricky to implement and doesn\u2019t provide explanations. Gradient-based saliency maps are a popular method for explanations and interpretations. This intuitive method selects an input, performs a forward pass, computes the gradient with respect to the pixels, and visualizes the gradients. Essentially, how much does a unit change in the value of the input\u2019s pixels affect the prediction of the model? This is a straightforward and common method. Similar to the challenge with attention, the explanations may not be correct, and the overall method is fragile and sensitive to small changes.","title":"Understand The Contribution of Features To The Prediction"},{"location":"spring2021/lecture-10/#understand-the-contribution-of-training-data-points-to-the-prediction","text":"Instead of focusing on features and their explicit relevance to the prediction, we can also take a hard look at the training data points themselves. Prototypes and criticisms are one such approach, though it is less applicable to deep learning. In this method, prototypes are clusters of data that explain much of the variance in the model. Criticisms are data points not explained by the prototypes. Another approach is to look specifically at \u201c influential instances \u201d or data points that cause major changes in the model\u2019s predictions when removed from the data set.","title":"Understand The Contribution of Training Data Points To The Prediction"},{"location":"spring2021/lecture-10/#do-you-need-explainability","text":"A good question to ask yourself whether or not \u201cexplainable AI\u201d is a real need for your applications. There are a couple of cases where this question can be useful: Regulators demand it. In this case, there\u2019s not much you can do besides produce some kind of explainable model. However, it can be helpful to ask for clarification on what explainability is judged as. Users demand it. In some cases, users themselves may want trust or explainability in the system. Investigate the necessity for the explainability and trust to come directly from the model itself. Can good product design inspire trust more effectively? For example, allowing doctors to simply override models can reduce the immediate need for explainability. A big associated concern is how often users interact with the model. Infrequent interactions likely require explainable AI, as humans do not get a chance to build their feel for the system. More frequent interactions allow for the simpler objective of interpretability. Deployment demands it. Sometimes, ML stakeholders may demand explainability as a component of ensuring confidence in ML system deployment. In this context, explainability is the wrong objective; domain predictability is the real aim . Rather than full-on explainability, interpretability can be helpful for deployment, especially visualizations for debugging. At present, true explainability for deep learning models is not possible . Current explanation methods are not faithful to the original model performance; it can be easy to cherry-pick specific examples that can overstate explainability. Furthermore, these methods tend to be unreliable and highly sensitive to the input. Finally, as described in the attention section, the full explanation is often not available to modern explainability methods. Because of these reasons, explainability is not practically feasible for deep learning models (as of 2021). Read Cynthia Rudin\u2019s 2019 paper for more detail.","title":"Do You Need \"Explainability\"?"},{"location":"spring2021/lecture-10/#caveats-for-explainable-and-interpretable-ai","text":"If you genuinely need to explain your model\u2019s predictions, use an interpretable model family (read more here ). Don\u2019t try to force-fit deep learning explainability methods; they produce cool results but are not reliable enough for production use cases. Specific interpretability methods like LIME and SHAP are instrumental in helping users reach interpretability thresholds faster. Finally, the visualization for interpretability can be pretty useful for debugging.","title":"Caveats For Explainable and Interpretable AI"},{"location":"spring2021/lecture-10/#5-resources","text":"ML Test Score Paper Behavioral testing paper Jeremy Jordan\u2019s effective testing Robustness Gym Made with ML\u2019s guide to testing Eugene Yan\u2019s practical guide to maintaining machine learning Chip Huyen\u2019s CS329 lecture on evaluating models Interpretable ML Book [1] https://vwo.com/ab-testing-2/","title":"5 - Resources"},{"location":"spring2021/lecture-11/","text":"Lecture 11: Deployment & Monitoring Video Deployment: Monitoring: Slides Download slides as PDF Notes Notes were taken by James Le and Vishnu Rachakonda Download notes as PDF ML in production scales to meet users\u2019 demands by delivering thousands to millions of predictions per second. On the other hand, models in notebooks only work if you run the cells in the right order. To be frank, most data scientists and ML engineers do not know how to build production ML systems. Therefore, the goal of this lecture is to give you different flavors of accomplishing that task. I - Model Deployment 1 - Types of Deployment One way to conceptualize different approaches to deploy ML models is to think about where to deploy them in your application\u2019s overall architecture. The client-side runs locally on the user machine (web browser, mobile devices, etc..) It connects to the server-side that runs your code remotely. The server connects with a database to pull data out, render the data, and show the data to the user. Batch Prediction Batch prediction means that you train the models offline, dump the results into a database, then run the rest of the application normally. You periodically run your model on new data coming in and cache the results in a database. Batch prediction is commonly used in production when the universe of inputs is relatively small (e.g., one prediction per user per day). The pros of batch prediction: It is simple to implement. It requires relatively low latency to the user. The cons of batch prediction: It does not scale to complex input types. Users do not get the most up-to-date predictions. Models frequently become \u201cstale\u201d and hard to detect. Model-In-Service Model-in-service means that you package up your model and include it in the deployed web server. Then, the web server loads the model and calls it to make predictions. The pros of model-in-service prediction: It reuses your existing infrastructure. The cons of model-in-service prediction: The web server may be written in a different language. Models may change more frequently than the server code. Large models can eat into the resources for your webserver. Server hardware is not optimized for your model (e.g., no GPUs). Model and server may scale differently. Model-As-Service Model-as-service means that you deploy the model separately as its own service. The client and server can interact with the model by making requests to the model service and receiving responses. The pros of model-as-service prediction: It is dependable, as model bugs are less likely to crash the web app. It is scalable, as you can choose the optimal hardware for the model and scale it appropriately. It is flexible, as you can easily reuse the model across multiple applications. The cons of model-as-service prediction: It adds latency. It adds infrastructural complexity. Most importantly, you are now on the hook to run a model service... 2 - Building A Model Service REST APIs REST APIs represent a way of serving predictions in response to canonically formatted HTTP requests. There are alternatives such as gRPC and GraphQL . For instance, in your command line, you can use curl to post some data to an URL and get back JSON that contains the model predictions. Sadly, there is no standard way of formatting the data that goes into an ML model. Dependency Management Model predictions depend on the code , the model weights , and the code dependencies . All three need to be present on your webserver. For code and model weights, you can simply copy them locally (or write a script to extract them if they are large). But dependencies are trickier because they cause troubles. As they are hard to make consistent and update, your model behavior might change accordingly. There are two high-level strategies to manage code dependencies: You constrain the dependencies of your model. You use containers. ONNX If you go with the first strategy, you need a standard neural network format. The Open Neural Network Exchange (ONNX, for short) is designed to allow framework interoperability. The dream is to mix different frameworks, such that frameworks that are good for development (PyTorch) don\u2019t also have to be good at inference (Caffe2). The promise is that you can train a model with one tool stack and then deploy it using another for inference/prediction. ONNX is a robust and open standard for preventing framework lock-in and ensuring that your models will be usable in the long run. The reality is that since ML libraries change quickly, there are often bugs in the translation layer. Furthermore, how do you deal with non-library code (like feature transformations)? Docker If you go with the second strategy, you want to learn Docker . Docker is a computer program that performs operating-system-level virtualization, also known as containerization. What is a container, you might ask? It is a standardized unit of fully packaged software used for local development, shipping code, and deploying system. The best way to describe it intuitively is to think of a process surrounded by its filesystem. You run one or a few related processes, and they see a whole filesystem, not shared by anyone. This makes containers extremely portable , as they are detached from the underlying hardware and the platform that runs them. They are very lightweight , as a minimal amount of data needs to be included. They are secure , as the exposed attack surface of a container is extremely small. Note here that containers are different from virtual machines . Virtual machines require the hypervisor to virtualize a full hardware stack. There are also multiple guest operating systems, making them larger and more extended to boot. This is what AWS / GCP / Azure cloud instances are. Containers, on the other hand, require no hypervisor/hardware virtualization. All containers share the same host kernel. There are dedicated isolated user-space environments, making them much smaller in size and faster to boot. In brief, you should familiarize yourself with these basic concepts: Dockerfile defines how to build an image. Image is a built packaged environment. Containe r is where images are run inside. Repository hosts different versions of an image. Registry is a set of repositories. Furthermore, Docker has a robust ecosystem. It has the DockerHub for community-contributed images. It\u2019s incredibly easy to search for images that meet your needs, ready to pull down and use with little-to-no modification. Though Docker presents how to deal with each of the individual microservices, we also need an orchestrator to handle the whole cluster of services. Such an orchestrator distributes containers onto the underlying virtual machines or bare metal so that these containers talk to each other and coordinate to solve the task at hand. The standard container orchestration tool is Kubernetes . Performance Optimization We will talk mostly about how to run your model service faster on a single machine. Here are the key questions that you want to address: Do you want inference on a GPU or not? How can you run multiple copies of the model at the same time? How to make the model smaller? How to improve model performance via caching, batching, and GPU sharing? GPU or no GPU? Here are the pros of GPU inference: You use the same hardware that your model is trained on probably. If your model gets bigger and you want to limit model size or tune batch size, you will get high throughput. Here are the cons of GPU inference: GPU is complex to set up. GPUs are expensive. Concurrency Instead of running a single model copy on your machine, you run multiple model copies on different CPUs or cores. In practice, you need to be careful about thread tuning - making sure that each model copy only uses the minimum number of threads required. Read this blog post from Roblox for the details. Model distillation Model distillation is a compression technique in which a small \u201cstudent\u201d model is trained to reproduce the behavior of a large \u201cteacher\u201d model. The method was first proposed by Bucila et al., 2006 and generalized by Hinton et al., 2015 . In distillation, knowledge is transferred from the teacher model to the student by minimizing a loss function. The target is the distribution of class probabilities predicted by the teacher model. That is\u200a\u2014\u200athe output of a softmax function on the teacher model\u2019s logits. Distillation can be finicky to do yourself, so it is infrequently used in practice . Read this blog post from Derrick Mwiti for several model distillation techniques for deep learning. Model quantization Model quantization is a model compression technique that makes the model physically smaller to save disk space and require less memory during computation to run faster. It decreases the numerical precision of a model\u2019s weights. In other words, each weight is permanently encoded using fewer bits. Note here that there are tradeoffs with accuracy . A straightforward method is implemented in the TensorFlow Lite toolkit . It turns a matrix of 32-bit floats into 8-bit integers by applying a simple \u201ccenter-and-scale\u201d transform to it: W_8 = W_32 / scale + shift (scale and shift are determined individually for each weight matrix). This way, the 8-bit W is used in matrix multiplication, and only the result is then corrected by applying the \u201ccenter-and-scale\u201d operation in reverse. PyTorch also has quantization built-in that includes three techniques: dynamic quantization, post-training static quantization, and quantization-aware training. Caching For many ML models, the input distribution is non-uniform (some are more common than others). Caching takes advantage of that. Instead of constantly calling the model on every input no matter what, we first cache the model\u2019s frequently-used inputs . Before calling the model, we check the cache and only call it on the frequently-used inputs. Caching techniques can get very fancy, but the most basic way to get started is using Python\u2019s functools . Batching Typically, ML models achieve higher throughput when making predictions in parallel (especially true for GPU inference). At a high level, here\u2019s how batching works: You gather predictions that are coming in until you have a batch for your system. Then, you run the model on that batch and return predictions to those users who request them. You need to tune the batch size and address the tradeoff between throughput and latency. You need to have a way to shortcut the process if latency becomes too long. The last caveat is that you probably do not want to implement batching yourself . Sharing The GPU Your model may not take up all of the GPU memory with your inference batch size. Why not run multiple models on the same GPU? You probably want to use a model serving solution that supports this out of the box. Model Serving Libraries There are canonical open-source model serving libraries for both PyTorch ( TorchServe ) and TensorFlow ( TensorFlow Serving ). Ray Serve is another promising choice. Even NVIDIA has joined the game with Triton Inference Server . Horizontal Scaling If you have too much traffic for a single machine, let\u2019s split traffic among multiple machines . At a high level, you duplicate your prediction service, use a load balancer to split traffic, and send the traffic to the appropriate copy of your service. In practice, there are two common methods: Use a container orchestration toolkit like Kubernetes. Use a serverless option like AWS Lambda. Container Orchestration In this paradigm, your Docker containers are coordinated by Kubernetes. K8s provides a single service for you to send requests to. Then it divides up traffic that gets sent to that service to virtual copies of your containers (that are running on your infrastructure). You can build a system like this yourself on top of K8s if you want to. But there are emerging frameworks that can handle all such infrastructure out of the box if you have a K8s cluster running. KFServing is a part of the Kubeflow package, a popular K8s-native ML infrastructure solution. Seldon provides a model serving stack on top of K8s. Deploying Code As Serverless Functions The idea here is that the app code and dependencies are packaged into .zip files (or Docker containers) with a single entry point function. All the major cloud providers such as AWS Lambda, Google Cloud Functions, or Azure Functions will manage everything else: instant scaling to 10,000+ requests per second, load balancing, etc. The good thing is that you only pay for compute-time . Furthermore, this approach lowers your DevOps load, as you do not own any servers. The tradeoff is that you have to work with severe constraints : Your entire deployment package is quite limited. You can only do CPU execution. It can be challenging to build model pipelines. There are limited state management and deployment tooling. Model Deployment If serving is how you turn a model into something that can respond to requests, deployment is how you roll out, manage, and update these services. You probably want to be able to roll out gradually , roll back instantly , and deploy pipelines of models . Many challenging infrastructure considerations go into this, but hopefully, your deployment library will take care of this for you. Managed Options If you do not want to deal with any of the things mentioned thus far, there are managed options in the market. All major cloud providers have ones that enable you to package your model in a predefined way and turn it into an API. Startups like Algorithmia and Cortex are some alternatives. The big drawback is that pricing tends to be high, so you pay a premium fee in exchange for convenience . Takeaways If you are making CPU inference, you can get away with scaling by launching more servers or going serverless. Serverless makes sense if you can get away with CPUs, and traffic is spiky or low-volume. If you are using GPU inference, serving tools will save you time. It\u2019s worth keeping an eye on startups in this space for GPU inference. 3 - Edge Deployment Edge prediction means that you first send the model weights to the client edge device. Then, the client loads the model and interacts with it directly. The pros of edge prediction: It has low latency. It does not require an Internet connection. It satisfies data security requirements, as data does not need to leave the user\u2019s device. The cons of edge prediction: The client often has limited hardware resources available. Embedded and mobile frameworks are less full-featured than TensorFlow and PyTorch. It is challenging to update models. It is difficult to monitor and debug when things go wrong. Tools For Edge Deployment TensorRT is NVIDIA\u2019s framework meant to help you optimize models for inference on NVIDIA devices in data centers and embedded/automotive environments. TensorRT is also integrated with application-specific SDKs to provide developers a unified path to deploy conversational AI, recommender, video conference, and streaming apps in production. ApacheTVM is an open-source machine learning compiler framework for CPUs, GPUs, and ML accelerators. It aims to enable ML engineers to optimize and run computations efficiently on any hardware backend. In particular, it compiles ML models into minimum deployable modules and provides the infrastructure to automatically optimize models on more backends with better performance. Tensorflow Lite provides a trained TensorFlow model framework to be compressed and deployed to a mobile or embedded application. TensorFlow\u2019s computationally expensive training process can still be performed in the environment that best suits it (personal server, cloud, overclocked computer). TensorFlow Lite then takes the resulting model (frozen graph, SavedModel, or HDF5 model) as input, packages, deploys, and then interprets it in the client application, handling the resource-conserving optimizations along the way. PyTorch Mobile is a framework for helping mobile developers and machine learning engineers embed PyTorch models on-device. Currently, it allows any TorchScript model to run directly inside iOS and Android applications. PyTorch Mobile\u2019s initial release supports many different quantization techniques, which shrink model sizes without significantly affecting performance. PyTorch Mobile also allows developers to directly convert a PyTorch model to a mobile-ready format without needing to work through other tools/frameworks. JavaScript is a portable way of running code on different devices. Tensorflow.js enables you to run TensorFlow code in JavaScript. You can use off-the-shelf JavaScript models or convert Python TensorFlow models to run in the browser or under Node.js, retrain pre-existing ML models using your data, and build/train models directly in JavaScript using flexible and intuitive APIs. Core ML was released by Apple back in 2017. It is optimized for on-device performance, which minimizes a model\u2019s memory footprint and power consumption. Running strictly on the device also ensures that user data is kept secure. The app runs even in the absence of a network connection. Generally speaking, it is straightforward to use with just a few lines of code needed to integrate a complete ML model into your device. The downside is that you can only make the model inference, as no model training is possible. ML Kit was announced by Google Firebase in 2018. It enables developers to utilize ML in mobile apps either with (1) inference in the cloud via API or (2) inference on-device (like Core ML). For the former option, ML Kit offers six base APIs with pertained models such as Image Labeling, Text Recognition, and Barcode Scanning. For the latter option, ML Kit offers lower accuracy but more security to user data, compared to the cloud version. If you are interested in either of the above options, check out this comparison by the FritzAI team. Additionally, FritzAI is an ML platform for mobile developers that provide pre-trained models, developer tools, and SDKs for iOS, Android, and Unity. More Efficient Models Another thing to consider for edge deployment is to make the models more efficient. One way to do this is to use the same quantization and distillation techniques discussed above. Another way is to pick mobile-friendly model architectures . The first successful example is MobileNet , which performs various downsampling techniques to a traditional ConvNet architecture to maximize accuracy while being mindful of the restricted resources for a mobile or an embedded device. This analysis by Yusuke Uchida explains why MobileNet and its variants are fast. A well-known case study of applying knowledge distillation in practice is Hugging Face\u2019s DistilBERT , a smaller language model derived from the supervision of the popular BERT language model. DistilBERT removes the toke-type embeddings and the pooler (used for the next sentence classification task) from BERT while keeping the rest of the architecture identical and reducing the number of layers by a factor of two. Overall, DistilBERT has about half the total number of parameters of the BERT base and retains 95% of BERT\u2019s performances on the language understanding benchmark GLUE. Mindset For Edge Deployment It is crucial to choose your architecture with your target hardware in mind . Specifically, you can make up a factor of 2-10 through distillation, quantization, and other tricks (but not more than that). Once you have a model that works on your edge device, you can iterate locally as long as you add model size and latency to your metrics and avoid regression. You should treat tuning the model for your device as an additional risk in the deployment cycle and test it accordingly. In other words, you should always test your models on production hardware before deploying them for real. Since models can be finicky, it\u2019s a good idea to build fallback mechanisms into the application if the model fails or is too slow. Takeaways Web deployment is easier, so only perform edge deployment if you need to. You should choose your framework to match the available hardware and corresponding mobile frameworks. Else, you can try Apache TVM to be more flexible. You should start considering hardware constraints at the beginning of the project and choose the architectures accordingly. II - Model Monitoring Once you deploy models, how do you make sure they are staying healthy and working well? Enter model monitoring. Many things can go wrong with a model once it\u2019s been trained. This can happen even if your model has been trained properly, with a reasonable validation and test loss, as well as robust performance across various slices and quality predictions. Even after you\u2019ve troubleshot and tested a model, things can still go wrong! 1 - Why Model Degrades Post-Deployment? Model performance tends to degrade after you\u2019ve deployed a model. Why does this occur? In supervised learning, we seek to fit a function f to approximate a posterior using the data available to us. If any component of this process changes (i.e., the data x), the deployed model can see an unexpectedly degraded performance. See the below chart for examples of how such post-deployment degradations can occur theoretically and in practice: In summary, there are three core ways that the model\u2019s performance can degrade: data drift , concept drift , and domain shift . In data drift, the underlying data expectation that your model is built can unexpectedly change, perhaps through a bug in the upstream data pipeline or even due to malicious users feeding the model bad data. In concept drift, the actual outcome you seek to model, or the relationship between the data and the outcome, may fray. For example, users may start to pick movies in a different manner based on the output of your model, thereby changing the fundamental \u201cconcept\u201d the model needs to approximate. Finally, in domain shift, if your dataset does not appropriately sample the production, post-deployment setting, the model\u2019s performance may suffer; this could be considered a \u201clong tail\u201d scenario, where many rare examples that are not present in the development data occur. 2 - Data Drift There are a few different types of data drift: Instantaneous drift : In this situation, the paradigm of the draft dramatically shifts. Examples are deploying the model in a new domain (e.g., self-driving car model in a new city), a bug in the preprocessing pipeline, or even major external shifts like COVID. Gradual drift : In this situation, the value of data gradually changes with time. For example, users\u2019 preferences may change over time, or new concepts can get introduced to the domain. Periodic drift : Data can have fluctuating value due to underlying patterns like seasonality or time zones. Temporary drift : The most difficult to detect, drift can occur through a short-term change in the data that shifts back to normal. This could be via a short-lived malicious attack, or even simply because a user with different demographics or behaviors uses your product in a way that it\u2019s not designed to be used. While these categories may seem like purely academic categories, the consequences of data shift are very real . This is a real problem that affects many companies and is only now starting to get the attention it merits. 3 - What Should You Monitor? There are four core types of signals to monitor for machine learning models. These metrics trade off with another in terms of how informative they are and how easy they are to access. Put simply, the harder a metric may be to monitor, the more useful it likely is. The hardest and best metrics to monitor are model performance metrics , though these can be difficult to acquire in real-time (labels are hard to come by). Business metrics can be helpful signals of model degradation in monitoring but can easily be confounded by other impactful considerations. Model inputs and predictions are a simple way to identify high-level drift and are very easy to gather. Still, they can be difficult to assess in terms of actual performance impact, leaving it more of an art than science. Finally, system performance (e.g., GPU usage) can be a coarse method of catching serious bugs. In considering which metrics to focus on, prioritize ground-truth metrics (model and business metrics), then approximate performance metrics (business and input/outputs), and finally, system health metrics. 4 - How Do You Measure Distribution Changes? Select A Reference Window To measure distribution changes in metrics you\u2019re monitoring, start by picking a reference set of production data to compare new data to. There are a few different ways of picking this reference data (e.g., sliding window or fixed window of production data), but the most practical thing to do is to use your training or evaluation data as the reference . Data coming in looking different from what you developed your model using is an important signal to act on. Select A Measurement Window After picking a reference window, the next step is to choose a measurement window to compare, measure distance, and evaluate for drift. The challenge is that selecting a measurement window is highly problem-dependent. One solution is to pick one or several window sizes and slide them over the data . To avoid recomputing metrics over and over again, when you slide the window, it\u2019s worth looking into the literature on mergeable (quantile) sketching algorithms . Compare Windows Using A Distance Metric What distance metrics should we use to compare the reference window to the measurement window? Some 1-D metric categories are: Rule-based distance metrics (e.g., data quality): Summary statistics, the volume of data points, number of missing values, or more complex tests like overall comparisons are common data quality checks that can be applied. Great Expectations is a valuable library for this. Definitely invest in simple rule-based metrics. They catch a large number of bugs, as publications from Amazon and Google detail. Statistical distance metrics (e.g., KS statistics, KL divergence, D_1 distance, etc.) KL Divergence : Defined as the expectation of a ratio of logs of two different distributions, this commonly known metric is very sensitive to what happens in the tails of the distribution. It\u2019s not well-suited to data shift testing since it\u2019s easily disturbed, is not interpretable, and struggles with data in different ranges. KS Statistic : This metric is defined as the max distance between CDFs, which is easy to interpret and is thus used widely in practice. Say yes to the KS statistic! D1 Distance : Defined as the sum of distances between PDFs, this is a metric used at Google. Despite seeming less principled, it\u2019s easily interpretable and has the added benefit of knowing Google uses it (so why not you?). An open area of research is understanding the impact of differing drift patterns on distance metrics and model performance. Another open area of research is high-dimensional distance metrics. Some options here are: Maximum mean discrepancy Performing multiple 1D comparisons across the data: While suffering from the multiple hypothesis testing problem , this is a practical approach. Prioritize some features for 1D comparisons: Another option is to avoid testing all the features and only focus on those that merit comparison; for example, those features you know may have shifted in the data. Projections : In this approach, large data points are put through a dimensionality reduction process and then subject to a two-sample statistical test. Reducing the dimensionality with a domain-specific approach (e.g., mean pixel value for images, length of sentence) is recommended. At a high level, this entire distance metric work aims to identify not just a score for any data shift but also understand its impact on the model. While choosing a metric can be complicated with all the possible options, you should focus on understanding your model\u2019s robustness in a post-deployment scenario. 5 - How Do You Tell If A Change Is Bad? There\u2019s no hard and fast rule for finding if a change in the data is bad. An easy option is to set thresholds on the test values. Don\u2019t use a statistical test like the KS test, as they are too sensitive to small shifts. Other options include setting manual ranges, comparing values over time, or even applying an unsupervised model to detect outliers. In practice, fixed rules and specified ranges of test values are used most in practice. 6 - Tools For Monitoring There are three categories of tools useful for monitoring: System monitoring tools like AWS CloudWatch , Datadog , New Relic , and honeycomb test traditional performance metrics Data quality tools like Great Expectations , Anomalo , and Monte Carlo test if specific windows of data violate rules or assumptions. ML monitoring tools like Arize , Fiddler , and Arthur can also be useful, as they specifically test models. 7 - Evaluation Store Monitoring is more central to ML than for traditional software . In traditional SWE, most bugs cause loud failures, and the data that is monitored is most valuable to detect and diagnose problems. If the system is working well, the data from these metrics and monitoring systems may not be useful. In machine learning, however, monitoring plays a different role. First off, bugs in ML systems often lead to silent degradations in performance. Furthermore, the data that is monitored in ML is literally the code used to train the next iteration of models. Because monitoring is so essential to ML systems, tightly integrating it into the ML system architecture brings major benefits. In particular, better integrating and monitoring practices, or creating an evaluation store, can close the data flywheel loop , a concept we talked about earlier in the class. As we build models, we create a mapping between data and model. As the data changes and we retrain models, monitoring these changes doesn\u2019t become an endpoint--it becomes a part of the entire model development process. Monitoring, via an evaluation store, should touch all parts of your stack. One challenge that this process helps solve is effectively choosing which data points to collect, store, and label. Evaluation stores can help identify which data to collect more points for based on uncertain performance. As more data is collected and labeled, efficient retraining can be performed using evaluation store guidance. Conclusion In summary, make sure to monitor your models! Something will always go wrong, and you should have a system to catch errors. Start by looking at data quality metrics and system metrics, as they are easiest. In a perfect world, the testing and monitoring should be linked, and they should help you close the data flywheel. There will be a lot of tooling and research that will hopefully come soon!","title":"Lecture 11: Deployment & Monitoring"},{"location":"spring2021/lecture-11/#lecture-11-deployment-monitoring","text":"","title":"Lecture 11: Deployment &amp; Monitoring"},{"location":"spring2021/lecture-11/#video","text":"Deployment: Monitoring:","title":"Video"},{"location":"spring2021/lecture-11/#slides","text":"Download slides as PDF","title":"Slides"},{"location":"spring2021/lecture-11/#notes","text":"Notes were taken by James Le and Vishnu Rachakonda Download notes as PDF ML in production scales to meet users\u2019 demands by delivering thousands to millions of predictions per second. On the other hand, models in notebooks only work if you run the cells in the right order. To be frank, most data scientists and ML engineers do not know how to build production ML systems. Therefore, the goal of this lecture is to give you different flavors of accomplishing that task.","title":"Notes"},{"location":"spring2021/lecture-11/#i-model-deployment","text":"","title":"I - Model Deployment"},{"location":"spring2021/lecture-11/#1-types-of-deployment","text":"One way to conceptualize different approaches to deploy ML models is to think about where to deploy them in your application\u2019s overall architecture. The client-side runs locally on the user machine (web browser, mobile devices, etc..) It connects to the server-side that runs your code remotely. The server connects with a database to pull data out, render the data, and show the data to the user.","title":"1 - Types of Deployment"},{"location":"spring2021/lecture-11/#batch-prediction","text":"Batch prediction means that you train the models offline, dump the results into a database, then run the rest of the application normally. You periodically run your model on new data coming in and cache the results in a database. Batch prediction is commonly used in production when the universe of inputs is relatively small (e.g., one prediction per user per day). The pros of batch prediction: It is simple to implement. It requires relatively low latency to the user. The cons of batch prediction: It does not scale to complex input types. Users do not get the most up-to-date predictions. Models frequently become \u201cstale\u201d and hard to detect.","title":"Batch Prediction"},{"location":"spring2021/lecture-11/#model-in-service","text":"Model-in-service means that you package up your model and include it in the deployed web server. Then, the web server loads the model and calls it to make predictions. The pros of model-in-service prediction: It reuses your existing infrastructure. The cons of model-in-service prediction: The web server may be written in a different language. Models may change more frequently than the server code. Large models can eat into the resources for your webserver. Server hardware is not optimized for your model (e.g., no GPUs). Model and server may scale differently.","title":"Model-In-Service"},{"location":"spring2021/lecture-11/#model-as-service","text":"Model-as-service means that you deploy the model separately as its own service. The client and server can interact with the model by making requests to the model service and receiving responses. The pros of model-as-service prediction: It is dependable, as model bugs are less likely to crash the web app. It is scalable, as you can choose the optimal hardware for the model and scale it appropriately. It is flexible, as you can easily reuse the model across multiple applications. The cons of model-as-service prediction: It adds latency. It adds infrastructural complexity. Most importantly, you are now on the hook to run a model service...","title":"Model-As-Service"},{"location":"spring2021/lecture-11/#2-building-a-model-service","text":"","title":"2 - Building A Model Service"},{"location":"spring2021/lecture-11/#rest-apis","text":"REST APIs represent a way of serving predictions in response to canonically formatted HTTP requests. There are alternatives such as gRPC and GraphQL . For instance, in your command line, you can use curl to post some data to an URL and get back JSON that contains the model predictions. Sadly, there is no standard way of formatting the data that goes into an ML model.","title":"REST APIs"},{"location":"spring2021/lecture-11/#dependency-management","text":"Model predictions depend on the code , the model weights , and the code dependencies . All three need to be present on your webserver. For code and model weights, you can simply copy them locally (or write a script to extract them if they are large). But dependencies are trickier because they cause troubles. As they are hard to make consistent and update, your model behavior might change accordingly. There are two high-level strategies to manage code dependencies: You constrain the dependencies of your model. You use containers.","title":"Dependency Management"},{"location":"spring2021/lecture-11/#onnx","text":"If you go with the first strategy, you need a standard neural network format. The Open Neural Network Exchange (ONNX, for short) is designed to allow framework interoperability. The dream is to mix different frameworks, such that frameworks that are good for development (PyTorch) don\u2019t also have to be good at inference (Caffe2). The promise is that you can train a model with one tool stack and then deploy it using another for inference/prediction. ONNX is a robust and open standard for preventing framework lock-in and ensuring that your models will be usable in the long run. The reality is that since ML libraries change quickly, there are often bugs in the translation layer. Furthermore, how do you deal with non-library code (like feature transformations)?","title":"ONNX"},{"location":"spring2021/lecture-11/#docker","text":"If you go with the second strategy, you want to learn Docker . Docker is a computer program that performs operating-system-level virtualization, also known as containerization. What is a container, you might ask? It is a standardized unit of fully packaged software used for local development, shipping code, and deploying system. The best way to describe it intuitively is to think of a process surrounded by its filesystem. You run one or a few related processes, and they see a whole filesystem, not shared by anyone. This makes containers extremely portable , as they are detached from the underlying hardware and the platform that runs them. They are very lightweight , as a minimal amount of data needs to be included. They are secure , as the exposed attack surface of a container is extremely small. Note here that containers are different from virtual machines . Virtual machines require the hypervisor to virtualize a full hardware stack. There are also multiple guest operating systems, making them larger and more extended to boot. This is what AWS / GCP / Azure cloud instances are. Containers, on the other hand, require no hypervisor/hardware virtualization. All containers share the same host kernel. There are dedicated isolated user-space environments, making them much smaller in size and faster to boot. In brief, you should familiarize yourself with these basic concepts: Dockerfile defines how to build an image. Image is a built packaged environment. Containe r is where images are run inside. Repository hosts different versions of an image. Registry is a set of repositories. Furthermore, Docker has a robust ecosystem. It has the DockerHub for community-contributed images. It\u2019s incredibly easy to search for images that meet your needs, ready to pull down and use with little-to-no modification. Though Docker presents how to deal with each of the individual microservices, we also need an orchestrator to handle the whole cluster of services. Such an orchestrator distributes containers onto the underlying virtual machines or bare metal so that these containers talk to each other and coordinate to solve the task at hand. The standard container orchestration tool is Kubernetes .","title":"Docker"},{"location":"spring2021/lecture-11/#performance-optimization","text":"We will talk mostly about how to run your model service faster on a single machine. Here are the key questions that you want to address: Do you want inference on a GPU or not? How can you run multiple copies of the model at the same time? How to make the model smaller? How to improve model performance via caching, batching, and GPU sharing?","title":"Performance Optimization"},{"location":"spring2021/lecture-11/#gpu-or-no-gpu","text":"Here are the pros of GPU inference: You use the same hardware that your model is trained on probably. If your model gets bigger and you want to limit model size or tune batch size, you will get high throughput. Here are the cons of GPU inference: GPU is complex to set up. GPUs are expensive.","title":"GPU or no GPU?"},{"location":"spring2021/lecture-11/#concurrency","text":"Instead of running a single model copy on your machine, you run multiple model copies on different CPUs or cores. In practice, you need to be careful about thread tuning - making sure that each model copy only uses the minimum number of threads required. Read this blog post from Roblox for the details.","title":"Concurrency"},{"location":"spring2021/lecture-11/#model-distillation","text":"Model distillation is a compression technique in which a small \u201cstudent\u201d model is trained to reproduce the behavior of a large \u201cteacher\u201d model. The method was first proposed by Bucila et al., 2006 and generalized by Hinton et al., 2015 . In distillation, knowledge is transferred from the teacher model to the student by minimizing a loss function. The target is the distribution of class probabilities predicted by the teacher model. That is\u200a\u2014\u200athe output of a softmax function on the teacher model\u2019s logits. Distillation can be finicky to do yourself, so it is infrequently used in practice . Read this blog post from Derrick Mwiti for several model distillation techniques for deep learning.","title":"Model distillation"},{"location":"spring2021/lecture-11/#model-quantization","text":"Model quantization is a model compression technique that makes the model physically smaller to save disk space and require less memory during computation to run faster. It decreases the numerical precision of a model\u2019s weights. In other words, each weight is permanently encoded using fewer bits. Note here that there are tradeoffs with accuracy . A straightforward method is implemented in the TensorFlow Lite toolkit . It turns a matrix of 32-bit floats into 8-bit integers by applying a simple \u201ccenter-and-scale\u201d transform to it: W_8 = W_32 / scale + shift (scale and shift are determined individually for each weight matrix). This way, the 8-bit W is used in matrix multiplication, and only the result is then corrected by applying the \u201ccenter-and-scale\u201d operation in reverse. PyTorch also has quantization built-in that includes three techniques: dynamic quantization, post-training static quantization, and quantization-aware training.","title":"Model quantization"},{"location":"spring2021/lecture-11/#caching","text":"For many ML models, the input distribution is non-uniform (some are more common than others). Caching takes advantage of that. Instead of constantly calling the model on every input no matter what, we first cache the model\u2019s frequently-used inputs . Before calling the model, we check the cache and only call it on the frequently-used inputs. Caching techniques can get very fancy, but the most basic way to get started is using Python\u2019s functools .","title":"Caching"},{"location":"spring2021/lecture-11/#batching","text":"Typically, ML models achieve higher throughput when making predictions in parallel (especially true for GPU inference). At a high level, here\u2019s how batching works: You gather predictions that are coming in until you have a batch for your system. Then, you run the model on that batch and return predictions to those users who request them. You need to tune the batch size and address the tradeoff between throughput and latency. You need to have a way to shortcut the process if latency becomes too long. The last caveat is that you probably do not want to implement batching yourself .","title":"Batching"},{"location":"spring2021/lecture-11/#sharing-the-gpu","text":"Your model may not take up all of the GPU memory with your inference batch size. Why not run multiple models on the same GPU? You probably want to use a model serving solution that supports this out of the box.","title":"Sharing The GPU"},{"location":"spring2021/lecture-11/#model-serving-libraries","text":"There are canonical open-source model serving libraries for both PyTorch ( TorchServe ) and TensorFlow ( TensorFlow Serving ). Ray Serve is another promising choice. Even NVIDIA has joined the game with Triton Inference Server .","title":"Model Serving Libraries"},{"location":"spring2021/lecture-11/#horizontal-scaling","text":"If you have too much traffic for a single machine, let\u2019s split traffic among multiple machines . At a high level, you duplicate your prediction service, use a load balancer to split traffic, and send the traffic to the appropriate copy of your service. In practice, there are two common methods: Use a container orchestration toolkit like Kubernetes. Use a serverless option like AWS Lambda.","title":"Horizontal Scaling"},{"location":"spring2021/lecture-11/#container-orchestration","text":"In this paradigm, your Docker containers are coordinated by Kubernetes. K8s provides a single service for you to send requests to. Then it divides up traffic that gets sent to that service to virtual copies of your containers (that are running on your infrastructure). You can build a system like this yourself on top of K8s if you want to. But there are emerging frameworks that can handle all such infrastructure out of the box if you have a K8s cluster running. KFServing is a part of the Kubeflow package, a popular K8s-native ML infrastructure solution. Seldon provides a model serving stack on top of K8s.","title":"Container Orchestration"},{"location":"spring2021/lecture-11/#deploying-code-as-serverless-functions","text":"The idea here is that the app code and dependencies are packaged into .zip files (or Docker containers) with a single entry point function. All the major cloud providers such as AWS Lambda, Google Cloud Functions, or Azure Functions will manage everything else: instant scaling to 10,000+ requests per second, load balancing, etc. The good thing is that you only pay for compute-time . Furthermore, this approach lowers your DevOps load, as you do not own any servers. The tradeoff is that you have to work with severe constraints : Your entire deployment package is quite limited. You can only do CPU execution. It can be challenging to build model pipelines. There are limited state management and deployment tooling.","title":"Deploying Code As Serverless Functions"},{"location":"spring2021/lecture-11/#model-deployment","text":"If serving is how you turn a model into something that can respond to requests, deployment is how you roll out, manage, and update these services. You probably want to be able to roll out gradually , roll back instantly , and deploy pipelines of models . Many challenging infrastructure considerations go into this, but hopefully, your deployment library will take care of this for you.","title":"Model Deployment"},{"location":"spring2021/lecture-11/#managed-options","text":"If you do not want to deal with any of the things mentioned thus far, there are managed options in the market. All major cloud providers have ones that enable you to package your model in a predefined way and turn it into an API. Startups like Algorithmia and Cortex are some alternatives. The big drawback is that pricing tends to be high, so you pay a premium fee in exchange for convenience .","title":"Managed Options"},{"location":"spring2021/lecture-11/#takeaways","text":"If you are making CPU inference, you can get away with scaling by launching more servers or going serverless. Serverless makes sense if you can get away with CPUs, and traffic is spiky or low-volume. If you are using GPU inference, serving tools will save you time. It\u2019s worth keeping an eye on startups in this space for GPU inference.","title":"Takeaways"},{"location":"spring2021/lecture-11/#3-edge-deployment","text":"Edge prediction means that you first send the model weights to the client edge device. Then, the client loads the model and interacts with it directly. The pros of edge prediction: It has low latency. It does not require an Internet connection. It satisfies data security requirements, as data does not need to leave the user\u2019s device. The cons of edge prediction: The client often has limited hardware resources available. Embedded and mobile frameworks are less full-featured than TensorFlow and PyTorch. It is challenging to update models. It is difficult to monitor and debug when things go wrong.","title":"3 - Edge Deployment"},{"location":"spring2021/lecture-11/#tools-for-edge-deployment","text":"TensorRT is NVIDIA\u2019s framework meant to help you optimize models for inference on NVIDIA devices in data centers and embedded/automotive environments. TensorRT is also integrated with application-specific SDKs to provide developers a unified path to deploy conversational AI, recommender, video conference, and streaming apps in production. ApacheTVM is an open-source machine learning compiler framework for CPUs, GPUs, and ML accelerators. It aims to enable ML engineers to optimize and run computations efficiently on any hardware backend. In particular, it compiles ML models into minimum deployable modules and provides the infrastructure to automatically optimize models on more backends with better performance. Tensorflow Lite provides a trained TensorFlow model framework to be compressed and deployed to a mobile or embedded application. TensorFlow\u2019s computationally expensive training process can still be performed in the environment that best suits it (personal server, cloud, overclocked computer). TensorFlow Lite then takes the resulting model (frozen graph, SavedModel, or HDF5 model) as input, packages, deploys, and then interprets it in the client application, handling the resource-conserving optimizations along the way. PyTorch Mobile is a framework for helping mobile developers and machine learning engineers embed PyTorch models on-device. Currently, it allows any TorchScript model to run directly inside iOS and Android applications. PyTorch Mobile\u2019s initial release supports many different quantization techniques, which shrink model sizes without significantly affecting performance. PyTorch Mobile also allows developers to directly convert a PyTorch model to a mobile-ready format without needing to work through other tools/frameworks. JavaScript is a portable way of running code on different devices. Tensorflow.js enables you to run TensorFlow code in JavaScript. You can use off-the-shelf JavaScript models or convert Python TensorFlow models to run in the browser or under Node.js, retrain pre-existing ML models using your data, and build/train models directly in JavaScript using flexible and intuitive APIs. Core ML was released by Apple back in 2017. It is optimized for on-device performance, which minimizes a model\u2019s memory footprint and power consumption. Running strictly on the device also ensures that user data is kept secure. The app runs even in the absence of a network connection. Generally speaking, it is straightforward to use with just a few lines of code needed to integrate a complete ML model into your device. The downside is that you can only make the model inference, as no model training is possible. ML Kit was announced by Google Firebase in 2018. It enables developers to utilize ML in mobile apps either with (1) inference in the cloud via API or (2) inference on-device (like Core ML). For the former option, ML Kit offers six base APIs with pertained models such as Image Labeling, Text Recognition, and Barcode Scanning. For the latter option, ML Kit offers lower accuracy but more security to user data, compared to the cloud version. If you are interested in either of the above options, check out this comparison by the FritzAI team. Additionally, FritzAI is an ML platform for mobile developers that provide pre-trained models, developer tools, and SDKs for iOS, Android, and Unity.","title":"Tools For Edge Deployment"},{"location":"spring2021/lecture-11/#more-efficient-models","text":"Another thing to consider for edge deployment is to make the models more efficient. One way to do this is to use the same quantization and distillation techniques discussed above. Another way is to pick mobile-friendly model architectures . The first successful example is MobileNet , which performs various downsampling techniques to a traditional ConvNet architecture to maximize accuracy while being mindful of the restricted resources for a mobile or an embedded device. This analysis by Yusuke Uchida explains why MobileNet and its variants are fast. A well-known case study of applying knowledge distillation in practice is Hugging Face\u2019s DistilBERT , a smaller language model derived from the supervision of the popular BERT language model. DistilBERT removes the toke-type embeddings and the pooler (used for the next sentence classification task) from BERT while keeping the rest of the architecture identical and reducing the number of layers by a factor of two. Overall, DistilBERT has about half the total number of parameters of the BERT base and retains 95% of BERT\u2019s performances on the language understanding benchmark GLUE.","title":"More Efficient Models"},{"location":"spring2021/lecture-11/#mindset-for-edge-deployment","text":"It is crucial to choose your architecture with your target hardware in mind . Specifically, you can make up a factor of 2-10 through distillation, quantization, and other tricks (but not more than that). Once you have a model that works on your edge device, you can iterate locally as long as you add model size and latency to your metrics and avoid regression. You should treat tuning the model for your device as an additional risk in the deployment cycle and test it accordingly. In other words, you should always test your models on production hardware before deploying them for real. Since models can be finicky, it\u2019s a good idea to build fallback mechanisms into the application if the model fails or is too slow.","title":"Mindset For Edge Deployment"},{"location":"spring2021/lecture-11/#takeaways_1","text":"Web deployment is easier, so only perform edge deployment if you need to. You should choose your framework to match the available hardware and corresponding mobile frameworks. Else, you can try Apache TVM to be more flexible. You should start considering hardware constraints at the beginning of the project and choose the architectures accordingly.","title":"Takeaways"},{"location":"spring2021/lecture-11/#ii-model-monitoring","text":"Once you deploy models, how do you make sure they are staying healthy and working well? Enter model monitoring. Many things can go wrong with a model once it\u2019s been trained. This can happen even if your model has been trained properly, with a reasonable validation and test loss, as well as robust performance across various slices and quality predictions. Even after you\u2019ve troubleshot and tested a model, things can still go wrong!","title":"II - Model Monitoring"},{"location":"spring2021/lecture-11/#1-why-model-degrades-post-deployment","text":"Model performance tends to degrade after you\u2019ve deployed a model. Why does this occur? In supervised learning, we seek to fit a function f to approximate a posterior using the data available to us. If any component of this process changes (i.e., the data x), the deployed model can see an unexpectedly degraded performance. See the below chart for examples of how such post-deployment degradations can occur theoretically and in practice: In summary, there are three core ways that the model\u2019s performance can degrade: data drift , concept drift , and domain shift . In data drift, the underlying data expectation that your model is built can unexpectedly change, perhaps through a bug in the upstream data pipeline or even due to malicious users feeding the model bad data. In concept drift, the actual outcome you seek to model, or the relationship between the data and the outcome, may fray. For example, users may start to pick movies in a different manner based on the output of your model, thereby changing the fundamental \u201cconcept\u201d the model needs to approximate. Finally, in domain shift, if your dataset does not appropriately sample the production, post-deployment setting, the model\u2019s performance may suffer; this could be considered a \u201clong tail\u201d scenario, where many rare examples that are not present in the development data occur.","title":"1 - Why Model Degrades Post-Deployment?"},{"location":"spring2021/lecture-11/#2-data-drift","text":"There are a few different types of data drift: Instantaneous drift : In this situation, the paradigm of the draft dramatically shifts. Examples are deploying the model in a new domain (e.g., self-driving car model in a new city), a bug in the preprocessing pipeline, or even major external shifts like COVID. Gradual drift : In this situation, the value of data gradually changes with time. For example, users\u2019 preferences may change over time, or new concepts can get introduced to the domain. Periodic drift : Data can have fluctuating value due to underlying patterns like seasonality or time zones. Temporary drift : The most difficult to detect, drift can occur through a short-term change in the data that shifts back to normal. This could be via a short-lived malicious attack, or even simply because a user with different demographics or behaviors uses your product in a way that it\u2019s not designed to be used. While these categories may seem like purely academic categories, the consequences of data shift are very real . This is a real problem that affects many companies and is only now starting to get the attention it merits.","title":"2 - Data Drift"},{"location":"spring2021/lecture-11/#3-what-should-you-monitor","text":"There are four core types of signals to monitor for machine learning models. These metrics trade off with another in terms of how informative they are and how easy they are to access. Put simply, the harder a metric may be to monitor, the more useful it likely is. The hardest and best metrics to monitor are model performance metrics , though these can be difficult to acquire in real-time (labels are hard to come by). Business metrics can be helpful signals of model degradation in monitoring but can easily be confounded by other impactful considerations. Model inputs and predictions are a simple way to identify high-level drift and are very easy to gather. Still, they can be difficult to assess in terms of actual performance impact, leaving it more of an art than science. Finally, system performance (e.g., GPU usage) can be a coarse method of catching serious bugs. In considering which metrics to focus on, prioritize ground-truth metrics (model and business metrics), then approximate performance metrics (business and input/outputs), and finally, system health metrics.","title":"3 - What Should You Monitor?"},{"location":"spring2021/lecture-11/#4-how-do-you-measure-distribution-changes","text":"","title":"4 - How Do You Measure Distribution Changes?"},{"location":"spring2021/lecture-11/#select-a-reference-window","text":"To measure distribution changes in metrics you\u2019re monitoring, start by picking a reference set of production data to compare new data to. There are a few different ways of picking this reference data (e.g., sliding window or fixed window of production data), but the most practical thing to do is to use your training or evaluation data as the reference . Data coming in looking different from what you developed your model using is an important signal to act on.","title":"Select A Reference Window"},{"location":"spring2021/lecture-11/#select-a-measurement-window","text":"After picking a reference window, the next step is to choose a measurement window to compare, measure distance, and evaluate for drift. The challenge is that selecting a measurement window is highly problem-dependent. One solution is to pick one or several window sizes and slide them over the data . To avoid recomputing metrics over and over again, when you slide the window, it\u2019s worth looking into the literature on mergeable (quantile) sketching algorithms .","title":"Select A Measurement Window"},{"location":"spring2021/lecture-11/#compare-windows-using-a-distance-metric","text":"What distance metrics should we use to compare the reference window to the measurement window? Some 1-D metric categories are: Rule-based distance metrics (e.g., data quality): Summary statistics, the volume of data points, number of missing values, or more complex tests like overall comparisons are common data quality checks that can be applied. Great Expectations is a valuable library for this. Definitely invest in simple rule-based metrics. They catch a large number of bugs, as publications from Amazon and Google detail. Statistical distance metrics (e.g., KS statistics, KL divergence, D_1 distance, etc.) KL Divergence : Defined as the expectation of a ratio of logs of two different distributions, this commonly known metric is very sensitive to what happens in the tails of the distribution. It\u2019s not well-suited to data shift testing since it\u2019s easily disturbed, is not interpretable, and struggles with data in different ranges. KS Statistic : This metric is defined as the max distance between CDFs, which is easy to interpret and is thus used widely in practice. Say yes to the KS statistic! D1 Distance : Defined as the sum of distances between PDFs, this is a metric used at Google. Despite seeming less principled, it\u2019s easily interpretable and has the added benefit of knowing Google uses it (so why not you?). An open area of research is understanding the impact of differing drift patterns on distance metrics and model performance. Another open area of research is high-dimensional distance metrics. Some options here are: Maximum mean discrepancy Performing multiple 1D comparisons across the data: While suffering from the multiple hypothesis testing problem , this is a practical approach. Prioritize some features for 1D comparisons: Another option is to avoid testing all the features and only focus on those that merit comparison; for example, those features you know may have shifted in the data. Projections : In this approach, large data points are put through a dimensionality reduction process and then subject to a two-sample statistical test. Reducing the dimensionality with a domain-specific approach (e.g., mean pixel value for images, length of sentence) is recommended. At a high level, this entire distance metric work aims to identify not just a score for any data shift but also understand its impact on the model. While choosing a metric can be complicated with all the possible options, you should focus on understanding your model\u2019s robustness in a post-deployment scenario.","title":"Compare Windows Using A Distance Metric"},{"location":"spring2021/lecture-11/#5-how-do-you-tell-if-a-change-is-bad","text":"There\u2019s no hard and fast rule for finding if a change in the data is bad. An easy option is to set thresholds on the test values. Don\u2019t use a statistical test like the KS test, as they are too sensitive to small shifts. Other options include setting manual ranges, comparing values over time, or even applying an unsupervised model to detect outliers. In practice, fixed rules and specified ranges of test values are used most in practice.","title":"5 - How Do You Tell If A Change Is Bad?"},{"location":"spring2021/lecture-11/#6-tools-for-monitoring","text":"There are three categories of tools useful for monitoring: System monitoring tools like AWS CloudWatch , Datadog , New Relic , and honeycomb test traditional performance metrics Data quality tools like Great Expectations , Anomalo , and Monte Carlo test if specific windows of data violate rules or assumptions. ML monitoring tools like Arize , Fiddler , and Arthur can also be useful, as they specifically test models.","title":"6 - Tools For Monitoring"},{"location":"spring2021/lecture-11/#7-evaluation-store","text":"Monitoring is more central to ML than for traditional software . In traditional SWE, most bugs cause loud failures, and the data that is monitored is most valuable to detect and diagnose problems. If the system is working well, the data from these metrics and monitoring systems may not be useful. In machine learning, however, monitoring plays a different role. First off, bugs in ML systems often lead to silent degradations in performance. Furthermore, the data that is monitored in ML is literally the code used to train the next iteration of models. Because monitoring is so essential to ML systems, tightly integrating it into the ML system architecture brings major benefits. In particular, better integrating and monitoring practices, or creating an evaluation store, can close the data flywheel loop , a concept we talked about earlier in the class. As we build models, we create a mapping between data and model. As the data changes and we retrain models, monitoring these changes doesn\u2019t become an endpoint--it becomes a part of the entire model development process. Monitoring, via an evaluation store, should touch all parts of your stack. One challenge that this process helps solve is effectively choosing which data points to collect, store, and label. Evaluation stores can help identify which data to collect more points for based on uncertain performance. As more data is collected and labeled, efficient retraining can be performed using evaluation store guidance.","title":"7 - Evaluation Store"},{"location":"spring2021/lecture-11/#conclusion","text":"In summary, make sure to monitor your models! Something will always go wrong, and you should have a system to catch errors. Start by looking at data quality metrics and system metrics, as they are easiest. In a perfect world, the testing and monitoring should be linked, and they should help you close the data flywheel. There will be a lot of tooling and research that will hopefully come soon!","title":"Conclusion"},{"location":"spring2021/lecture-12/","text":"Lecture 12: Research Directions Video Slides Download slides as PDF Notes Notes were taken by James Le and Vishnu Rachakonda Download notes as PDF Of all disciplines, deep learning is probably the one where research and practice are closest together . Often, something gets invented in research and is put into production in less than a year. Therefore, it\u2019s good to be aware of research trends that you might want to incorporate in projects you are working on. Because the number of ML and AI papers increases exponentially, there\u2019s no way that you can read every paper. Thus, you need other methods to keep up with research. This lecture provides a sampling of research directions, the overall research theme running across these samples, and advice on keeping up with the relentless flood of new research. 1 - Unsupervised Learning Deep supervised learning, the default way of doing ML, works! But it requires so much annotated data. Can we get around it by learning with fewer labels? The answer is yes! And there are two major approaches: deep semi-supervised learning and deep unsupervised learning. Deep Semi-Supervised Learning Semi-supervised means half supervised, half unsupervised. Assuming a classification problem where each data point belongs to one of the classes, we attempt to come up with an intuition to complete the labeling for the unlabeled data points. One way to formalize this is: If anything is close to a labeled example, then it will assume that label. Thus, we can propagate the labels out from where they are given to the neighboring data points. How can we generalize the approach above to image classification? Xie et al. (2020) proposes Noisy Student Training : First, they train a teacher model with labeled data. Then, they infer pseudo-labels on the unlabeled data. These are not real labels, but those that they get from using the trained teacher model. Even though these labels are not perfect (because they train on a small amount of labeled data), they can still see where they are more confident about those pseudo labels and inject those into their training set as additional labeled data. When they retrain, they use dropout, data augmentation, and stochastic depth to inject noise into the training process. This enables the student model to be more robust and generalizable. Deep Unsupervised Learning Deep semi-supervised learning assumes that the labels in the supervised dataset are still valid for the unsupervised dataset. There\u2019s a limit to the applicability because we assume that the unlabeled data is roughly from the same distribution as the labeled data . With deep unsupervised learning, we can transfer the learning with multi-headed networks . First, we train a neural network. Then, we have two tasks and give the network two heads - one for task 1 and another for task 2. Most parameters live in the shared trunk of the network\u2019s body. Thus, when you train for task 1 and task 2, most of the learnings are shared. Only a little bit gets specialized to task 1 versus task 2. The key hypothesis here is that: For task 1 (which is unsupervised), if the neural network is smart enough to do things like predicting the next word in a sentence, generating realistic images, or translating images from one scale to another; then that same neural network is ready to do deep supervised learning from a very small dataset for task 2 (what we care about). GPT-2 For instance, task 1 could be predicting the next word in a sentence, while task 2 could be predicting the sentiment in a corpus. OpenAI\u2019s GPT-2 is the landmark result for next-word prediction where deep unsupervised learning could work. The results were so realistic, and there was a lot of press coverage. OpenAI deemed it to be too dangerous to be released at the time. Furthermore, GPT-2 can tackle complex common sense reasoning and question answering tasks for various benchmarks. The table below displays those benchmarks where GPT-2 was evaluated on. The details of the tasks do not really matter. What\u2019s more interesting is that: This is the first time a model, trained unsupervised on a lot of text to predict the next token and fine-tuned to specific supervised tasks, beats prior methods that might have been more specialized to each of these supervised tasks . Another fascinating insight is that as we grow the number of model parameters, the performance goes up consistently. This means with unsupervised learning, we can incorporate much more data for larger models . This research funding inspired OpenAI to fundraise $1B for future projects to essentially have more compute available to train larger models because it seems like doing that will lead to better results. So far, that has been true ( GPT-3 performs better than GPT-2). BERT BERT is Google\u2019s approach that came out around the same time as GPT-2. While GPT-2 predicts the next word or token, BERT predicts a word or token that was removed. In this task, the neural network looks at the entire corpus as it fills things back in, which often helps in later tasks (as the neural network has already been unsupervised-train on the entire text). The table below displays BERT\u2019s performance on the GLUE benchmark . The takeaway message is not so much in the details of these supervised tasks; but the fact that these tasks have a relatively small amount of labeled data compared to the unsupervised training that happens ahead of time. As BERT outperformed all SOTA methods, it revolutionized how natural language processing should be done. BERT is one of the biggest updates that Google has made since RankBrain in 2015 and has proven successful in comprehending the intent of the searcher behind a search query. Unsupervised Learning In Vision Can we do the same thing for vision tasks? Let\u2019s explore a few of them. Predict A Missing Patch: A patch is high-dimensional, so the number of possibilities in that patch is very high (much larger than the number of words in English, for instance). Therefore, it\u2019s challenging to predict precisely and make that work as well as in languages. Solve Jigsaw Puzzles: If the network can do this, it understands something about images of the world. The trunk of the network should hopefully be reusable. Predict Rotation: Here, you collect random images and predict what degree has been rotated. Existing methods work immensely well for such a task. A technique that stood out in recent times is contrastive learning , which includes two variants - SimCLR (Chen et al., 2020) and MoCo (He et al., 2019). Here\u2019s how you train your model with contrastive learning: Imagine that you download two images of a dog and a cat from the Internet, and you don\u2019t have labels yet. You duplicate the dog image and make two versions of it (a greyscale version and a cropped version). For these two dog versions, the neural network should bring them together while pushing the cat image far away. You then fine-tune with a simple linear classifier on top of training completely unsupervised. This means that you must get the right features extracted from the images during training. The results of contrastive learning methods confirm that the higher the number of model parameters, the better the accuracy. 2 - Reinforcement Learning Reinforcement learning (RL) has not been practical yet but nevertheless has shown promising results. In RL, the AI is an agent, more so than just a pattern recognizer. The agent acts in an environment where it is goal-oriented. It wants to achieve something during the process, which is represented by a reward function. Challenges Compared to unsupervised learning, RL brings about a host of additional challenges: Credit assignment: When the RL agent sees something, it has to take action. But it is not told whether the action was good or bad right away. Stability: Because the RL agent learns by trial and error, it can destabilize and make big mistakes. Thus, it needs to be clever in updating itself not to destroy things along the way. Exploration: The RL agent has to try things that have not been done before. Despite these challenges, some great RL successes have happened. Successes DeepMind has shown that neural networks can learn to play the Atari game back in 2013. Under the hood is the Deep Q-Network architecture, which was trained from its own trial-and-error, looking at the score in the game to internalize what actions might be good or bad. The game of Go was cracked by DeepMind - showing that the computer can play better than the best human player ( AlphaGo , AlphaGoZero , and AlphaZero ). RL also works for the robot locomotion task. You don\u2019t have to design the controller yourself. You just implement the RL algorithm ( TRPO , GAE , DDPG , PPO , and more) and let the agent train itself, which is a general approach to have AI systems acquire new skills. In fact, the robot can acquire such a variety of skills, as demonstrated in this DeepMimic work. You can also accomplish the above for non-human-like characters in dynamic animation tasks. This is going to change how you can design video games or animated movies. Instead of designing the keyframes for every step along the way in your video or your game, you can train an agent to go from point A to point B directly. RL has been shown to work on real robots . BRETT (Berkeley Robot for the Elimination of Tedious Tasks) could learn to put blocks into matching openings in under an hour using a neural network trained from scratch. This technique has been used for NASA SuperBall robots for space exploration ideas. A similar idea was applied to robotic manipulation solving Rubik\u2019s cube , done at OpenAI in 2019. The in-hand manipulation is a very difficult robotic control problem that was mastered with RL. CovariantAI The fact that RL worked so well actually inspired Pieter and his former students (Tianhao Zhang, Rocky Duan, and Peter Chen) to start a company called Covariant in 2017. Their goal is to bring these advances from the lab into the real world. An example is autonomous order picking . 3 - Unsupervised Reinforcement Learning RL achieved mastery on many simulated domains. But we must ask the question: How fast is the learning itself? Tsividis et al., 2017 shows that a human can learn in about 15 minutes to perform better than Double DQN (a SOTA approach at the time of the study) learned after 115 hours. How can we bridge this learning gap? Based on the 2018 DeepMind Control Suite , pixel-based learning needs 50M more training steps than state-based learning to solve the same tasks. Maybe we can develop an unsupervised learning approach to turn pixel-level representations (which are not that informative) into a new representation that is much more similar to the underlying state. CURL brings together contrastive learning and RL. In RL, there\u2019s typically a replay buffer where we store the past experiences. We load observations from there and feed them into an encoder neural network. The network has two heads: an actor to estimate the best action to take next and a critic to estimate how good that action would be. CURL adds an extra head at the bottom, which includes augmented observations, and does contrastive learning on that. Similar configurations of the robot are brought closer together, while different ones are separated. The results confirm that CURL can match existing SOTA approaches that learn from states and from pixels. However, it struggles in hard environments, with insufficient labeled images being the root cause. 4 - Meta Reinforcement Learning The majority of fully general RL algorithms work well for any environments that can be mathematically defined. However, environments encountered in the real world are a tiny subset of all environments that could be defined. Maybe the learning takes such a long time because the algorithms are too general. If they are a bit more specialized in things they will encounter, perhaps the learning is faster. Can we develop a fast RL algorithm to take advantage of this? In traditional RL research, human experts develop the RL algorithm. However, there are still no RL algorithms nearly as good as humans after many years. Can we learn a better RL algorithm? Or even learn a better entire agent? RL^2 RL^2 ( Duan et al., 2016 ) is a meta-RL framework proposed to tackle this issue: Imagine that we have multiple meta-training environments (A, B, and so on). We also have a meta-RL algorithm that learns the RL algorithm and outputs a \u201cfast\u201d RL agent (from having interacted with these environments). In the future, our agent will be in an environment F that is related to A, B, and so on. Formally speaking, RL^2 maximizes the expected reward on the training Markov Decision Process (MDP) but can generalize to testing MDP. The RL agent is represented as a Recurrent Neural Network (RNN), a generic computation architecture where: Different weights in the RNN mean different RL algorithms and priors. Different activations in the RNN mean different current policies. The meta-trained objective can be optimized with an existing \u201cslow\u201d RL algorithm. The resulting RNN is ready to be dropped in a new environment. RL^2 was evaluated on a classic Multi-Armed Bandit setting and performed better than provably (asymptotically) optimal RL algorithms invented by humans like Gittings Index, UCB1, and Thompson Sampling. Another task that RL^2 was evaluated on is visual navigation , where the agent explores a maze and finds a specified target as quickly as possible. Although this setting is maze-specific, we can scale up RL^2 to other large-scale games and robotic environments and use it to learn in a new environment quickly. Learn More Schmidhuber. Evolutionary principles in self-referential learning . (1987) Wiering, Schmidhuber. Solving POMDPs with Levin search and EIRA . (1996) Schmidhuber, Zhao, Wiering. Shifting inductive bias with success-story algorithm, adaptive Levin search, and incremental self-improvement . (MLJ 1997) Schmidhuber, Zhao, Schraudolph. Reinforcement learning with self-modifying policies (1998) Zhao, Schmidhuber. Solving a complex prisoner\u2019s dilemma with self-modifying policies . (1998) Schmidhuber. A general method for incremental self-improvement and multiagent learning . (1999) Singh, Lewis, Barto. Where do rewards come from? (2009) Singh, Lewis, Barto. Intrinsically Motivated Reinforcement Learning: An Evolutionary Perspective (2010) Niekum, Spector, Barto. Evolution of reward functions for reinforcement learning (2011) Wang et al., (2016). Learning to Reinforcement Learn Finn et al., (2017). Model-Agnostic Meta-Learning (MAML) Mishra, Rohinenjad et al., (2017). Simple Neural AttentIve Meta-Learner Frans et al., (2017). Meta-Learning Shared Hierarchies 5 - Few-Shot Imitation Learning People often complement RL with imitation learning , which is basically supervised learning where the output is an action for an agent. This gives you more signal than traditional RL since for every input, you consistently have a corresponding output. As the diagram below shows, the imitation learning algorithm learns a policy in a supervised manner from many demonstrations and outputs the correct action based on the environment. The challenge for imitation learning is to collect enough demonstrations to train an algorithm , which is time-consuming. To make the collection of demonstrations more efficient, we can apply multi-task meta-learning. Many demonstrations for different tasks can be learned by an algorithm, whose output is fed to a one-shot imitator that picks the correct action based on a single demonstration. This process is referred to as one-shot imitation learning ( Duan et al., 2017 ), as displayed below. Conveniently, one-shot imitators are trained using traditional network architectures. A combination of CNNs, RNNs, and MLPs perform the heavy visual processing to understand the relevant actions in training demos and recommend the right action for the current frame of an inference demo. One example of this in action is block stacking . Learn More Abbeel et al., (2008). Learning For Control From Multiple Demonstrations Kolter, Ng. The Stanford LittleDog: A Learning And Rapid Replanning Approach To Quadrupled Locomotion (2008) Ziebart et al., (2008). Maximum Entropy Inverse Reinforcement Learning Schulman et al., (2013). Motion Planning with Sequential Convex Optimization and Convex Collision Checking Finn, Levine. Deep Visual Foresight for Planning Robot Motion (2016) 6 - Domain Randomization Simulated data collection is a logical substitute for expensive real data collection. It is less expensive, more scalable, and less dangerous (e.g., in the case of robots) to capture at scale. Given this logic, how can we make sure simulated data best matches real-world conditions? Use Realistic Simulated Data One approach is to make the simulator you use for training models as realistic as possible. Two variants of doing this are to carefully match the simulation to the world ( James and John, 2016 ; Johns, Leutenegger, and Division, 2016 ; Mahler et al., 2017 ; Koenemann et al., 2015 ) and augment simulated data with real data ( Richter et al., 2016 ; Bousmalis et al., 2017 ). While this option is logically appealing, it can be hard and slow to do in practice. Domain Confusion Another option is domain confusion ( Tzeng et al., 2014 ; Rusu et al., 2016 ). In this approach, suppose you train a model on real and simulated data at the same time. After completing training, a discriminator network examines the original network at some layer to understand if the original network is learning something about the real world. If you can fool the discriminator with the output of the layer, the original network has completely integrated its understanding of real and simulated data. In effect, there is no difference between simulated and real data to the original network, and the layers following the examined layer can be trained fully on simulated data. Domain Randomization Finally, a simpler approach called domain randomization ( Tobin et al., 2017 ; Sadeghi and Levine, 2016 ) has taken off of late. In this approach, rather than making simulated data fully realistic, the priority is to generate as much variation in the simulated data as possible. For example, in the below tabletop scenes, the dramatic variety of the scenes (e.g., background colors of green and purple) can help the model generalize well to the real world, even though the real world looks nothing like these scenes. This approach has shown promise in drone flight and pose estimation . The simple logic of more data leading to better performance in real-world settings is powerfully illustrated by domain randomization and obviates the need for existing variation methods like pre-training on ImageNet. 7 - Deep Learning For Science and Engineering AlphaFold In other areas of this lecture, we\u2019ve been focusing on research areas of machine learning where humans already perform well (i.e., pose estimation or grasping). In science and engineering applications, we enter the realm of machine learning performing tasks humans cannot. The most famous result is AlphaFold , a Deepmind-created system that solved protein folding, an important biological challenge. In the CASP challenge, AlphaFold 2 far outpaced all other results in performance. AlphaFold is quite complicated, as it maps an input protein sequence to similar protein sequences and subsequently decides the folding structure based on the evolutionary history of complementary amino acids. Other examples of DL systems solving science and engineering challenges are in circuit design , high-energy physics , and symbolic mathematics . Learn More AlphaFold: Improved protein structure prediction using potentials from deep learning . Deepmind (Senior et al.) BagNet: Berkeley Analog Generator with Layout Optimizer Boosted with Deep Neural Networks . K. Hakhamaneshi, N. Werblun, P. Abbeel, V. Stojanovic. IEEE/ACM International Conference on Computer-Aided Design (ICAD), Westminster, Colorado, November 2019. Evaluating Protein Transfer Learning with TAPE . R. Rao, N. Bhattacharya, N. Thomas, Y, Duan, X. Chen, J. Canny, P. Abbeel, Y. Song. Opening the black box: the anatomy of a deep learning atomistic potential . Justin Smith Exploring Machine Learning Applications to Enable Next-Generation Chemistry . Jennifer Wei (Google). GANs for HEP . Ben Nachman Deep Learning for Symbolic Mathematics . G. Lample and F. Charton. A Survey of Deep Learning for Scientific Discovery . Maithra Raghu, Eric Schmidt. 8 - Overarching Research Theme As compute scales to support incredible numbers of FLOPs, more science and engineering challenges will be solved with deep learning systems. There has been exponential growth in the amount of compute used to generate the most impressive research results like GPT-3. As compute and data become more available, we open a new problem territory that we can refer to as deep learning to learn . More specifically, throughout history, the constraint on solving problems has been human ingenuity. This is a particularly challenging realm to contribute novel results to because we\u2019re competing against the combined intellectual might available throughout history. Is our present ingenuity truly greater than that of others 20-30 years ago, let alone 200-300? Probably not. However, our ability to bring new tools like compute and data most certainly is. Therefore, spending as much time in this new problem territory, where data and compute help solve problems , is likely to generate exciting and novel results more frequently in the long run. 9 - How To Keep Up \u201c Give a man a fish and you feed him for a day, teach a man to fish and you feed him for a lifetime \u201d (Lao Tzu) Here are some tips on how to keep up with ML research: (Mostly) don\u2019t read (most) papers. There are just too many! When you do want to keep up, use the following: Tutorials at conferences: these capture the essence of important concepts in a practical, distilled way Graduate courses and seminars Yannic Kilcher YouTube channel Two Minutes Paper Channel The Batch by Andrew Ng Import AI by Jack Clark If you DO decide to read papers, Follow a principled process for reading papers Use Arxiv Sanity Twitter AI/DL Facebook Group ML Subreddit Start a reading group: read papers together with friends - either everyone reads then discusses, or one or two people read and give tutorials to others. Finally, should you do a Ph.D. or not? You don\u2019t have to do a Ph.D. to work in AI! However, if you REALLY want to become one of the world\u2019s experts in a topic you care about, then a Ph.D. is a technically deep and demanding path to get there. Crudely speaking, a Ph.D. enables you to develop new tools and techniques rather than using existing tools and techniques.","title":"Lecture 12: Research Directions"},{"location":"spring2021/lecture-12/#lecture-12-research-directions","text":"","title":"Lecture 12: Research Directions"},{"location":"spring2021/lecture-12/#video","text":"","title":"Video"},{"location":"spring2021/lecture-12/#slides","text":"Download slides as PDF","title":"Slides"},{"location":"spring2021/lecture-12/#notes","text":"Notes were taken by James Le and Vishnu Rachakonda Download notes as PDF Of all disciplines, deep learning is probably the one where research and practice are closest together . Often, something gets invented in research and is put into production in less than a year. Therefore, it\u2019s good to be aware of research trends that you might want to incorporate in projects you are working on. Because the number of ML and AI papers increases exponentially, there\u2019s no way that you can read every paper. Thus, you need other methods to keep up with research. This lecture provides a sampling of research directions, the overall research theme running across these samples, and advice on keeping up with the relentless flood of new research.","title":"Notes"},{"location":"spring2021/lecture-12/#1-unsupervised-learning","text":"Deep supervised learning, the default way of doing ML, works! But it requires so much annotated data. Can we get around it by learning with fewer labels? The answer is yes! And there are two major approaches: deep semi-supervised learning and deep unsupervised learning.","title":"1 - Unsupervised Learning"},{"location":"spring2021/lecture-12/#deep-semi-supervised-learning","text":"Semi-supervised means half supervised, half unsupervised. Assuming a classification problem where each data point belongs to one of the classes, we attempt to come up with an intuition to complete the labeling for the unlabeled data points. One way to formalize this is: If anything is close to a labeled example, then it will assume that label. Thus, we can propagate the labels out from where they are given to the neighboring data points. How can we generalize the approach above to image classification? Xie et al. (2020) proposes Noisy Student Training : First, they train a teacher model with labeled data. Then, they infer pseudo-labels on the unlabeled data. These are not real labels, but those that they get from using the trained teacher model. Even though these labels are not perfect (because they train on a small amount of labeled data), they can still see where they are more confident about those pseudo labels and inject those into their training set as additional labeled data. When they retrain, they use dropout, data augmentation, and stochastic depth to inject noise into the training process. This enables the student model to be more robust and generalizable.","title":"Deep Semi-Supervised Learning"},{"location":"spring2021/lecture-12/#deep-unsupervised-learning","text":"Deep semi-supervised learning assumes that the labels in the supervised dataset are still valid for the unsupervised dataset. There\u2019s a limit to the applicability because we assume that the unlabeled data is roughly from the same distribution as the labeled data . With deep unsupervised learning, we can transfer the learning with multi-headed networks . First, we train a neural network. Then, we have two tasks and give the network two heads - one for task 1 and another for task 2. Most parameters live in the shared trunk of the network\u2019s body. Thus, when you train for task 1 and task 2, most of the learnings are shared. Only a little bit gets specialized to task 1 versus task 2. The key hypothesis here is that: For task 1 (which is unsupervised), if the neural network is smart enough to do things like predicting the next word in a sentence, generating realistic images, or translating images from one scale to another; then that same neural network is ready to do deep supervised learning from a very small dataset for task 2 (what we care about).","title":"Deep Unsupervised Learning"},{"location":"spring2021/lecture-12/#gpt-2","text":"For instance, task 1 could be predicting the next word in a sentence, while task 2 could be predicting the sentiment in a corpus. OpenAI\u2019s GPT-2 is the landmark result for next-word prediction where deep unsupervised learning could work. The results were so realistic, and there was a lot of press coverage. OpenAI deemed it to be too dangerous to be released at the time. Furthermore, GPT-2 can tackle complex common sense reasoning and question answering tasks for various benchmarks. The table below displays those benchmarks where GPT-2 was evaluated on. The details of the tasks do not really matter. What\u2019s more interesting is that: This is the first time a model, trained unsupervised on a lot of text to predict the next token and fine-tuned to specific supervised tasks, beats prior methods that might have been more specialized to each of these supervised tasks . Another fascinating insight is that as we grow the number of model parameters, the performance goes up consistently. This means with unsupervised learning, we can incorporate much more data for larger models . This research funding inspired OpenAI to fundraise $1B for future projects to essentially have more compute available to train larger models because it seems like doing that will lead to better results. So far, that has been true ( GPT-3 performs better than GPT-2).","title":"GPT-2"},{"location":"spring2021/lecture-12/#bert","text":"BERT is Google\u2019s approach that came out around the same time as GPT-2. While GPT-2 predicts the next word or token, BERT predicts a word or token that was removed. In this task, the neural network looks at the entire corpus as it fills things back in, which often helps in later tasks (as the neural network has already been unsupervised-train on the entire text). The table below displays BERT\u2019s performance on the GLUE benchmark . The takeaway message is not so much in the details of these supervised tasks; but the fact that these tasks have a relatively small amount of labeled data compared to the unsupervised training that happens ahead of time. As BERT outperformed all SOTA methods, it revolutionized how natural language processing should be done. BERT is one of the biggest updates that Google has made since RankBrain in 2015 and has proven successful in comprehending the intent of the searcher behind a search query.","title":"BERT"},{"location":"spring2021/lecture-12/#unsupervised-learning-in-vision","text":"Can we do the same thing for vision tasks? Let\u2019s explore a few of them. Predict A Missing Patch: A patch is high-dimensional, so the number of possibilities in that patch is very high (much larger than the number of words in English, for instance). Therefore, it\u2019s challenging to predict precisely and make that work as well as in languages. Solve Jigsaw Puzzles: If the network can do this, it understands something about images of the world. The trunk of the network should hopefully be reusable. Predict Rotation: Here, you collect random images and predict what degree has been rotated. Existing methods work immensely well for such a task. A technique that stood out in recent times is contrastive learning , which includes two variants - SimCLR (Chen et al., 2020) and MoCo (He et al., 2019). Here\u2019s how you train your model with contrastive learning: Imagine that you download two images of a dog and a cat from the Internet, and you don\u2019t have labels yet. You duplicate the dog image and make two versions of it (a greyscale version and a cropped version). For these two dog versions, the neural network should bring them together while pushing the cat image far away. You then fine-tune with a simple linear classifier on top of training completely unsupervised. This means that you must get the right features extracted from the images during training. The results of contrastive learning methods confirm that the higher the number of model parameters, the better the accuracy.","title":"Unsupervised Learning In Vision"},{"location":"spring2021/lecture-12/#2-reinforcement-learning","text":"Reinforcement learning (RL) has not been practical yet but nevertheless has shown promising results. In RL, the AI is an agent, more so than just a pattern recognizer. The agent acts in an environment where it is goal-oriented. It wants to achieve something during the process, which is represented by a reward function.","title":"2 - Reinforcement Learning"},{"location":"spring2021/lecture-12/#challenges","text":"Compared to unsupervised learning, RL brings about a host of additional challenges: Credit assignment: When the RL agent sees something, it has to take action. But it is not told whether the action was good or bad right away. Stability: Because the RL agent learns by trial and error, it can destabilize and make big mistakes. Thus, it needs to be clever in updating itself not to destroy things along the way. Exploration: The RL agent has to try things that have not been done before. Despite these challenges, some great RL successes have happened.","title":"Challenges"},{"location":"spring2021/lecture-12/#successes","text":"DeepMind has shown that neural networks can learn to play the Atari game back in 2013. Under the hood is the Deep Q-Network architecture, which was trained from its own trial-and-error, looking at the score in the game to internalize what actions might be good or bad. The game of Go was cracked by DeepMind - showing that the computer can play better than the best human player ( AlphaGo , AlphaGoZero , and AlphaZero ). RL also works for the robot locomotion task. You don\u2019t have to design the controller yourself. You just implement the RL algorithm ( TRPO , GAE , DDPG , PPO , and more) and let the agent train itself, which is a general approach to have AI systems acquire new skills. In fact, the robot can acquire such a variety of skills, as demonstrated in this DeepMimic work. You can also accomplish the above for non-human-like characters in dynamic animation tasks. This is going to change how you can design video games or animated movies. Instead of designing the keyframes for every step along the way in your video or your game, you can train an agent to go from point A to point B directly. RL has been shown to work on real robots . BRETT (Berkeley Robot for the Elimination of Tedious Tasks) could learn to put blocks into matching openings in under an hour using a neural network trained from scratch. This technique has been used for NASA SuperBall robots for space exploration ideas. A similar idea was applied to robotic manipulation solving Rubik\u2019s cube , done at OpenAI in 2019. The in-hand manipulation is a very difficult robotic control problem that was mastered with RL.","title":"Successes"},{"location":"spring2021/lecture-12/#covariantai","text":"The fact that RL worked so well actually inspired Pieter and his former students (Tianhao Zhang, Rocky Duan, and Peter Chen) to start a company called Covariant in 2017. Their goal is to bring these advances from the lab into the real world. An example is autonomous order picking .","title":"CovariantAI"},{"location":"spring2021/lecture-12/#3-unsupervised-reinforcement-learning","text":"RL achieved mastery on many simulated domains. But we must ask the question: How fast is the learning itself? Tsividis et al., 2017 shows that a human can learn in about 15 minutes to perform better than Double DQN (a SOTA approach at the time of the study) learned after 115 hours. How can we bridge this learning gap? Based on the 2018 DeepMind Control Suite , pixel-based learning needs 50M more training steps than state-based learning to solve the same tasks. Maybe we can develop an unsupervised learning approach to turn pixel-level representations (which are not that informative) into a new representation that is much more similar to the underlying state. CURL brings together contrastive learning and RL. In RL, there\u2019s typically a replay buffer where we store the past experiences. We load observations from there and feed them into an encoder neural network. The network has two heads: an actor to estimate the best action to take next and a critic to estimate how good that action would be. CURL adds an extra head at the bottom, which includes augmented observations, and does contrastive learning on that. Similar configurations of the robot are brought closer together, while different ones are separated. The results confirm that CURL can match existing SOTA approaches that learn from states and from pixels. However, it struggles in hard environments, with insufficient labeled images being the root cause.","title":"3 - Unsupervised Reinforcement Learning"},{"location":"spring2021/lecture-12/#4-meta-reinforcement-learning","text":"The majority of fully general RL algorithms work well for any environments that can be mathematically defined. However, environments encountered in the real world are a tiny subset of all environments that could be defined. Maybe the learning takes such a long time because the algorithms are too general. If they are a bit more specialized in things they will encounter, perhaps the learning is faster. Can we develop a fast RL algorithm to take advantage of this? In traditional RL research, human experts develop the RL algorithm. However, there are still no RL algorithms nearly as good as humans after many years. Can we learn a better RL algorithm? Or even learn a better entire agent?","title":"4 - Meta Reinforcement Learning"},{"location":"spring2021/lecture-12/#rl2","text":"RL^2 ( Duan et al., 2016 ) is a meta-RL framework proposed to tackle this issue: Imagine that we have multiple meta-training environments (A, B, and so on). We also have a meta-RL algorithm that learns the RL algorithm and outputs a \u201cfast\u201d RL agent (from having interacted with these environments). In the future, our agent will be in an environment F that is related to A, B, and so on. Formally speaking, RL^2 maximizes the expected reward on the training Markov Decision Process (MDP) but can generalize to testing MDP. The RL agent is represented as a Recurrent Neural Network (RNN), a generic computation architecture where: Different weights in the RNN mean different RL algorithms and priors. Different activations in the RNN mean different current policies. The meta-trained objective can be optimized with an existing \u201cslow\u201d RL algorithm. The resulting RNN is ready to be dropped in a new environment. RL^2 was evaluated on a classic Multi-Armed Bandit setting and performed better than provably (asymptotically) optimal RL algorithms invented by humans like Gittings Index, UCB1, and Thompson Sampling. Another task that RL^2 was evaluated on is visual navigation , where the agent explores a maze and finds a specified target as quickly as possible. Although this setting is maze-specific, we can scale up RL^2 to other large-scale games and robotic environments and use it to learn in a new environment quickly.","title":"RL^2"},{"location":"spring2021/lecture-12/#learn-more","text":"Schmidhuber. Evolutionary principles in self-referential learning . (1987) Wiering, Schmidhuber. Solving POMDPs with Levin search and EIRA . (1996) Schmidhuber, Zhao, Wiering. Shifting inductive bias with success-story algorithm, adaptive Levin search, and incremental self-improvement . (MLJ 1997) Schmidhuber, Zhao, Schraudolph. Reinforcement learning with self-modifying policies (1998) Zhao, Schmidhuber. Solving a complex prisoner\u2019s dilemma with self-modifying policies . (1998) Schmidhuber. A general method for incremental self-improvement and multiagent learning . (1999) Singh, Lewis, Barto. Where do rewards come from? (2009) Singh, Lewis, Barto. Intrinsically Motivated Reinforcement Learning: An Evolutionary Perspective (2010) Niekum, Spector, Barto. Evolution of reward functions for reinforcement learning (2011) Wang et al., (2016). Learning to Reinforcement Learn Finn et al., (2017). Model-Agnostic Meta-Learning (MAML) Mishra, Rohinenjad et al., (2017). Simple Neural AttentIve Meta-Learner Frans et al., (2017). Meta-Learning Shared Hierarchies","title":"Learn More"},{"location":"spring2021/lecture-12/#5-few-shot-imitation-learning","text":"People often complement RL with imitation learning , which is basically supervised learning where the output is an action for an agent. This gives you more signal than traditional RL since for every input, you consistently have a corresponding output. As the diagram below shows, the imitation learning algorithm learns a policy in a supervised manner from many demonstrations and outputs the correct action based on the environment. The challenge for imitation learning is to collect enough demonstrations to train an algorithm , which is time-consuming. To make the collection of demonstrations more efficient, we can apply multi-task meta-learning. Many demonstrations for different tasks can be learned by an algorithm, whose output is fed to a one-shot imitator that picks the correct action based on a single demonstration. This process is referred to as one-shot imitation learning ( Duan et al., 2017 ), as displayed below. Conveniently, one-shot imitators are trained using traditional network architectures. A combination of CNNs, RNNs, and MLPs perform the heavy visual processing to understand the relevant actions in training demos and recommend the right action for the current frame of an inference demo. One example of this in action is block stacking .","title":"5 - Few-Shot Imitation Learning"},{"location":"spring2021/lecture-12/#learn-more_1","text":"Abbeel et al., (2008). Learning For Control From Multiple Demonstrations Kolter, Ng. The Stanford LittleDog: A Learning And Rapid Replanning Approach To Quadrupled Locomotion (2008) Ziebart et al., (2008). Maximum Entropy Inverse Reinforcement Learning Schulman et al., (2013). Motion Planning with Sequential Convex Optimization and Convex Collision Checking Finn, Levine. Deep Visual Foresight for Planning Robot Motion (2016)","title":"Learn More"},{"location":"spring2021/lecture-12/#6-domain-randomization","text":"Simulated data collection is a logical substitute for expensive real data collection. It is less expensive, more scalable, and less dangerous (e.g., in the case of robots) to capture at scale. Given this logic, how can we make sure simulated data best matches real-world conditions?","title":"6 - Domain Randomization"},{"location":"spring2021/lecture-12/#use-realistic-simulated-data","text":"One approach is to make the simulator you use for training models as realistic as possible. Two variants of doing this are to carefully match the simulation to the world ( James and John, 2016 ; Johns, Leutenegger, and Division, 2016 ; Mahler et al., 2017 ; Koenemann et al., 2015 ) and augment simulated data with real data ( Richter et al., 2016 ; Bousmalis et al., 2017 ). While this option is logically appealing, it can be hard and slow to do in practice.","title":"Use Realistic Simulated Data"},{"location":"spring2021/lecture-12/#domain-confusion","text":"Another option is domain confusion ( Tzeng et al., 2014 ; Rusu et al., 2016 ). In this approach, suppose you train a model on real and simulated data at the same time. After completing training, a discriminator network examines the original network at some layer to understand if the original network is learning something about the real world. If you can fool the discriminator with the output of the layer, the original network has completely integrated its understanding of real and simulated data. In effect, there is no difference between simulated and real data to the original network, and the layers following the examined layer can be trained fully on simulated data.","title":"Domain Confusion"},{"location":"spring2021/lecture-12/#domain-randomization","text":"Finally, a simpler approach called domain randomization ( Tobin et al., 2017 ; Sadeghi and Levine, 2016 ) has taken off of late. In this approach, rather than making simulated data fully realistic, the priority is to generate as much variation in the simulated data as possible. For example, in the below tabletop scenes, the dramatic variety of the scenes (e.g., background colors of green and purple) can help the model generalize well to the real world, even though the real world looks nothing like these scenes. This approach has shown promise in drone flight and pose estimation . The simple logic of more data leading to better performance in real-world settings is powerfully illustrated by domain randomization and obviates the need for existing variation methods like pre-training on ImageNet.","title":"Domain Randomization"},{"location":"spring2021/lecture-12/#7-deep-learning-for-science-and-engineering","text":"","title":"7 - Deep Learning For Science and Engineering"},{"location":"spring2021/lecture-12/#alphafold","text":"In other areas of this lecture, we\u2019ve been focusing on research areas of machine learning where humans already perform well (i.e., pose estimation or grasping). In science and engineering applications, we enter the realm of machine learning performing tasks humans cannot. The most famous result is AlphaFold , a Deepmind-created system that solved protein folding, an important biological challenge. In the CASP challenge, AlphaFold 2 far outpaced all other results in performance. AlphaFold is quite complicated, as it maps an input protein sequence to similar protein sequences and subsequently decides the folding structure based on the evolutionary history of complementary amino acids. Other examples of DL systems solving science and engineering challenges are in circuit design , high-energy physics , and symbolic mathematics .","title":"AlphaFold"},{"location":"spring2021/lecture-12/#learn-more_2","text":"AlphaFold: Improved protein structure prediction using potentials from deep learning . Deepmind (Senior et al.) BagNet: Berkeley Analog Generator with Layout Optimizer Boosted with Deep Neural Networks . K. Hakhamaneshi, N. Werblun, P. Abbeel, V. Stojanovic. IEEE/ACM International Conference on Computer-Aided Design (ICAD), Westminster, Colorado, November 2019. Evaluating Protein Transfer Learning with TAPE . R. Rao, N. Bhattacharya, N. Thomas, Y, Duan, X. Chen, J. Canny, P. Abbeel, Y. Song. Opening the black box: the anatomy of a deep learning atomistic potential . Justin Smith Exploring Machine Learning Applications to Enable Next-Generation Chemistry . Jennifer Wei (Google). GANs for HEP . Ben Nachman Deep Learning for Symbolic Mathematics . G. Lample and F. Charton. A Survey of Deep Learning for Scientific Discovery . Maithra Raghu, Eric Schmidt.","title":"Learn More"},{"location":"spring2021/lecture-12/#8-overarching-research-theme","text":"As compute scales to support incredible numbers of FLOPs, more science and engineering challenges will be solved with deep learning systems. There has been exponential growth in the amount of compute used to generate the most impressive research results like GPT-3. As compute and data become more available, we open a new problem territory that we can refer to as deep learning to learn . More specifically, throughout history, the constraint on solving problems has been human ingenuity. This is a particularly challenging realm to contribute novel results to because we\u2019re competing against the combined intellectual might available throughout history. Is our present ingenuity truly greater than that of others 20-30 years ago, let alone 200-300? Probably not. However, our ability to bring new tools like compute and data most certainly is. Therefore, spending as much time in this new problem territory, where data and compute help solve problems , is likely to generate exciting and novel results more frequently in the long run.","title":"8 - Overarching Research Theme"},{"location":"spring2021/lecture-12/#9-how-to-keep-up","text":"\u201c Give a man a fish and you feed him for a day, teach a man to fish and you feed him for a lifetime \u201d (Lao Tzu) Here are some tips on how to keep up with ML research: (Mostly) don\u2019t read (most) papers. There are just too many! When you do want to keep up, use the following: Tutorials at conferences: these capture the essence of important concepts in a practical, distilled way Graduate courses and seminars Yannic Kilcher YouTube channel Two Minutes Paper Channel The Batch by Andrew Ng Import AI by Jack Clark If you DO decide to read papers, Follow a principled process for reading papers Use Arxiv Sanity Twitter AI/DL Facebook Group ML Subreddit Start a reading group: read papers together with friends - either everyone reads then discusses, or one or two people read and give tutorials to others. Finally, should you do a Ph.D. or not? You don\u2019t have to do a Ph.D. to work in AI! However, if you REALLY want to become one of the world\u2019s experts in a topic you care about, then a Ph.D. is a technically deep and demanding path to get there. Crudely speaking, a Ph.D. enables you to develop new tools and techniques rather than using existing tools and techniques.","title":"9 - How To Keep Up"},{"location":"spring2021/lecture-13/","text":"\u2728Lecture 13: ML Teams and Startups\u2728 Video Slides PDF Download Notes Download notes as PDF Notes were taken by James Le and Vishnu Rachakonda Over the past few years, machine learning (ML) has grown tremendously. But as young as ML is as a discipline, the craft of managing an ML team is even younger. Many of today\u2019s ML managers were thrust into management roles out of necessity or because they were the best individual contributors, and many come from purely academic backgrounds. At some companies, engineering or product leaders are tasked with building new ML functions without real ML experience. Running any technical team is hard: You have to hire great people. You need to manage and develop them. You need to manage your team\u2019s output and make sure your vectors are aligned. You would want to make good long-term technical choices and manage technical debt. You also must manage expectations from leadership. Running an ML team is even harder: ML talents are expensive and scarce. ML teams have a diverse set of roles. ML projects have unclear timelines and high uncertainty. ML is also the \u201c high-interest credit card of technical debt .\" Leadership often doesn\u2019t understand ML. The goals of this lecture are two-fold: (1) to give you insight into how to think about building and managing ML teams (as a leader); and (2) to help you get a job in ML (as a newcomer). 1 - ML Roles Common Roles Let\u2019s take a look at the most common ML roles and the skills they require: The ML Product Manager works with the ML team, other business functions, the end-users, and the data owners. This person designs documentation, creates wireframes, and comes up with the plan to prioritize and execute ML projects. The DevOps Engineer deploys and monitors production systems. This person handles the infrastructure that runs the deployed ML product using platforms like AWS or GCP. The Data Engineer builds data pipelines, aggregates and collects data from storage, and monitors data behavior. This person works with distributed systems using tools such as Hadoop, Kafka, Airflow. The ML Engineer trains and deploys prediction models. This person uses tools like TensorFlow and Docker to work with prediction systems running on real data in production. The ML Researcher trains prediction models, often those that are forward-looking or not production-critical. This person uses libraries like TensorFlow and PyTorch on notebook environments to build models and reports describing their experiments. The Data Scientist is a blanket term used to describe all of the roles above. In some organizations, this role entails answering business questions via analytics. He/she can work with wide-ranging tools from SQL and Excel to Pandas and Scikit-Learn. Skills Required So what skills are needed for these roles? The chart above displays a nice visual, where the horizontal axis is the level of ML expertise and the size of the bubble is the level of communication and technical writing (the bigger, the better). The ML DevOps is primarily a software engineering role, which often comes from a standard software engineering pipeline. The Data Engineer belongs to the software engineering team that works actively with ML teams. The ML Engineer requires a rare mix of ML and Software Engineering skills. This person is either an engineer with significant self-teaching OR a science/engineering Ph.D. who works as a traditional software engineer after graduate school. The ML Researcher is an ML expert who usually has an MS or Ph.D. degree in Computer Science or Statistics or finishes an industrial fellowship program. The ML Product Manager is just like a traditional Product Manager, but with a deep knowledge of the ML development process and mindset. The Data Scientist role constitutes a wide range of backgrounds from undergraduate to Ph.D. students. 2 - ML Organizations Organization Archetypes There exists not yet a consensus on the right way to structure an ML team. Still, a few best practices are contingent upon different organization archetypes and their ML maturity level. First, let\u2019s see what the different ML organization archetypes are. Archetype 1 - Nascent and Ad-hoc ML These are organizations where no one is doing ML, or ML is done on an ad-hoc basis. Obviously, there is little ML expertise in-house. They are either small-to-medium businesses or less technology-forward large companies in industries like education or logistics. There is often low-hanging fruit for ML. But there is little support for ML projects, and it\u2019s challenging to hire and retain good talent. Archetype 2 - Research and Development ML These are organizations in which ML efforts are centered in the R&D arm of the organization. They often hire ML researchers and doctorate students with experience publishing papers. They are larger companies in sectors such as oil and gas, manufacturing, or telecommunications. They can hire experienced researchers and work on long-term business priorities to get big wins. However, it is very difficult to get quality data. Most often, this type of research work rarely translates into actual business value, so usually, the amount of investment remains small. Archetype 3 - Product-Embedded ML These are organizations where certain product teams or business units have ML expertise alongside their software or analytics talent. These ML individuals report up to the team\u2019s engineering/tech lead. They are either software companies or financial services companies. ML improvements are likely to lead to business value. Furthermore, there is a tight feedback cycle between idea iteration and product improvement. Unfortunately, it is still very hard to hire and develop top talent, and access to data and compute resources can lag. There are also potential conflicts between ML project cycles and engineering management, so long-term ML projects can be hard to justify. Archetype 4 - Independent ML Division These are organizations in which the ML division reports directly to senior leadership. The ML Product Managers work with Researchers and Engineers to build ML into client-facing products. They can sometimes publish long-term research. They are often large financial services companies. Talent density allows them to hire and train top practitioners. Senior leaders can marshal data and compute resources. This gives the organizations to invest in tooling, practices, and culture around ML development. A disadvantage is that model handoffs to different business lines can be challenging since users need the buy-in to ML benefits and get educated on the model use. Also, feedback cycles can be slow. Archetype 5 - ML-First These are organizations in which the CEO invests in ML, and there are experts across the business focusing on quick wins. The ML division works on challenging and long-term projects. They are large tech companies and ML-focused startups. They have the best data access (data thinking permeates the organization), the most attractive recruiting funnel (challenging ML problems tends to attract top talent), and the easiest deployment procedure (product teams understand ML well enough). This type of organization archetype is hard to implement in practice since it is culturally difficult to embed ML thinking everywhere. Team Structure Design Choices Depending on the above archetype that your organization resembles, you can make the appropriate design choices, which broadly speaking follow these three categories: Software Engineer vs. Research : To what extent is the ML team responsible for building or integrating with software? How important are Software Engineering skills on the team? Data Ownership : How much control does the ML team have over data collection, warehousing, labeling, and pipelining? Model Ownership : Is the ML team responsible for deploying models into production? Who maintains the deployed models? Below are our design suggestions: If your organization focuses on ML R&D : Research is most definitely prioritized over Software Engineering skills. Because of this, there would potentially be a lack of collaboration between these two groups. ML team has no control over the data and typically will not have data engineers to support them. ML models are rarely deployed into production. If your organization has ML embedded into the product : Software Engineering skills will be prioritized over Research skills. Often, the researchers would need strong engineering skills since everyone would be expected to product-ionize his/her models. ML teams generally do not own data production and data management. They will need to work with data engineers to build data pipelines. ML engineers totally own the models that they deploy into production. If your organization has an independent ML division : Each team has a potent mix of engineering and research skills; therefore, they work closely together within teams. ML team has a voice in data governance discussions, as well as a robust data engineering function. ML team hands-off models to users but is still responsible for maintaining them. If your organization is ML-First : Different teams are more or less research-oriented, but in general, research teams collaborate closely with engineering teams. ML team often owns the company-wide data infrastructure. ML team hands the models to users, who are responsible for operating and maintaining them. The picture below neatly sums up these suggestions: 3 - Managing ML Teams Managing ML Teams Is Challenging The process of actually managing an ML team is quite challenging for four reasons: Engineering Estimation: It\u2019s hard to know how easy or hard an ML project is in advance. As you explore the data and experiment with different models, there is enormous scope for new learnings about the problem that materially impact the timeline. Furthermore, knowing what methods will work is often impossible. This makes it hard to say upfront how long or how much work may go into an ML project. Nonlinear Progress: As the chart below from a blog post by Lukas Biewald (CEO of Weights and Biases) shows, progress on ML projects is unpredictable over time, even when the effort expended grows considerably. It\u2019s very common for projects to stall for extended periods of time. Cultural gaps: The relative culture of engineering and research professionals is very different. Research tends to favor novel, creative ideas, while engineering prefers tried and true methods that work. As a result, ML teams often experience a clash of cultures, which can turn toxic if not appropriately managed. A core challenge of running ML teams is addressing the cultural barriers between ML and software engineering so that teams can harmoniously experiment and deliver ML products. Leadership Deficits : It\u2019s common to see a lack of detailed understanding of ML at senior levels of management in many companies. As a result, expressing feasibility and setting the right expectations for ML projects, especially high-priority ones, can be hard. How To Manage ML Teams Better Managing ML teams is hardly a solved problem, but you can take steps to improve the process. Plan Probabilistically Many engineering projects are managed in a waterfall fashion, with the sequential tasks defined up front clearly. Instead of forcing this method of engineering management on difficult ML projects, try assigning a likelihood of success to different tasks to better capture the experimental process inherent to ML engineering. As these tasks progress or stall, rapidly re-evaluate your task ordering to better match what is working. Having this sense of both (1) how likely a task is to succeed and (2) how important it is makes project planning considerably more realistic. Have A Portfolio Of Approaches Embrace multiple ideas and approaches to solve crucial research challenges that gate production ML. Don\u2019t make your plan dependent on one approach working! Measure Inputs, Not Results As you work through several approaches in your portfolio, do not overly emphasize whose ideas ultimately work as a reflection of contribution quality. This can negatively impact team members\u2019 creativity, as they focus more on trying to find only what they currently think could work, rather than experimenting in a high-quality fashion (which is ultimately what leads to ML success). Have Researchers and Engineers Work Together The collaboration between engineering and research is essential for quality ML products to get into production. Emphasize collaboration across the groups and professionals! Get End-to-end Pipelines Together Quickly to Demonstrate Quick Wins Taking this approach makes it more likely that your ML project will succeed in the long term. It allows you to demonstrate progress to your leadership more effectively and clearly. Educate Leadership on ML Timeline Uncertainty This can be hard, as leadership is ultimately accountable for addressing blind spots and understanding timeline risk. There are things you can do, however, to help improve leadership\u2019s knowledge about ML timelines. Avoid building hype around narrow progress metrics material only to the ML team (e.g., \u201c We improved F1 score by 0.2 and have achieved awesome performance! \u201d). Instead, be realistic, communicate risk, and emphasize real product impact (e.g., \u201cOur model improvements should increase the number of conversions by 10%, though we must continue to validate its performance on additional demographic factors.) Sharing resources like this a16z primer and this class from Prof. Pieter Abbeel can increase awareness of your company\u2019s leadership. 4 - Hiring/Getting Hired The AI Talent Gap With the novelty of ML systems, it\u2019s fair to say that not many people have built real ML systems. Estimates vary from as few as 10,000 (Element AI) to as many as 200-300,000 people (Tencent). Whatever way you slice the numbers (contained in this blog post ), the reality is that there is not much-experienced talent in the AI/ML field, especially compared to the number of trained software developers in the US (3.6M) or in the world (18.2M). Sourcing Because of this shallow talent pool and the skyrocketing demand, hiring for ML positions is pretty hard. Typical ML roles come in the following structure: ML Adjacent roles: ML product manager, DevOps, Data Engineer Core ML Roles: ML Engineer, ML Research/ML Scientist Business analytics roles: Data Scientist For ML adjacent roles, traditional ML knowledge is less important, as demonstrated interest, conversational understanding, and experience can help these professionals play an impactful role on ML teams. Let\u2019s focus on how to hire for the core ML roles . While there\u2019s no perfect way to hire ML engineers , there\u2019s definitely a wrong way to hire them, with extensive job descriptions that demand only the best qualifications. Certainly, there are many good examples of this bad practice floating around. Rather than this unrealistic process, consider hiring for software engineering skills, an interest in ML, and a desire to learn . You can always train people in the art and science of ML, especially when they come with strong software engineering fundamentals. Another option is to consider adding junior talent , as many recent grads come out with good ML knowledge nowadays. Finally, and most importantly, be more specific about what you need the position and professional to do. It\u2019s impossible to find one person that can do everything from full-fledged DevOps to algorithm development. To hire ML researchers , here are our tips: Evaluate the quality of publications, over the quantity, with an eye towards the originality of the ideas, the execution, etc. Prioritize researchers that focus on important problems instead of trendy problems. Experience outside academia is also a positive, as these researchers may be able to transition to industry more effectively. Finally, keep an open mind about research talent and consider talented people without PhDs or from adjacent fields like physics, statistics, etc. To find quality candidates for these roles, some ideas for sourcing are: To experiment with standard job recruiting avenues like LinkedIn, Hired, recruiters, on-campus-recruiting, etc. To monitor arXiv and top conferences and flag first authors of papers you like. To look for good implementations of papers you like. To attend ML research conferences (NeurIPS, ICML, etc.) As you seek to recruit, stay on top of what professionals want and make an effort to position your company accordingly. ML practitioners want to be empowered to do great work with interesting data. Building a culture of learning and impact can help recruit the best talent to your team. Additionally, sell sell sell! Talent needs to know how good your team is and how meaningful the mission can be. Interviewing As you interview candidates for ML roles, try to validate your hypotheses of their strengths while testing a minimum bar on weaker aspects . For example, make sure ML researchers can think creatively about new ML problems while ensuring they meet a baseline for code quality. It\u2019s essential to test both ML knowledge and software engineering skill for all industry professionals, though the relative strengths can vary. The actual ML interview process is much less well-defined than software engineering interviews, though it is modeled off of it. Some helpful inclusions are projects or exercises that test the ability to work with ML-specific code, like take-home ML projects. Finding A Job To find an ML job, you can take a look at the following sources: Standard sources such as LinkedIn, recruiters, on-campus recruiting, etc. ML research conferences (NeurIPS, ICLR, ICML). Apply directly (remember, there\u2019s a talent gap!). Standing out for competitive roles can be tricky! Here are some tips in increasing order of impressiveness that you can apply to differentiate yourself: Build software engineering skills (e.g., at a well-known software company). Exhibit ML interest (e.g., conference attendance, online courses certificates, etc.). Show you have a broad knowledge of ML (e.g., write blog posts synthesizing a research area). Demonstrate ability to get ML projects done (e.g., create side projects, re-implement papers). Prove you can think creatively in ML (e.g., win Kaggle competitions, publish papers). As you prepare for interviews, prepare for both the traditional ML theoretical topics and the general software engineering interview (e.g., read Cracking the Coding Interview ). 5 - Conclusion Being a new and evolving discipline for most traditional organizations, forming ML teams is full of known and unknown challenges. Here are the final few take-homes: There are many different skills involved in production ML, so there are opportunities for many people to contribute. ML teams are becoming more standalone and more interdisciplinary . Managing ML teams is complex. There is no silver bullet, but shifting toward probabilistic planning can help. ML talent is scarce . As a manager, be specific about what skills are must-have in the ML job descriptions. As a job seeker, it can be brutally challenging to break in as an outsider, so use projects as a signal to build awareness.","title":"\u2728Lecture 13: ML Teams and Startups\u2728"},{"location":"spring2021/lecture-13/#lecture-13-ml-teams-and-startups","text":"","title":"\u2728Lecture 13: ML Teams and Startups\u2728"},{"location":"spring2021/lecture-13/#video","text":"","title":"Video"},{"location":"spring2021/lecture-13/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lecture-13/#notes","text":"Download notes as PDF Notes were taken by James Le and Vishnu Rachakonda Over the past few years, machine learning (ML) has grown tremendously. But as young as ML is as a discipline, the craft of managing an ML team is even younger. Many of today\u2019s ML managers were thrust into management roles out of necessity or because they were the best individual contributors, and many come from purely academic backgrounds. At some companies, engineering or product leaders are tasked with building new ML functions without real ML experience. Running any technical team is hard: You have to hire great people. You need to manage and develop them. You need to manage your team\u2019s output and make sure your vectors are aligned. You would want to make good long-term technical choices and manage technical debt. You also must manage expectations from leadership. Running an ML team is even harder: ML talents are expensive and scarce. ML teams have a diverse set of roles. ML projects have unclear timelines and high uncertainty. ML is also the \u201c high-interest credit card of technical debt .\" Leadership often doesn\u2019t understand ML. The goals of this lecture are two-fold: (1) to give you insight into how to think about building and managing ML teams (as a leader); and (2) to help you get a job in ML (as a newcomer).","title":"Notes"},{"location":"spring2021/lecture-13/#1-ml-roles","text":"","title":"1 - ML Roles"},{"location":"spring2021/lecture-13/#common-roles","text":"Let\u2019s take a look at the most common ML roles and the skills they require: The ML Product Manager works with the ML team, other business functions, the end-users, and the data owners. This person designs documentation, creates wireframes, and comes up with the plan to prioritize and execute ML projects. The DevOps Engineer deploys and monitors production systems. This person handles the infrastructure that runs the deployed ML product using platforms like AWS or GCP. The Data Engineer builds data pipelines, aggregates and collects data from storage, and monitors data behavior. This person works with distributed systems using tools such as Hadoop, Kafka, Airflow. The ML Engineer trains and deploys prediction models. This person uses tools like TensorFlow and Docker to work with prediction systems running on real data in production. The ML Researcher trains prediction models, often those that are forward-looking or not production-critical. This person uses libraries like TensorFlow and PyTorch on notebook environments to build models and reports describing their experiments. The Data Scientist is a blanket term used to describe all of the roles above. In some organizations, this role entails answering business questions via analytics. He/she can work with wide-ranging tools from SQL and Excel to Pandas and Scikit-Learn.","title":"Common Roles"},{"location":"spring2021/lecture-13/#skills-required","text":"So what skills are needed for these roles? The chart above displays a nice visual, where the horizontal axis is the level of ML expertise and the size of the bubble is the level of communication and technical writing (the bigger, the better). The ML DevOps is primarily a software engineering role, which often comes from a standard software engineering pipeline. The Data Engineer belongs to the software engineering team that works actively with ML teams. The ML Engineer requires a rare mix of ML and Software Engineering skills. This person is either an engineer with significant self-teaching OR a science/engineering Ph.D. who works as a traditional software engineer after graduate school. The ML Researcher is an ML expert who usually has an MS or Ph.D. degree in Computer Science or Statistics or finishes an industrial fellowship program. The ML Product Manager is just like a traditional Product Manager, but with a deep knowledge of the ML development process and mindset. The Data Scientist role constitutes a wide range of backgrounds from undergraduate to Ph.D. students.","title":"Skills Required"},{"location":"spring2021/lecture-13/#2-ml-organizations","text":"","title":"2 - ML Organizations"},{"location":"spring2021/lecture-13/#organization-archetypes","text":"There exists not yet a consensus on the right way to structure an ML team. Still, a few best practices are contingent upon different organization archetypes and their ML maturity level. First, let\u2019s see what the different ML organization archetypes are.","title":"Organization Archetypes"},{"location":"spring2021/lecture-13/#archetype-1-nascent-and-ad-hoc-ml","text":"These are organizations where no one is doing ML, or ML is done on an ad-hoc basis. Obviously, there is little ML expertise in-house. They are either small-to-medium businesses or less technology-forward large companies in industries like education or logistics. There is often low-hanging fruit for ML. But there is little support for ML projects, and it\u2019s challenging to hire and retain good talent.","title":"Archetype 1 - Nascent and Ad-hoc ML"},{"location":"spring2021/lecture-13/#archetype-2-research-and-development-ml","text":"These are organizations in which ML efforts are centered in the R&D arm of the organization. They often hire ML researchers and doctorate students with experience publishing papers. They are larger companies in sectors such as oil and gas, manufacturing, or telecommunications. They can hire experienced researchers and work on long-term business priorities to get big wins. However, it is very difficult to get quality data. Most often, this type of research work rarely translates into actual business value, so usually, the amount of investment remains small.","title":"Archetype 2 - Research and Development ML"},{"location":"spring2021/lecture-13/#archetype-3-product-embedded-ml","text":"These are organizations where certain product teams or business units have ML expertise alongside their software or analytics talent. These ML individuals report up to the team\u2019s engineering/tech lead. They are either software companies or financial services companies. ML improvements are likely to lead to business value. Furthermore, there is a tight feedback cycle between idea iteration and product improvement. Unfortunately, it is still very hard to hire and develop top talent, and access to data and compute resources can lag. There are also potential conflicts between ML project cycles and engineering management, so long-term ML projects can be hard to justify.","title":"Archetype 3 - Product-Embedded ML"},{"location":"spring2021/lecture-13/#archetype-4-independent-ml-division","text":"These are organizations in which the ML division reports directly to senior leadership. The ML Product Managers work with Researchers and Engineers to build ML into client-facing products. They can sometimes publish long-term research. They are often large financial services companies. Talent density allows them to hire and train top practitioners. Senior leaders can marshal data and compute resources. This gives the organizations to invest in tooling, practices, and culture around ML development. A disadvantage is that model handoffs to different business lines can be challenging since users need the buy-in to ML benefits and get educated on the model use. Also, feedback cycles can be slow.","title":"Archetype 4 - Independent ML Division"},{"location":"spring2021/lecture-13/#archetype-5-ml-first","text":"These are organizations in which the CEO invests in ML, and there are experts across the business focusing on quick wins. The ML division works on challenging and long-term projects. They are large tech companies and ML-focused startups. They have the best data access (data thinking permeates the organization), the most attractive recruiting funnel (challenging ML problems tends to attract top talent), and the easiest deployment procedure (product teams understand ML well enough). This type of organization archetype is hard to implement in practice since it is culturally difficult to embed ML thinking everywhere.","title":"Archetype 5 - ML-First"},{"location":"spring2021/lecture-13/#team-structure-design-choices","text":"Depending on the above archetype that your organization resembles, you can make the appropriate design choices, which broadly speaking follow these three categories: Software Engineer vs. Research : To what extent is the ML team responsible for building or integrating with software? How important are Software Engineering skills on the team? Data Ownership : How much control does the ML team have over data collection, warehousing, labeling, and pipelining? Model Ownership : Is the ML team responsible for deploying models into production? Who maintains the deployed models? Below are our design suggestions: If your organization focuses on ML R&D : Research is most definitely prioritized over Software Engineering skills. Because of this, there would potentially be a lack of collaboration between these two groups. ML team has no control over the data and typically will not have data engineers to support them. ML models are rarely deployed into production. If your organization has ML embedded into the product : Software Engineering skills will be prioritized over Research skills. Often, the researchers would need strong engineering skills since everyone would be expected to product-ionize his/her models. ML teams generally do not own data production and data management. They will need to work with data engineers to build data pipelines. ML engineers totally own the models that they deploy into production. If your organization has an independent ML division : Each team has a potent mix of engineering and research skills; therefore, they work closely together within teams. ML team has a voice in data governance discussions, as well as a robust data engineering function. ML team hands-off models to users but is still responsible for maintaining them. If your organization is ML-First : Different teams are more or less research-oriented, but in general, research teams collaborate closely with engineering teams. ML team often owns the company-wide data infrastructure. ML team hands the models to users, who are responsible for operating and maintaining them. The picture below neatly sums up these suggestions:","title":"Team Structure Design Choices"},{"location":"spring2021/lecture-13/#3-managing-ml-teams","text":"","title":"3 - Managing ML Teams"},{"location":"spring2021/lecture-13/#managing-ml-teams-is-challenging","text":"The process of actually managing an ML team is quite challenging for four reasons: Engineering Estimation: It\u2019s hard to know how easy or hard an ML project is in advance. As you explore the data and experiment with different models, there is enormous scope for new learnings about the problem that materially impact the timeline. Furthermore, knowing what methods will work is often impossible. This makes it hard to say upfront how long or how much work may go into an ML project. Nonlinear Progress: As the chart below from a blog post by Lukas Biewald (CEO of Weights and Biases) shows, progress on ML projects is unpredictable over time, even when the effort expended grows considerably. It\u2019s very common for projects to stall for extended periods of time. Cultural gaps: The relative culture of engineering and research professionals is very different. Research tends to favor novel, creative ideas, while engineering prefers tried and true methods that work. As a result, ML teams often experience a clash of cultures, which can turn toxic if not appropriately managed. A core challenge of running ML teams is addressing the cultural barriers between ML and software engineering so that teams can harmoniously experiment and deliver ML products. Leadership Deficits : It\u2019s common to see a lack of detailed understanding of ML at senior levels of management in many companies. As a result, expressing feasibility and setting the right expectations for ML projects, especially high-priority ones, can be hard.","title":"Managing ML Teams Is Challenging"},{"location":"spring2021/lecture-13/#how-to-manage-ml-teams-better","text":"Managing ML teams is hardly a solved problem, but you can take steps to improve the process.","title":"How To Manage ML Teams Better"},{"location":"spring2021/lecture-13/#plan-probabilistically","text":"Many engineering projects are managed in a waterfall fashion, with the sequential tasks defined up front clearly. Instead of forcing this method of engineering management on difficult ML projects, try assigning a likelihood of success to different tasks to better capture the experimental process inherent to ML engineering. As these tasks progress or stall, rapidly re-evaluate your task ordering to better match what is working. Having this sense of both (1) how likely a task is to succeed and (2) how important it is makes project planning considerably more realistic.","title":"Plan Probabilistically"},{"location":"spring2021/lecture-13/#have-a-portfolio-of-approaches","text":"Embrace multiple ideas and approaches to solve crucial research challenges that gate production ML. Don\u2019t make your plan dependent on one approach working!","title":"Have A Portfolio Of Approaches"},{"location":"spring2021/lecture-13/#measure-inputs-not-results","text":"As you work through several approaches in your portfolio, do not overly emphasize whose ideas ultimately work as a reflection of contribution quality. This can negatively impact team members\u2019 creativity, as they focus more on trying to find only what they currently think could work, rather than experimenting in a high-quality fashion (which is ultimately what leads to ML success).","title":"Measure Inputs, Not Results"},{"location":"spring2021/lecture-13/#have-researchers-and-engineers-work-together","text":"The collaboration between engineering and research is essential for quality ML products to get into production. Emphasize collaboration across the groups and professionals!","title":"Have Researchers and Engineers Work Together"},{"location":"spring2021/lecture-13/#get-end-to-end-pipelines-together-quickly-to-demonstrate-quick-wins","text":"Taking this approach makes it more likely that your ML project will succeed in the long term. It allows you to demonstrate progress to your leadership more effectively and clearly.","title":"Get End-to-end Pipelines Together Quickly to Demonstrate Quick Wins"},{"location":"spring2021/lecture-13/#educate-leadership-on-ml-timeline-uncertainty","text":"This can be hard, as leadership is ultimately accountable for addressing blind spots and understanding timeline risk. There are things you can do, however, to help improve leadership\u2019s knowledge about ML timelines. Avoid building hype around narrow progress metrics material only to the ML team (e.g., \u201c We improved F1 score by 0.2 and have achieved awesome performance! \u201d). Instead, be realistic, communicate risk, and emphasize real product impact (e.g., \u201cOur model improvements should increase the number of conversions by 10%, though we must continue to validate its performance on additional demographic factors.) Sharing resources like this a16z primer and this class from Prof. Pieter Abbeel can increase awareness of your company\u2019s leadership.","title":"Educate Leadership on ML Timeline Uncertainty"},{"location":"spring2021/lecture-13/#4-hiringgetting-hired","text":"","title":"4 - Hiring/Getting Hired"},{"location":"spring2021/lecture-13/#the-ai-talent-gap","text":"With the novelty of ML systems, it\u2019s fair to say that not many people have built real ML systems. Estimates vary from as few as 10,000 (Element AI) to as many as 200-300,000 people (Tencent). Whatever way you slice the numbers (contained in this blog post ), the reality is that there is not much-experienced talent in the AI/ML field, especially compared to the number of trained software developers in the US (3.6M) or in the world (18.2M).","title":"The AI Talent Gap"},{"location":"spring2021/lecture-13/#sourcing","text":"Because of this shallow talent pool and the skyrocketing demand, hiring for ML positions is pretty hard. Typical ML roles come in the following structure: ML Adjacent roles: ML product manager, DevOps, Data Engineer Core ML Roles: ML Engineer, ML Research/ML Scientist Business analytics roles: Data Scientist For ML adjacent roles, traditional ML knowledge is less important, as demonstrated interest, conversational understanding, and experience can help these professionals play an impactful role on ML teams. Let\u2019s focus on how to hire for the core ML roles . While there\u2019s no perfect way to hire ML engineers , there\u2019s definitely a wrong way to hire them, with extensive job descriptions that demand only the best qualifications. Certainly, there are many good examples of this bad practice floating around. Rather than this unrealistic process, consider hiring for software engineering skills, an interest in ML, and a desire to learn . You can always train people in the art and science of ML, especially when they come with strong software engineering fundamentals. Another option is to consider adding junior talent , as many recent grads come out with good ML knowledge nowadays. Finally, and most importantly, be more specific about what you need the position and professional to do. It\u2019s impossible to find one person that can do everything from full-fledged DevOps to algorithm development. To hire ML researchers , here are our tips: Evaluate the quality of publications, over the quantity, with an eye towards the originality of the ideas, the execution, etc. Prioritize researchers that focus on important problems instead of trendy problems. Experience outside academia is also a positive, as these researchers may be able to transition to industry more effectively. Finally, keep an open mind about research talent and consider talented people without PhDs or from adjacent fields like physics, statistics, etc. To find quality candidates for these roles, some ideas for sourcing are: To experiment with standard job recruiting avenues like LinkedIn, Hired, recruiters, on-campus-recruiting, etc. To monitor arXiv and top conferences and flag first authors of papers you like. To look for good implementations of papers you like. To attend ML research conferences (NeurIPS, ICML, etc.) As you seek to recruit, stay on top of what professionals want and make an effort to position your company accordingly. ML practitioners want to be empowered to do great work with interesting data. Building a culture of learning and impact can help recruit the best talent to your team. Additionally, sell sell sell! Talent needs to know how good your team is and how meaningful the mission can be.","title":"Sourcing"},{"location":"spring2021/lecture-13/#interviewing","text":"As you interview candidates for ML roles, try to validate your hypotheses of their strengths while testing a minimum bar on weaker aspects . For example, make sure ML researchers can think creatively about new ML problems while ensuring they meet a baseline for code quality. It\u2019s essential to test both ML knowledge and software engineering skill for all industry professionals, though the relative strengths can vary. The actual ML interview process is much less well-defined than software engineering interviews, though it is modeled off of it. Some helpful inclusions are projects or exercises that test the ability to work with ML-specific code, like take-home ML projects.","title":"Interviewing"},{"location":"spring2021/lecture-13/#finding-a-job","text":"To find an ML job, you can take a look at the following sources: Standard sources such as LinkedIn, recruiters, on-campus recruiting, etc. ML research conferences (NeurIPS, ICLR, ICML). Apply directly (remember, there\u2019s a talent gap!). Standing out for competitive roles can be tricky! Here are some tips in increasing order of impressiveness that you can apply to differentiate yourself: Build software engineering skills (e.g., at a well-known software company). Exhibit ML interest (e.g., conference attendance, online courses certificates, etc.). Show you have a broad knowledge of ML (e.g., write blog posts synthesizing a research area). Demonstrate ability to get ML projects done (e.g., create side projects, re-implement papers). Prove you can think creatively in ML (e.g., win Kaggle competitions, publish papers). As you prepare for interviews, prepare for both the traditional ML theoretical topics and the general software engineering interview (e.g., read Cracking the Coding Interview ).","title":"Finding A Job"},{"location":"spring2021/lecture-13/#5-conclusion","text":"Being a new and evolving discipline for most traditional organizations, forming ML teams is full of known and unknown challenges. Here are the final few take-homes: There are many different skills involved in production ML, so there are opportunities for many people to contribute. ML teams are becoming more standalone and more interdisciplinary . Managing ML teams is complex. There is no silver bullet, but shifting toward probabilistic planning can help. ML talent is scarce . As a manager, be specific about what skills are must-have in the ML job descriptions. As a job seeker, it can be brutally challenging to break in as an outsider, so use projects as a signal to build awareness.","title":"5 - Conclusion"},{"location":"spring2021/lecture-2a/","text":"Lecture 2A: CNNs Video Slides PDF Download Notes In this video, we first review convolution operation, the most basic property of Convolutional Neural Networks. Then, we look at other important operations for ConvNets. Finally, we transition to looking at a classic ConvNet architecture called LeNet. 00:00 - Introduction 01:08 - Convolutional Filters 07:10 - Filter Stacks and ConvNets 11:25 - Strides and Padding 14:35 - Filter Math 21:44 - Convolution Implementation Notes 24:04 - Increasing the Receptive Field with Dilated Convolutions 27:30 - Decreasing the Tensor Size with Pooling and 1x1-Convolutions 30:54 - LeNet Architecture","title":"Lecture 2A: CNNs"},{"location":"spring2021/lecture-2a/#lecture-2a-cnns","text":"","title":"Lecture 2A: CNNs"},{"location":"spring2021/lecture-2a/#video","text":"","title":"Video"},{"location":"spring2021/lecture-2a/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lecture-2a/#notes","text":"In this video, we first review convolution operation, the most basic property of Convolutional Neural Networks. Then, we look at other important operations for ConvNets. Finally, we transition to looking at a classic ConvNet architecture called LeNet. 00:00 - Introduction 01:08 - Convolutional Filters 07:10 - Filter Stacks and ConvNets 11:25 - Strides and Padding 14:35 - Filter Math 21:44 - Convolution Implementation Notes 24:04 - Increasing the Receptive Field with Dilated Convolutions 27:30 - Decreasing the Tensor Size with Pooling and 1x1-Convolutions 30:54 - LeNet Architecture","title":"Notes"},{"location":"spring2021/lecture-2b/","text":"Lecture 2B: Computer Vision Video Slides PDF Download Notes In this video, we will review notable applications of deep learning in computer vision. First, we will tour some ConvNet architectures. Then, we will talk about localization, detection, and segmentation problems. We will conclude with more advanced methods. Learn more at this website: https://paperswithcode.com/area/computer-vision 00:00 - Introduction 02:51 - AlexNet 05:09 - ZFNet 06:54 - VGGNet 09:06 - GoogLeNet 11:57 - ResNet 15:15 - SqueezeNet 17:05 - Architecture Comparisons 20:00 - Localization, Detection, and Segmentation Tasks 24:00 - Overfeat, YOLO, and SSD Methods 28:01 - Region Proposal Methods (R-CNN, Faster R-CNN, Mask R-CNN, U-Net) 34:33 - Advanced Tasks (3D Shape Inference, Face Landmark Recognition, and Pose Estimation) 37:00 - Adversarial Attacks 40:56 - Style Transfer","title":"Lecture 2B: Computer Vision"},{"location":"spring2021/lecture-2b/#lecture-2b-computer-vision","text":"","title":"Lecture 2B: Computer Vision"},{"location":"spring2021/lecture-2b/#video","text":"","title":"Video"},{"location":"spring2021/lecture-2b/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lecture-2b/#notes","text":"In this video, we will review notable applications of deep learning in computer vision. First, we will tour some ConvNet architectures. Then, we will talk about localization, detection, and segmentation problems. We will conclude with more advanced methods. Learn more at this website: https://paperswithcode.com/area/computer-vision 00:00 - Introduction 02:51 - AlexNet 05:09 - ZFNet 06:54 - VGGNet 09:06 - GoogLeNet 11:57 - ResNet 15:15 - SqueezeNet 17:05 - Architecture Comparisons 20:00 - Localization, Detection, and Segmentation Tasks 24:00 - Overfeat, YOLO, and SSD Methods 28:01 - Region Proposal Methods (R-CNN, Faster R-CNN, Mask R-CNN, U-Net) 34:33 - Advanced Tasks (3D Shape Inference, Face Landmark Recognition, and Pose Estimation) 37:00 - Adversarial Attacks 40:56 - Style Transfer","title":"Notes"},{"location":"spring2021/lecture-3/","text":"Lecture 3: RNNs Video Slides PDF Download Notes 00:00 - Introduction 01:34 - Sequence Problems 06:28 - Review of RNNs 22:00 - Vanishing Gradient Issue 27:52 - LSTMs and Its Variants 34:10 - Bidirectionality and Attention from Google's Neural Machine Translation 46:38 - CTC Loss 52:12 - Pros and Cons of Encoder-Decoder LSTM Architectures 54:55 - WaveNet","title":"Lecture 3: RNNs"},{"location":"spring2021/lecture-3/#lecture-3-rnns","text":"","title":"Lecture 3: RNNs"},{"location":"spring2021/lecture-3/#video","text":"","title":"Video"},{"location":"spring2021/lecture-3/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lecture-3/#notes","text":"00:00 - Introduction 01:34 - Sequence Problems 06:28 - Review of RNNs 22:00 - Vanishing Gradient Issue 27:52 - LSTMs and Its Variants 34:10 - Bidirectionality and Attention from Google's Neural Machine Translation 46:38 - CTC Loss 52:12 - Pros and Cons of Encoder-Decoder LSTM Architectures 54:55 - WaveNet","title":"Notes"},{"location":"spring2021/lecture-4/","text":"Lecture 4: Transformers Video Slides PDF Download Notes In this video, you will learn about the origin of transfer learning in computer vision, its application in NLP in the form of embedding, NLP's ImageNet moment, and the Transformers model families. 00:00 - Introduction 00:42 - Transfer Learning in Computer Vision 04:00 - Embeddings and Language Models 10:09 - NLP's ImageNet moment: ELMO and ULMFit on datasets like SQuAD, SNLI, and GLUE 16:49 - Rise of Transformers 18:20 - Attention in Detail: (Masked) Self-Attention, Positional Encoding, and Layer Normalization 27:33 - Transformers Variants: BERT, GPT/GPT-2/GPT-3, DistillBERT, T5, etc. 36:20 - GPT3 Demos 42:53 - Future Directions","title":"Lecture 4: Transformers"},{"location":"spring2021/lecture-4/#lecture-4-transformers","text":"","title":"Lecture 4: Transformers"},{"location":"spring2021/lecture-4/#video","text":"","title":"Video"},{"location":"spring2021/lecture-4/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lecture-4/#notes","text":"In this video, you will learn about the origin of transfer learning in computer vision, its application in NLP in the form of embedding, NLP's ImageNet moment, and the Transformers model families. 00:00 - Introduction 00:42 - Transfer Learning in Computer Vision 04:00 - Embeddings and Language Models 10:09 - NLP's ImageNet moment: ELMO and ULMFit on datasets like SQuAD, SNLI, and GLUE 16:49 - Rise of Transformers 18:20 - Attention in Detail: (Masked) Self-Attention, Positional Encoding, and Layer Normalization 27:33 - Transformers Variants: BERT, GPT/GPT-2/GPT-3, DistillBERT, T5, etc. 36:20 - GPT3 Demos 42:53 - Future Directions","title":"Notes"},{"location":"spring2021/lecture-5/","text":"Lecture 5: ML Projects Learn how to set up Machine Learning projects like a pro. This includes an understanding of the ML lifecycle, an acute mind of the feasibility and impact, an awareness of the project archetypes, and an obsession with metrics and baselines. Video Slides PDF Download Detailed Notes By James Le and Vishnu Rachakonda 1 - Why Do ML Projects Fail? Based on a report from TechRepublic a few years back, despite increased interest in adopting machine learning (ML) in the enterprise, 85% of machine learning projects ultimately fail to deliver on their intended promises to business. Failure can happen for many reasons; however, a few glaring dangers will cause any AI project to crash and burn. ML is still very much a research endeavor. Therefore it is very challenging to aim for a 100% success rate. Many ML projects are technically infeasible or poorly scoped. Many ML projects never leap production, thus getting stuck at the prototype phase. Many ML projects have unclear success criteria because of a lack of understanding of the value proposition. Many ML projects are poorly managed because of a lack of interest from leadership. 2 - Lifecycle It\u2019s essential to understand what constitutes all of the activities in a machine learning project. Typically speaking, there are four major phases: Planning and Project Setup : At this phase, we want to decide the problem to work on, determine the requirements and goals, figure out how to allocate resources properly, consider the ethical implications, etc. Data Collection and Labeling : At this phase, we want to collect training data and potentially annotate them with ground truth, depending on the specific sources where they come from. We may find that it\u2019s too hard to get the data, or it might be easier to label for a different task. If that\u2019s the case, go back to phase 1. Model Training and Model Debugging : At this phase, we want to implement baseline models quickly, find and reproduce state-of-the-art methods for the problem domain, debug our implementation, and improve the model performance for specific tasks. We may realize that we need to collect more data or that labeling is unreliable (thus, go back to phase 2). Or we may recognize that the task is too challenging and there is a tradeoff between project requirements (thus, go back to phase 1). Model Deploying and Model Testing : At this phase, we want to pilot the model in a constrained environment (i.e., in the lab), write tests to prevent regressions, and roll the model into production. We may see that the model doesn\u2019t work well in the lab, so we want to keep improving the model\u2019s accuracy (thus, go back to phase 3). Or we may want to fix the mismatch between training data and production data by collecting more data and mining hard cases (thus go back to phase 2). Or we may find out that the metric picked doesn\u2019t actually drive downstream user behavior, and/or performance in the real world isn\u2019t great. In such situations, we want to revisit the projects\u2019 metrics and requirements (thus, go back to phase 1). Besides the per-project activities mentioned above, there are two other things that any ML team will need to solve across any projects they get involved with: (1) building the team and hiring people; and (2) setting up infrastructure and tooling to build ML systems repeatedly and at scale. Additionally, it might be useful to understand state-of-the-art results in your application domain so that you know what\u2019s possible and what to try next. 3 - Prioritizing Projects To prioritize projects to work on, you want to find high-impact problems and assess the potential costs associated with them. The picture below shows a general framework that encourages us to target projects with high impact and high feasibility. High Impact There are no silver bullets to find high-impact ML problems to work on, but here are a few useful mental models: Where can you take advantage of cheap prediction? Where is there friction in your product? Where can you automate complicated manual processes? What are other people doing? Cheap Prediction In the book \u201c Prediction Machines ,\u201d the authors (Ajay Agrawal, Joshua Gans, and Avi Goldfarb) come up with an excellent mental model on the economics of Artificial Intelligence: As AI reduces the cost of prediction and prediction is central for decision making, cheap predictions would be universal for problems across business domains . Therefore, you should look for projects where cheap predictions will have a huge business impact. Product Needs Another lens is to think about what your product needs. In the article \u201c Three Principles for Designing ML-Powered Products ,\u201d the Spotify Design team emphasizes the importance of building ML from a product perspective and looking for parts of the product experience with high friction . Automating those parts is exactly where there is a lot of impact for ML to make your business better. ML Strength In his popular blog post \u201c Software 2.0 ,\u201d Andrej Karpathy contrasts software 1.0 (which are traditional programs with explicit instructions) and software 2.0 (where humans specify goals, while the algorithm searches for a program that works). Software 2.0 programmers work with datasets, which get compiled via optimization\u200a\u2014\u200awhich works better, more general, and less computationally expensive. Therefore, you should look for complicated rule-based software where we can learn the rules instead of programming them. Inspiration From Others Instead of reinventing the wheel, you can look at what other companies are doing. In particular, check out papers from large frontier organizations (Google, Facebook, Nvidia, Netflix, etc.) and blog posts from top earlier-stage companies (Uber, Lyft, Spotify, Stripe, etc.). Here is a list of excellent ML use cases to check out (credit to Chip Huyen\u2019s ML Systems Design Lecture 2 Note ): Human-Centric Machine Learning Infrastructure at Netflix (Ville Tuulos, InfoQ 2019) 2020 state of enterprise machine learning (Algorithmia, 2020) Using Machine Learning to Predict Value of Homes On Airbnb (Robert Chang, Airbnb Engineering & Data Science, 2017) Using Machine Learning to Improve Streaming Quality at Netflix (Chaitanya Ekanadham, Netflix Technology Blog, 2018) 150 Successful Machine Learning Models: 6 Lessons Learned at Booking.com (Bernardi et al., KDD, 2019) How we grew from 0 to 4 million women on our fashion app, with a vertical machine learning approach (Gabriel Aldamiz, HackerNoon, 2018) Machine Learning-Powered Search Ranking of Airbnb Experiences (Mihajlo Grbovic, Airbnb Engineering & Data Science, 2019) From shallow to deep learning in fraud (Hao Yi Ong, Lyft Engineering, 2018) Space, Time and Groceries (Jeremy Stanley, Tech at Instacart, 2017) Creating a Modern OCR Pipeline Using Computer Vision and Deep Learning (Brad Neuberg, Dropbox Engineering, 2017) Scaling Machine Learning at Uber with Michelangelo (Jeremy Hermann and Mike Del Balso, Uber Engineering, 2019) Spotify\u2019s Discover Weekly: How machine learning finds your new music (Sophia Ciocca, 2017) High Feasibility The three primary cost drivers of ML projects in order of importance are data availability, accuracy requirement, and problem difficulty. Data Availability Here are the questions you need to ask concerning the data availability: How hard is it to acquire data? How expensive is data labeling? How much data will be needed? How stable is the data? What are the data security requirements? Accuracy Requirement Here are the questions you need to ask concerning the accuracy requirement: How costly are wrong predictions? How frequently does the system need to be right to be useful? What are the ethical implications? It is worth noting that ML project costs tend to scale super-linearly in the accuracy requirement. The fundamental reason is that you typically need a lot more data and more high-quality labels to achieve high accuracy numbers. Problem Difficulty Here are the questions you need to ask concerning the problem difficulty: Is the problem well-defined? Is there good published work on similar problems? What are the computing requirements? Can a human do it? So what\u2019s still hard in machine learning? As a caveat, it\u2019s historically very challenging to predict what types of problems will be difficult for ML to solve in the future. But generally speaking, both unsupervised learning and reinforcement learning are still hard, even though they show promise in limited domains where tons of data and compute are available. Zooming into supervised learning , here are three types of hard problems: Output is complex: These are problems where the output is high-dimensional or ambiguous. Examples include 3D reconstruction, video prediction, dialog systems, open-ended recommendation systems, etc. Reliability is required: These are problems where high precision and robustness are required. Examples include systems that can fail safely in out-of-distribution scenarios, is robust to adversarial attacks, or needs to tackle highly precise tasks. Generalization is required: These are problems with out-of-distribution data or in the domains of reasoning, planning, and causality. Examples include any systems for self-driving vehicles or any systems that deal with small data. Finally, this is a nice checklist for you to run an ML feasibility assessment: Are you sure that you need ML at all? Put in the work upfront to define success criteria with all of the stakeholders. Consider the ethics of using ML. Do a literature review. Try to build a labeled benchmark dataset rapidly. Build a minimal viable product with manual rules Are you \u201creally sure\u201d that you need ML at all? 4 - Archetypes So far, we\u2019ve talked about the lifecycle and the impact of all machine learning projects. Ultimately, we generally want these projects, or applications of machine learning, to be useful for products. As we consider how ML can be applied in products, it\u2019s helpful to note that there are common machine learning product archetypes or recurrent patterns through which machine learning is applied to products. You can think of these as \u201cmental models\u201d you can use to assess your project and easily prioritize the needed resources. There are three common archetypes in machine learning projects: Software 2.0 , Human-in-the-loop , and autonomous systems . They are shown in the table below, along with common examples and questions. We\u2019ll dive deeper into each. Archetype Examples Questions Software 2.0 - Improve code completion in IDE - Build customized recommendation system - Build a better video game AI - Do your models truly improve performance? - Does performance improvement generate business value? - Do performance improvements lead to a data flywheel? Human-in-the-loop - Turn sketches into slides - Email auto-completion - Help radiologists do job faster - How good does the system need to be to be useful? - How can you collect enough data to make it good? Autonomous Systems - Full self-driving - Automated customer support - Automated website design - What is an acceptable failure rate for the system? - How can you guarantee that it won\u2019t exceed the failure rate? - How inexpensively can you label data from the system? Software 2.0 Software 2.0, which we previously alluded to from the Karpathy article , is defined as \u201c augmenting existing rules-based or deterministic software with machine learning, a probabilistic approach .\u201d Examples of this are taking a code completer in an IDE and improving the experience for the user by adding an ML component. Rather than suggesting a command based solely on the leading characters the programmer has written, you might add a model that suggests commands based on previous commands the programmer has written. As you build a software 2.0 project, strongly consider the concept of the data flywheel . For certain ML projects, as you improve your model, your product will get better and more users will engage with the product, thereby generating more data for the model to get even better. It\u2019s a classic virtuous cycle and truly the gold standard for ML projects. In embarking on creating a data flywheel, critically consider where the model could fail in relation to your product. For example, do more users lead to collecting more data that is useful for improving your model? An actual system needs to be set up to capture this data and ensure that it's meaningful for the ML lifecycle. Furthermore, consider whether more data will lead to a better model (your job as an ML practitioner) or whether a better model and better predictions will actually lead to making the product better. Ideally, you should have a quantitative assessment of what makes your product \u201cbetter\u201d and map model improvement to it. Human-in-the-Loop (HIL) HIL systems are defined as machine learning systems where the output of your model will be reviewed by a human before being executed in the real world . For example, consider translating sketches into slides. An ML algorithm can take a sketch\u2019s input and suggest to a user a particular slide design. Every output of the ML model is considered and executed upon by a human, who ultimately has to decide on the slide\u2019s design. Autonomous Systems Autonomous systems are defined as machine learning systems where the system itself makes decisions or engages in outputs that are almost never reviewed by a human . Canonically, consider the self-driving car! Feasibility Let\u2019s discuss how the product archetypes relate back to project priority. In terms of feasibility and impact, the two axes on which we consider priority, software 2.0 tends to have high feasibility but potentially low impact. The existing system is often being optimized rather than wholly replaced. However, this status with respect to priority is not static by any means. Building a data flywheel into your software 2.0 project can improve your product\u2019s impact by improving the model\u2019s performance on the task and future ones. In the case of human-in-the-loop systems, their feasibility and impact sit squarely in between autonomous systems and software 2.0. HIL systems, in particular, can benefit disproportionately in their feasibility and impact from effective product design, which naturally takes into account how humans interact with technology and can mitigate risks for machine learning model behavior. Consider how the Facebook photo tagging algorithm is implemented. Rather than tagging the user itself, the algorithm frequently asks the user to tag themselves. This effective product design allows the model to perform more effectively in the user\u2019s eye and reduces the impact of false classifications. Grammarly similarly solicits user input as part of its product design through offering explanations. Finally, recommender systems also implement this idea. In general, good product design can smooth the rough edges of ML (check out the concept of designing collaborative AI ). There are industry-leading resources that can help you merge product design and ML. Apple\u2019s ML product design guidelines suggest three key questions to anyone seeking to put ML into a product: What role does ML play in your product? How can you learn from your users? How should your app handle mistakes? Associated with each question is a set of design paradigms that help address the answers to each question. There are similarly great resources from Microsoft and Spotify . Finally, autonomous systems can see their priority improved by improving their feasibility. Specifically, you can add humans in the loop or reduce the system\u2019s natural autonomy to improve its feasibility. In the case of self-driving cars, many companies add safety drivers as guardrails to improve autonomous systems. In Voyage \u2019s case, they take a more dramatic approach of constraining the problem for the autonomous system: they only run self-driving cars in senior living communities, a narrow subset of the broader self-driving problem. 5 - Metrics So far, we\u2019ve talked about the overall ideas around picking projects and structuring them based on their archetypes and the specific considerations that go into them. Now, we\u2019ll shift gears and be a little more tactical to focus on metrics and baselines, which will help you execute projects more effectively. Choosing a Metric Metrics help us evaluate models . There\u2019s a delicate balance between the real world (which is always messy and multifaceted) and the machine learning paradigm (which optimizes a single metric) in choosing a metric. In practical production settings, we often care about multiple dimensions of performance (i.e., accuracy, speed, cost, etc.). The challenge is to reconcile all the possible evaluation methods with the reality that ML systems work best at optimizing a single number. How can we balance these competing needs in building an ML product? As you start evaluating models, choose a single metric to focus on first , such as precision, accuracy, recall, etc. This can serve as an effective first filter of performance. Subsequently, you can put together a formula that combines all the metrics you care about. Note that it\u2019s important to be flexible and regularly update this formula as your models or the requirements for the product change. Combining Metrics Two simple ways of combining metrics into a formula are averaging and thresholding . Averaging is less common but easy and intuitive; you can just take a simple average or a weighted average of the model\u2019s metrics and pick the highest average. More practically, you can apply a threshold evaluation to the model\u2019s metrics. In this method, out of n evaluation metrics, you threshold n-1 and optimize the nth metric. For example, if we look at a model\u2019s precision, memory requirement, and cost to train, we might threshold the memory requirement (no more than X MB) and the cost (no more than $X) and optimize precision (as high as possible). As you choose which metrics to threshold and what to set their threshold values to, make sure to consider domain-specific needs and the actual values of the metrics (how good/bad they might be). 6 - Baselines In any product development process, setting expectations properly is vital. For machine learning products, baselines help us set expectations for how well our model will perform . In particular, baselines set a useful lower bound for our model\u2019s performance. What\u2019s the minimum expectation we should have for a model\u2019s performance? The better defined and clear the baseline is, the more useful it is for setting the right expectations. Examples of baselines are human performance on a similar task, state-of-the-art models, or even simple heuristics. Baselines are especially important for helping decide the next steps. Consider the example below of two models with the same loss curve but differing performance with respect to the baseline. Clearly, they require different action items! As seen below, on the left, where we are starting to approach or exceed the baseline, we need to be mindful of overfitting and perhaps incorporate regularization of some sort. On the right, where the baseline hugely exceeds our model\u2019s performance, we clearly have a lot of work to do to improve the model and address its underfitting. There are a number of sources to help us define useful baselines. Broadly speaking, there are external baselines (baselines defined by others) or internal baselines you can define yourself. With internal baselines, in particular, you don\u2019t need anything too complicated, or even something with ML! Simple tests like averaging across your dataset can help you understand if your model is achieving meaningful performance. If your model can\u2019t exceed a simple baseline like this, you might need to really re-evaluate the model. Human baselines are a particularly powerful form of baseline since we often seek to replace or augment human actions. In creating these baselines, note that there\u2019s usually an inverse relationship between the quality of the baseline and the ease of data collection. In a nutshell, the harder it is to get a human baseline, the better and more useful it probably is . For example, a Mechanical Turk-created baseline is easy to generate nowadays, but the quality might be hit or miss because of the variance in annotators. However, trained, specialized annotators can be hard to acquire, but the specificity of their knowledge translates into a great baseline. Choosing where to situate your baseline on this range, from low quality/easy to high quality/hard, depends on the domain. Concentrating data collection strategically, ideally in classes where the model is least performant, is a simple way of improving the quality of the baseline. TLDR Machine learning projects are iterative. Deploy something fast to begin the cycle. Choose projects with high impact and low cost of wrong predictions. The secret sauce to make projects work well is to build automated data flywheels. In the real world, you care about many things, but you should always have just one to work on. Good baselines help you invest your effort the right way. Further Resources Andrew Ng\u2019s \u201c Machine Learning Yearning \u201d Andrej Kaparthy\u2019s \u201c Software 2.0 \u201d Agrawal, Gans, and Goldfarb\u2019s \u201c The Economics of AI \u201d Chip Huyen\u2019s \u201c Introduction to Machine Learning Systems Design \u201d Apple\u2019s \u201c Human-Interface Guidelines for Machine Learning \u201d Google\u2019s \u201c Rules of Machine Learning \u201d","title":"Lecture 5: ML Projects"},{"location":"spring2021/lecture-5/#lecture-5-ml-projects","text":"Learn how to set up Machine Learning projects like a pro. This includes an understanding of the ML lifecycle, an acute mind of the feasibility and impact, an awareness of the project archetypes, and an obsession with metrics and baselines.","title":"Lecture 5: ML Projects"},{"location":"spring2021/lecture-5/#video","text":"","title":"Video"},{"location":"spring2021/lecture-5/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lecture-5/#detailed-notes","text":"By James Le and Vishnu Rachakonda","title":"Detailed Notes"},{"location":"spring2021/lecture-5/#1-why-do-ml-projects-fail","text":"Based on a report from TechRepublic a few years back, despite increased interest in adopting machine learning (ML) in the enterprise, 85% of machine learning projects ultimately fail to deliver on their intended promises to business. Failure can happen for many reasons; however, a few glaring dangers will cause any AI project to crash and burn. ML is still very much a research endeavor. Therefore it is very challenging to aim for a 100% success rate. Many ML projects are technically infeasible or poorly scoped. Many ML projects never leap production, thus getting stuck at the prototype phase. Many ML projects have unclear success criteria because of a lack of understanding of the value proposition. Many ML projects are poorly managed because of a lack of interest from leadership.","title":"1 - Why Do ML Projects Fail?"},{"location":"spring2021/lecture-5/#2-lifecycle","text":"It\u2019s essential to understand what constitutes all of the activities in a machine learning project. Typically speaking, there are four major phases: Planning and Project Setup : At this phase, we want to decide the problem to work on, determine the requirements and goals, figure out how to allocate resources properly, consider the ethical implications, etc. Data Collection and Labeling : At this phase, we want to collect training data and potentially annotate them with ground truth, depending on the specific sources where they come from. We may find that it\u2019s too hard to get the data, or it might be easier to label for a different task. If that\u2019s the case, go back to phase 1. Model Training and Model Debugging : At this phase, we want to implement baseline models quickly, find and reproduce state-of-the-art methods for the problem domain, debug our implementation, and improve the model performance for specific tasks. We may realize that we need to collect more data or that labeling is unreliable (thus, go back to phase 2). Or we may recognize that the task is too challenging and there is a tradeoff between project requirements (thus, go back to phase 1). Model Deploying and Model Testing : At this phase, we want to pilot the model in a constrained environment (i.e., in the lab), write tests to prevent regressions, and roll the model into production. We may see that the model doesn\u2019t work well in the lab, so we want to keep improving the model\u2019s accuracy (thus, go back to phase 3). Or we may want to fix the mismatch between training data and production data by collecting more data and mining hard cases (thus go back to phase 2). Or we may find out that the metric picked doesn\u2019t actually drive downstream user behavior, and/or performance in the real world isn\u2019t great. In such situations, we want to revisit the projects\u2019 metrics and requirements (thus, go back to phase 1). Besides the per-project activities mentioned above, there are two other things that any ML team will need to solve across any projects they get involved with: (1) building the team and hiring people; and (2) setting up infrastructure and tooling to build ML systems repeatedly and at scale. Additionally, it might be useful to understand state-of-the-art results in your application domain so that you know what\u2019s possible and what to try next.","title":"2 - Lifecycle"},{"location":"spring2021/lecture-5/#3-prioritizing-projects","text":"To prioritize projects to work on, you want to find high-impact problems and assess the potential costs associated with them. The picture below shows a general framework that encourages us to target projects with high impact and high feasibility.","title":"3 - Prioritizing Projects"},{"location":"spring2021/lecture-5/#high-impact","text":"There are no silver bullets to find high-impact ML problems to work on, but here are a few useful mental models: Where can you take advantage of cheap prediction? Where is there friction in your product? Where can you automate complicated manual processes? What are other people doing?","title":"High Impact"},{"location":"spring2021/lecture-5/#cheap-prediction","text":"In the book \u201c Prediction Machines ,\u201d the authors (Ajay Agrawal, Joshua Gans, and Avi Goldfarb) come up with an excellent mental model on the economics of Artificial Intelligence: As AI reduces the cost of prediction and prediction is central for decision making, cheap predictions would be universal for problems across business domains . Therefore, you should look for projects where cheap predictions will have a huge business impact.","title":"Cheap Prediction"},{"location":"spring2021/lecture-5/#product-needs","text":"Another lens is to think about what your product needs. In the article \u201c Three Principles for Designing ML-Powered Products ,\u201d the Spotify Design team emphasizes the importance of building ML from a product perspective and looking for parts of the product experience with high friction . Automating those parts is exactly where there is a lot of impact for ML to make your business better.","title":"Product Needs"},{"location":"spring2021/lecture-5/#ml-strength","text":"In his popular blog post \u201c Software 2.0 ,\u201d Andrej Karpathy contrasts software 1.0 (which are traditional programs with explicit instructions) and software 2.0 (where humans specify goals, while the algorithm searches for a program that works). Software 2.0 programmers work with datasets, which get compiled via optimization\u200a\u2014\u200awhich works better, more general, and less computationally expensive. Therefore, you should look for complicated rule-based software where we can learn the rules instead of programming them.","title":"ML Strength"},{"location":"spring2021/lecture-5/#inspiration-from-others","text":"Instead of reinventing the wheel, you can look at what other companies are doing. In particular, check out papers from large frontier organizations (Google, Facebook, Nvidia, Netflix, etc.) and blog posts from top earlier-stage companies (Uber, Lyft, Spotify, Stripe, etc.). Here is a list of excellent ML use cases to check out (credit to Chip Huyen\u2019s ML Systems Design Lecture 2 Note ): Human-Centric Machine Learning Infrastructure at Netflix (Ville Tuulos, InfoQ 2019) 2020 state of enterprise machine learning (Algorithmia, 2020) Using Machine Learning to Predict Value of Homes On Airbnb (Robert Chang, Airbnb Engineering & Data Science, 2017) Using Machine Learning to Improve Streaming Quality at Netflix (Chaitanya Ekanadham, Netflix Technology Blog, 2018) 150 Successful Machine Learning Models: 6 Lessons Learned at Booking.com (Bernardi et al., KDD, 2019) How we grew from 0 to 4 million women on our fashion app, with a vertical machine learning approach (Gabriel Aldamiz, HackerNoon, 2018) Machine Learning-Powered Search Ranking of Airbnb Experiences (Mihajlo Grbovic, Airbnb Engineering & Data Science, 2019) From shallow to deep learning in fraud (Hao Yi Ong, Lyft Engineering, 2018) Space, Time and Groceries (Jeremy Stanley, Tech at Instacart, 2017) Creating a Modern OCR Pipeline Using Computer Vision and Deep Learning (Brad Neuberg, Dropbox Engineering, 2017) Scaling Machine Learning at Uber with Michelangelo (Jeremy Hermann and Mike Del Balso, Uber Engineering, 2019) Spotify\u2019s Discover Weekly: How machine learning finds your new music (Sophia Ciocca, 2017)","title":"Inspiration From Others"},{"location":"spring2021/lecture-5/#high-feasibility","text":"The three primary cost drivers of ML projects in order of importance are data availability, accuracy requirement, and problem difficulty.","title":"High Feasibility"},{"location":"spring2021/lecture-5/#data-availability","text":"Here are the questions you need to ask concerning the data availability: How hard is it to acquire data? How expensive is data labeling? How much data will be needed? How stable is the data? What are the data security requirements?","title":"Data Availability"},{"location":"spring2021/lecture-5/#accuracy-requirement","text":"Here are the questions you need to ask concerning the accuracy requirement: How costly are wrong predictions? How frequently does the system need to be right to be useful? What are the ethical implications? It is worth noting that ML project costs tend to scale super-linearly in the accuracy requirement. The fundamental reason is that you typically need a lot more data and more high-quality labels to achieve high accuracy numbers.","title":"Accuracy Requirement"},{"location":"spring2021/lecture-5/#problem-difficulty","text":"Here are the questions you need to ask concerning the problem difficulty: Is the problem well-defined? Is there good published work on similar problems? What are the computing requirements? Can a human do it? So what\u2019s still hard in machine learning? As a caveat, it\u2019s historically very challenging to predict what types of problems will be difficult for ML to solve in the future. But generally speaking, both unsupervised learning and reinforcement learning are still hard, even though they show promise in limited domains where tons of data and compute are available. Zooming into supervised learning , here are three types of hard problems: Output is complex: These are problems where the output is high-dimensional or ambiguous. Examples include 3D reconstruction, video prediction, dialog systems, open-ended recommendation systems, etc. Reliability is required: These are problems where high precision and robustness are required. Examples include systems that can fail safely in out-of-distribution scenarios, is robust to adversarial attacks, or needs to tackle highly precise tasks. Generalization is required: These are problems with out-of-distribution data or in the domains of reasoning, planning, and causality. Examples include any systems for self-driving vehicles or any systems that deal with small data. Finally, this is a nice checklist for you to run an ML feasibility assessment: Are you sure that you need ML at all? Put in the work upfront to define success criteria with all of the stakeholders. Consider the ethics of using ML. Do a literature review. Try to build a labeled benchmark dataset rapidly. Build a minimal viable product with manual rules Are you \u201creally sure\u201d that you need ML at all?","title":"Problem Difficulty"},{"location":"spring2021/lecture-5/#4-archetypes","text":"So far, we\u2019ve talked about the lifecycle and the impact of all machine learning projects. Ultimately, we generally want these projects, or applications of machine learning, to be useful for products. As we consider how ML can be applied in products, it\u2019s helpful to note that there are common machine learning product archetypes or recurrent patterns through which machine learning is applied to products. You can think of these as \u201cmental models\u201d you can use to assess your project and easily prioritize the needed resources. There are three common archetypes in machine learning projects: Software 2.0 , Human-in-the-loop , and autonomous systems . They are shown in the table below, along with common examples and questions. We\u2019ll dive deeper into each. Archetype Examples Questions Software 2.0 - Improve code completion in IDE - Build customized recommendation system - Build a better video game AI - Do your models truly improve performance? - Does performance improvement generate business value? - Do performance improvements lead to a data flywheel? Human-in-the-loop - Turn sketches into slides - Email auto-completion - Help radiologists do job faster - How good does the system need to be to be useful? - How can you collect enough data to make it good? Autonomous Systems - Full self-driving - Automated customer support - Automated website design - What is an acceptable failure rate for the system? - How can you guarantee that it won\u2019t exceed the failure rate? - How inexpensively can you label data from the system?","title":"4 - Archetypes"},{"location":"spring2021/lecture-5/#software-20","text":"Software 2.0, which we previously alluded to from the Karpathy article , is defined as \u201c augmenting existing rules-based or deterministic software with machine learning, a probabilistic approach .\u201d Examples of this are taking a code completer in an IDE and improving the experience for the user by adding an ML component. Rather than suggesting a command based solely on the leading characters the programmer has written, you might add a model that suggests commands based on previous commands the programmer has written. As you build a software 2.0 project, strongly consider the concept of the data flywheel . For certain ML projects, as you improve your model, your product will get better and more users will engage with the product, thereby generating more data for the model to get even better. It\u2019s a classic virtuous cycle and truly the gold standard for ML projects. In embarking on creating a data flywheel, critically consider where the model could fail in relation to your product. For example, do more users lead to collecting more data that is useful for improving your model? An actual system needs to be set up to capture this data and ensure that it's meaningful for the ML lifecycle. Furthermore, consider whether more data will lead to a better model (your job as an ML practitioner) or whether a better model and better predictions will actually lead to making the product better. Ideally, you should have a quantitative assessment of what makes your product \u201cbetter\u201d and map model improvement to it.","title":"Software 2.0"},{"location":"spring2021/lecture-5/#human-in-the-loop-hil","text":"HIL systems are defined as machine learning systems where the output of your model will be reviewed by a human before being executed in the real world . For example, consider translating sketches into slides. An ML algorithm can take a sketch\u2019s input and suggest to a user a particular slide design. Every output of the ML model is considered and executed upon by a human, who ultimately has to decide on the slide\u2019s design.","title":"Human-in-the-Loop (HIL)"},{"location":"spring2021/lecture-5/#autonomous-systems","text":"Autonomous systems are defined as machine learning systems where the system itself makes decisions or engages in outputs that are almost never reviewed by a human . Canonically, consider the self-driving car!","title":"Autonomous Systems"},{"location":"spring2021/lecture-5/#feasibility","text":"Let\u2019s discuss how the product archetypes relate back to project priority. In terms of feasibility and impact, the two axes on which we consider priority, software 2.0 tends to have high feasibility but potentially low impact. The existing system is often being optimized rather than wholly replaced. However, this status with respect to priority is not static by any means. Building a data flywheel into your software 2.0 project can improve your product\u2019s impact by improving the model\u2019s performance on the task and future ones. In the case of human-in-the-loop systems, their feasibility and impact sit squarely in between autonomous systems and software 2.0. HIL systems, in particular, can benefit disproportionately in their feasibility and impact from effective product design, which naturally takes into account how humans interact with technology and can mitigate risks for machine learning model behavior. Consider how the Facebook photo tagging algorithm is implemented. Rather than tagging the user itself, the algorithm frequently asks the user to tag themselves. This effective product design allows the model to perform more effectively in the user\u2019s eye and reduces the impact of false classifications. Grammarly similarly solicits user input as part of its product design through offering explanations. Finally, recommender systems also implement this idea. In general, good product design can smooth the rough edges of ML (check out the concept of designing collaborative AI ). There are industry-leading resources that can help you merge product design and ML. Apple\u2019s ML product design guidelines suggest three key questions to anyone seeking to put ML into a product: What role does ML play in your product? How can you learn from your users? How should your app handle mistakes? Associated with each question is a set of design paradigms that help address the answers to each question. There are similarly great resources from Microsoft and Spotify . Finally, autonomous systems can see their priority improved by improving their feasibility. Specifically, you can add humans in the loop or reduce the system\u2019s natural autonomy to improve its feasibility. In the case of self-driving cars, many companies add safety drivers as guardrails to improve autonomous systems. In Voyage \u2019s case, they take a more dramatic approach of constraining the problem for the autonomous system: they only run self-driving cars in senior living communities, a narrow subset of the broader self-driving problem.","title":"Feasibility"},{"location":"spring2021/lecture-5/#5-metrics","text":"So far, we\u2019ve talked about the overall ideas around picking projects and structuring them based on their archetypes and the specific considerations that go into them. Now, we\u2019ll shift gears and be a little more tactical to focus on metrics and baselines, which will help you execute projects more effectively.","title":"5 - Metrics"},{"location":"spring2021/lecture-5/#choosing-a-metric","text":"Metrics help us evaluate models . There\u2019s a delicate balance between the real world (which is always messy and multifaceted) and the machine learning paradigm (which optimizes a single metric) in choosing a metric. In practical production settings, we often care about multiple dimensions of performance (i.e., accuracy, speed, cost, etc.). The challenge is to reconcile all the possible evaluation methods with the reality that ML systems work best at optimizing a single number. How can we balance these competing needs in building an ML product? As you start evaluating models, choose a single metric to focus on first , such as precision, accuracy, recall, etc. This can serve as an effective first filter of performance. Subsequently, you can put together a formula that combines all the metrics you care about. Note that it\u2019s important to be flexible and regularly update this formula as your models or the requirements for the product change.","title":"Choosing a Metric"},{"location":"spring2021/lecture-5/#combining-metrics","text":"Two simple ways of combining metrics into a formula are averaging and thresholding . Averaging is less common but easy and intuitive; you can just take a simple average or a weighted average of the model\u2019s metrics and pick the highest average. More practically, you can apply a threshold evaluation to the model\u2019s metrics. In this method, out of n evaluation metrics, you threshold n-1 and optimize the nth metric. For example, if we look at a model\u2019s precision, memory requirement, and cost to train, we might threshold the memory requirement (no more than X MB) and the cost (no more than $X) and optimize precision (as high as possible). As you choose which metrics to threshold and what to set their threshold values to, make sure to consider domain-specific needs and the actual values of the metrics (how good/bad they might be).","title":"Combining Metrics"},{"location":"spring2021/lecture-5/#6-baselines","text":"In any product development process, setting expectations properly is vital. For machine learning products, baselines help us set expectations for how well our model will perform . In particular, baselines set a useful lower bound for our model\u2019s performance. What\u2019s the minimum expectation we should have for a model\u2019s performance? The better defined and clear the baseline is, the more useful it is for setting the right expectations. Examples of baselines are human performance on a similar task, state-of-the-art models, or even simple heuristics. Baselines are especially important for helping decide the next steps. Consider the example below of two models with the same loss curve but differing performance with respect to the baseline. Clearly, they require different action items! As seen below, on the left, where we are starting to approach or exceed the baseline, we need to be mindful of overfitting and perhaps incorporate regularization of some sort. On the right, where the baseline hugely exceeds our model\u2019s performance, we clearly have a lot of work to do to improve the model and address its underfitting. There are a number of sources to help us define useful baselines. Broadly speaking, there are external baselines (baselines defined by others) or internal baselines you can define yourself. With internal baselines, in particular, you don\u2019t need anything too complicated, or even something with ML! Simple tests like averaging across your dataset can help you understand if your model is achieving meaningful performance. If your model can\u2019t exceed a simple baseline like this, you might need to really re-evaluate the model. Human baselines are a particularly powerful form of baseline since we often seek to replace or augment human actions. In creating these baselines, note that there\u2019s usually an inverse relationship between the quality of the baseline and the ease of data collection. In a nutshell, the harder it is to get a human baseline, the better and more useful it probably is . For example, a Mechanical Turk-created baseline is easy to generate nowadays, but the quality might be hit or miss because of the variance in annotators. However, trained, specialized annotators can be hard to acquire, but the specificity of their knowledge translates into a great baseline. Choosing where to situate your baseline on this range, from low quality/easy to high quality/hard, depends on the domain. Concentrating data collection strategically, ideally in classes where the model is least performant, is a simple way of improving the quality of the baseline.","title":"6 - Baselines"},{"location":"spring2021/lecture-5/#tldr","text":"Machine learning projects are iterative. Deploy something fast to begin the cycle. Choose projects with high impact and low cost of wrong predictions. The secret sauce to make projects work well is to build automated data flywheels. In the real world, you care about many things, but you should always have just one to work on. Good baselines help you invest your effort the right way.","title":"TLDR"},{"location":"spring2021/lecture-5/#further-resources","text":"Andrew Ng\u2019s \u201c Machine Learning Yearning \u201d Andrej Kaparthy\u2019s \u201c Software 2.0 \u201d Agrawal, Gans, and Goldfarb\u2019s \u201c The Economics of AI \u201d Chip Huyen\u2019s \u201c Introduction to Machine Learning Systems Design \u201d Apple\u2019s \u201c Human-Interface Guidelines for Machine Learning \u201d Google\u2019s \u201c Rules of Machine Learning \u201d","title":"Further Resources"},{"location":"spring2021/lecture-6/","text":"Lecture 6: Infrastructure & Tooling Video Slides PDF Download Detailed Notes 1 - Dream vs. Reality for ML Practitioners The dream of ML practitioners is that we are provided the data, and somehow we build an optimal machine learning prediction system available as a scalable API or an edge deployment. That deployment then generates more data for us, which can be used to improve our system. The reality is that you will have to: Aggregate, process, clean, label, and version the data Write and debug model code Provision compute Run many experiments and review the results Discover that you did something wrong or maybe try a different architecture -> Write more code and provision more compute Deploy the model when you are happy Monitor the predictions that the model makes on production data so that you can gather some good examples and feed them back to the initial data flywheel loop For example, the slide above is from Andrej Karpathy\u2019s talk at PyTorch Devcon 2019 discussing Tesla\u2019s self-driving system. Their dream is to build a system that goes from the data gathered through their training, evaluation, and inference processes and gets deployed on the cars. As people drive, more data will be collected and added back to the training set. As this process repeats, Tesla\u2019s ML engineers can all go on vacation :) The picture above (from the famous Google paper \u201c Machine Learning: The High-Interest Credit Card of Technical Debt \u201d) shows that the ML code portion in a real-world ML system is a lot smaller than the infrastructure needed for its support . As ML projects move from small-scale research experiments to large-scale industry deployments, your organization most likely will require a massive amount of infrastructure to support large inferences, distributed training, data processing pipelines, reproducible experiments, model monitoring, etc. 2 - Three Buckets of Tooling Landscape We can break down the landscape of all this necessary infrastructure into three buckets: data, training/evaluation, and deployment. The data bucket includes the data sources, data lakes/warehouses, data processing, data exploration, data versioning, and data labeling. The training/evaluation bucket includes compute sources, resource management, software engineering, frameworks and distributed training libraries, experiment management, and hyper-parameter tuning. The deployment bucket includes continuous integration and testing, edge deployment, web deployment, monitoring, and feature store. There are also several vendors offering \u201call-in-one\u201d MLOps solutions that cover all three buckets. This lecture focuses on the training/evaluation bucket. 3 - Software Engineering When it comes to writing deep learning code, Python is the clear programming language of choice . As a general-purpose language, Python is easy to learn and easily accessible, enabling you to find skilled developers on a faster basis. It has various scientific libraries for data wrangling and machine learning (Pandas, NumPy, Scikit-Learn, etc.). Regardless of whether your engineering colleagues write code in a lower-level language like C, C++, or Java, it is generally neat to join different components with a Python wrapper. When choosing your IDEs, there are many options out there (Vim, Emacs, Sublime Text, Jupyter, VS Code, PyCharm, Atom, etc.). Each of these has its uses in any application, and you\u2019re better to switch between them to remain agile without relying heavily on shortcuts and packages. It also helps teams work better if they can jump into different IDEs and comment/collaborate with other colleagues. In particular, Visual Studio Code makes for a very nice Python experience, where you have access to built-in git staging and diffing, peek at documentation, linter code as you write, and open projects remotely. Jupyter Notebooks have rapidly grown in popularity among data scientists to become the standard for quick prototyping and exploratory analysis. For example, Netflix based all of their machine learning workflows on them , effectively building a whole notebook infrastructure to leverage them as a unifying layer for scheduling workflows. Jeremy Howard develops his fast.ai codebase entirely with notebooks and introduces a project called nbdev that shows people how to develop well-tested code in a notebook environment. However, there are many problems with using notebooks as a last resort when working in teams that aim to build machine/deep learning products . Alexander Mueller's blog post outlines the five reasons why they suck: It is challenging to enable good code versioning because notebooks are big JSON files that cannot be merged automatically. Notebook \u201cIDE\u201d is primitive , as they have no integration, no lifting, and no code-style correction. Data scientists are not software engineers, and thus, tools that govern their code quality and help improve it are very important. It is very hard to structure code reasonably, put code into functions, and develop tests while working in notebooks. You better develop Python scripts based on test-driven development principles as soon as you want to reproduce some experiments and run notebooks frequently. Notebooks have out-of-order execution artifacts , meaning that you can easily destroy your current working state when jumping between cells of notebooks. It is also difficult to run long or distributed tasks . If you want to handle big datasets, better pull your code out of notebooks, start a Python folder, create fixtures, write tests, and then deploy your application to a cluster. Recently, a new application framework called Streamlit was introduced. The creators of the framework wanted machine learning engineers to be able to create beautiful apps without needing a tools team; in other words, these internal tools should arise as a natural byproduct of the machine learning workflow. According to the launch blog post , here are the core principles of Streamlit: Embrace Python scripting : Streamlit apps are just scripts that run from top to bottom. There\u2019s no hidden state. You can factor your code with function calls. If you know how to write Python scripts, you can write Streamlit apps. Treat widgets as variables : There are no callbacks in Streamlit. Every interaction simply reruns the script from top to bottom. This approach leads to a clean codebase. Reuse data and computation : Streamlit introduces a cache primitive that behaves like a persistent, immutable-by-default data store that lets Streamlit apps safely and effortlessly reuse information. Right now, Streamlit is building features that enable sharing machine learning projects to be as easy as pushing a web app to Heroku. We recommend using conda to set up your Python and CUDA environments and pip-tools to separate mutually compatible versions of all requirements for our lab . 4 - Compute Hardware We can break down the compute needs into an early-stage development step and a late-stage training/evaluation step. During the development stage, we write code, debug models, and look at the results. It\u2019d be nice to be able to compile and train models via an intuitive GUI quickly. During the training/evaluation stage, we design model architecture, search for hyper-parameters, and train large models. It\u2019d be nice to launch experiments and review results easily. Compute matters with each passing year due to the fact that the results came out of deep learning are using more and more compute (check out this 2018 report from OpenAI ). Looking at recent Transformer models, while OpenAI\u2019s GPT-3 has not been fully commercialized yet, Google already released the Switch Transformer with orders of magnitude larger in the number of parameters. So should you get your own hardware, go straight to the cloud, or use on-premise options? GPU Basics This is basically an NVIDIA game, as they are the only provider of good deep learning GPUs. However, Google\u2019s TPUs are the fastest, which is available only on GCP. There is a new NVIDIA architecture every year: Kepler -> Pascal -> Volta -> Turing -> Ampere. NVIDIA often released the server version of the cards first, then the \u201centhusiast\u201d version, and finally the consumer version. If you use these cards for business purposes, then you suppose to use the server version. GPUs have a different amount of RAM. You can only compute on the data that is on the GPU memory. The more data you can fit on the GPU, the larger your batches are, the faster your training goes . For deep learning, you use 32-bit precision. In fact, starting with the Volta architecture, NVIDIA developed tensor cores that are specifically designed for deep learning operations (mixed-precision between 32 and 16 bit). Tensor Cores reduce the used cycles needed for calculating multiply and addition operations and the reliance on repetitive shared memory access, thus saving additional cycles for memory access. This is very useful for the convolutional/Transformer models that are prevalent nowadays. Let\u2019s go through different GPU architectures: Kepler / Maxwell : They are 2-4x slower than the Pascal/Volta ones below. You should not buy these old guards (K80). Pascal : They are in the 1080 Ti cards from 2017, which are still useful if bought used (especially for recurrent neural networks). P100 is the equivalent cloud offering. Volta / Turing : These are the preferred choices over the Kepler and Pascal because of their support for 16-bit mixed-precision via tensor cores. Hardware options are 2080 Ti and Titan RTX, while the cloud option is V100. Ampere : This architecture is available in the latest hardware (3090) and cloud (A100) offerings. They have the most tensor cores, leading to at least 30% speedup over Turing. You can check out this recent GPU benchmark from Lambda Labs and consult Tim Dettmers\u2019 advice on which GPUs to get . Cloud Options Amazon Web Services, Google Cloud Platform, and Microsoft Azure are the cloud heavyweights with largely similar functions and prices. There are also startups like Lambda Labs and Corewave that provide cloud GPUs. On-Prem Options You can either build your own or buy pre-built devices from vendors like Lambda Labs, NVIDIA, Supermicro, Cirrascale, etc. Recommendations Even though the cloud is expensive, it\u2019s hard to make on-prem devices scale past a certain point. Furthermore, dev-ops things are easier to be done in the cloud than to be set up by yourself. And if your machine dies or requires maintenance, that will be a constant headache if you are responsible for managing it. Here are our recommendations for three profiles: Hobbyists: Build your own machine (maybe a 4x Turing or a 2x Ampere PC) during development. Either use the same PC or use cloud instances during training/evaluation. Startups: Buy a sizeable Lambda Labs machine for every ML scientist during development. Buy more shared server machines or use cloud instances during training/evaluation. Larger companies: Buy an even more powerful machine for every ML scientist during development. Use cloud with fast instances with proper provisioning and handling of failures during training/evaluation. 5 - Resource Management With all the resources we have discussed (compute, dependencies, etc.), our challenge turns to manage them across the specific use cases we may have. Across all the resources, our goal is always to be able to easily experiment with the necessary resources to achieve the desired application of ML for our product. For this challenge of allocating resources to experimenting users, there are some common solutions: Script a solution ourselves : In theory, this is the simplest solution. We can check if a resource is free and then lock it if a particular user is using it or wants to. SLURM : If we don't want to write the script entirely ourselves, standard cluster job schedulers like SLURM can help us. The workflow is as follows: First, a script defines a job\u2019s requirements. Then, the SLURM queue runner analyzes this and then executes the jobs on the correct resource. Docker/Kubernetes : The above approach might still be too manual for your needs, in which case you can turn to Docker/Kubernetes. Docker packages the dependency stack into a lighter-than-VM package called a container (that excludes the OS). Kubernetes lets us run these Docker containers on a cluster. In particular, Kubeflow is an OSS project started by Google that allows you to spawn/manage Jupyter notebooks and manage multi-step workflows. It also has lots of plug-ins for extra processes like hyperparameter tuning and model deployment. However, Kubeflow can be a challenge to setup. Custom ML software : There\u2019s a lot of novel work and all-in-one solutions being developed to provision compute resources for ML development efficiently. Platforms like AWS Sagemaker , Paperspace Gradient , and Determined AI are advancing. Newer startups like Anyscale and Grid.AI (creators of PyTorch Lightning) are also tackling this. Their vision is around allowing you to seamlessly go from training models on your computer to running lots of training jobs in the cloud with a simple set of SDK commands. 6 - Frameworks and Distributed Training Deep Learning Frameworks If you\u2019ve built a deep learning model in the last few years, you\u2019ve probably used a deep learning framework. Frameworks like TensorFlow have crucially shaped the development of the deep learning revolution. The reality is that deep learning frameworks have existed for a while. Projects like Theano and Torch have been around for 10+ years. In contemporary use, there are three main frameworks we\u2019ll focus on - TensorFlow , Keras , and PyTorch . We evaluate frameworks based on their utility for production and development . When TensorFlow came out in 2015, it was billed heavily as a production-optimized DL framework with an underlying static optimized graph that could be deployed across compute environments. However, TF 1.0 had a pretty unpleasant development experience; in addition to developing your models, you had to consider the underlying execution graph you were describing. This kind of \u201cmeta-development\u201d posed a challenge for newcomers. The Keras project solved many of these issues by offering a simpler way to define models, and eventually became a part of TensorFlow. PyTorch, when it was introduced in 2017, offered a polar opposite to TensorFlow. It made development super easy by consisting almost exclusively of simple Python commands, but was not designed to be fast at scale. Using TF/Keras or PyTorch is the current recommended way to build deep learning models unless you have a powerful reason not to. Essentially, both have converged to pretty similar points that balance development and production. TensorFlow adopted eager execution by default and became a lot easier to develop quickly in. PyTorch subsumed Caffe2 and became much faster as a result, specifically by adding the ability to compile speedier model artifacts. Nowadays, PyTorch has a lot of momentum, likely due to its ease of development. Newer projects like fast.ai and PyTorch Lighting add best practices and additional functionality to PyTorch, making it even more popular. According to this 2018 article on The Gradient , more than 80% of submissions are in PyTorch in academic projects. All these frameworks may seem like excessive quibbling, especially since PyTorch and TensorFlow have converged in important ways. Why do we even require such extensive frameworks? It\u2019s theoretically possible to define entire models and their required matrix math (e.g., a CNN) in NumPy, the classic Python numerical computing library. However, we quickly run into two challenges: back-propagating errors through our model and running the code on GPUs, which are powerful computation accelerators. For these issues to be addressed, we need frameworks to help us with auto-differentiation , an efficient way of computing the gradients, and software compatibility with GPUs , specifically interfacing with CUDA. Frameworks allow us to abstract the work required to achieve both features, while also layering in valuable abstractions for all the latest layer designs, optimizers, losses, and much more. As you can imagine, the abstractions offered by frameworks save us valuable time on getting our model to run and allow us to focus on optimizing our model. New projects like JAX and HuggingFace offer different or simpler abstractions. JAX focuses primarily on fast numerical computation with autodiff and GPUs across machine learning use cases (not just deep learning). HuggingFace abstracts entire model architectures in the NLP realm. Instead of loading individual layers, HuggingFace lets you load the entirety of a contemporary mode (along with weights)l like BERT, tremendously speeding up development time. HuggingFace works on both PyTorch and TensorFlow. Distributed Training Distributed training is a hot topic as the datasets and the models we train become too large to work on a single GPU. It\u2019s increasingly a must-do. The important thing to note is that distributed training is a process to conduct a single model training process ; don\u2019t confuse it with training multiple models on different GPUs. There are two approaches to distributed training: data parallelism and model parallelism. Data Parallelism Data parallelism is quite simple but powerful. If we have a batch size of X samples, which is too large for one GPU, we can split the X samples evenly across N GPUs. Each GPU calculates the gradients and passes them to a central node (either a GPU or a CPU), where the gradients are averaged and backpropagated through the distributed GPUs. This paradigm generally results in a linear speed-up time (e.g., two distributed GPUs results in a ~2X speed-up in training time). In modern frameworks like PyTorch, PyTorch Lightning, and even in schedulers like SLURM, data-parallel training can be achieved simply by specifying the number of GPUs or calling a data parallelism-enabling object (e.g., torch.nn.DataParallel ). Other tools like Horovod (from Uber) use non-framework-specific ways of enabling data parallelism (e.g., MPI, a standard multiprocessing framework). Ray , the original open-source project from the Anyscale team, was designed to enable general distributed computing applications in Python and can be similarly applied to data-parallel distributed training. Model Parallelism Model parallelism is a lot more complicated. If you can\u2019t fit your entire model\u2019s weights on a single GPU, you can split the weights across GPUs and pass data through each to train the weights. This usually adds a lot of complexity and should be avoided unless absolutely necessary. A better solution is to pony up for the best GPU available, either locally or in the cloud. You can also use gradient checkpointing, a clever trick wherein you write some gradients to disk as you compute them and load them only as you need them for updates. New work is coming out to make this easier (e.g., research and framework maturity). 7 - Experiment Management As you run numerous experiments to refine your model, it\u2019s easy to lose track of code, hyperparameters, and artifacts. Model iteration can lead to lots of complexity and messiness. For example, you could be monitoring the learning rate\u2019s impact on your model\u2019s performance metric. With multiple model runs, how will you monitor the impact of the hyperparameter? A low-tech way would be to manually track the results of all model runs in a spreadsheet. Without great attention to detail, this can quickly spiral into a messy or incomplete artifact. Dedicated experiment management platforms are a remedy to this issue. Let\u2019s cover a few of the most common ones: TensorBoard : This is the default experiment tracking platform that comes with TensorFlow. As a pro, it\u2019s easy to get started with. On the flip side, it\u2019s not very good for tracking and comparing multiple experiments. It\u2019s also not the best solution to store past work. MLFlow : An OSS project from Databricks, MLFlow is a complete platform for the ML lifecycle. They have great experiment and model run management at the core of their platform. Another open-source project, Keepsake , recently came out focused solely on experiment tracking. Paid platforms ( Comet.ml , Weights and Biases , Neptune ): Finally, outside vendors offer deep, thorough experiment management platforms, with tools like code diffs, report writing, data visualization, and model registering features. In our labs, we will use Weights and Biases. 8 - Hyperparameter Tuning To finalize models, we need to ensure that we have the optimal hyperparameters. Since hyperparameter optimization (as this process is called) can be a particularly compute-intensive process, it\u2019s useful to have software that can help. Using specific software can help us kill underperforming model runs with bad hyperparameters early (to save on cost) or help us intelligently sweep ranges of hyperparameter values. Luckily, there\u2019s an increasing number of software providers that do precisely this: SigOpt offers an API focused exclusively on efficient, iterative hyperparameter optimization. Specify a range of values, get SigOpt\u2019s recommended hyperparameter settings, run the model and return the results to SigOpt, and repeat the process until you\u2019ve found the best parameters for your model. Rather than an API, Ray Tune offers a local software (part of the broader Ray ecosystem) that integrates hyperparameter optimization with compute resource allocation. Jobs are scheduled with specific hyperparameters according to state-of-the-art methods, and underperforming jobs are automatically killed. Weights and Biases also has this feature! With a YAML file specification, we can specify a hyperparameter optimization job and perform a \u201c sweep ,\u201d during which W&B sends parameter settings to individual \u201cagents\u201d (our machines) and compares performance. 9 - \u201cAll-In-One\u201d Solutions Some platforms integrate all the aspects of the applied ML stack we\u2019ve discussed (experiment tracking, optimization, training, etc.) and wrap them into a single experience. To support the \u201clifecycle,\u201d these platforms typically include: Labeling and data querying services Model training, especially though job scaling and scheduling Experiment tracking and model versioning Development environments, typically through notebook-style interfaces Model deployment (e.g., via REST APIs) and monitoring One of the earliest examples of such a system is Facebook\u2019s FBLearner (2016), which encompassed data and feature storage, training, inference, and continuous learning based on user interactions with the model\u2019s outputs. You can imagine how powerful having one hub for all this activity can be for ML application and development speed. As a result, cloud vendors (Google, AWS, Azure) have developed similar all-in-one platforms, like Google Cloud AI Platform and AWS SageMaker . Startups like Paperspace Gradient , Neptune , and FloydHub also offer all-in-one platforms focused on deep learning. Determined AI , which focuses exclusively on the model development and training part of the lifecycle, is the rare open-source platform in this space. Domino Data Lab is a traditional ML-focused startup with an extensive feature set worth looking at. It\u2019s natural to expect more MLOps (as this kind of tooling and infra is referred to) companies and vendors to build out their feature set and become platform-oriented; Weights and Biases is a good example of this. In conclusion, take a look at the below table to compare a select number of MLOps platform vendors. Pricing is quite variable. Staying up to date across all the tooling can be a real challenge, but check out FSDL\u2019s Tooling Tuesdays on Twitter as a starting point!","title":"Lecture 6: Infrastructure & Tooling"},{"location":"spring2021/lecture-6/#lecture-6-infrastructure-tooling","text":"","title":"Lecture 6: Infrastructure &amp; Tooling"},{"location":"spring2021/lecture-6/#video","text":"","title":"Video"},{"location":"spring2021/lecture-6/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lecture-6/#detailed-notes","text":"","title":"Detailed Notes"},{"location":"spring2021/lecture-6/#1-dream-vs-reality-for-ml-practitioners","text":"The dream of ML practitioners is that we are provided the data, and somehow we build an optimal machine learning prediction system available as a scalable API or an edge deployment. That deployment then generates more data for us, which can be used to improve our system. The reality is that you will have to: Aggregate, process, clean, label, and version the data Write and debug model code Provision compute Run many experiments and review the results Discover that you did something wrong or maybe try a different architecture -> Write more code and provision more compute Deploy the model when you are happy Monitor the predictions that the model makes on production data so that you can gather some good examples and feed them back to the initial data flywheel loop For example, the slide above is from Andrej Karpathy\u2019s talk at PyTorch Devcon 2019 discussing Tesla\u2019s self-driving system. Their dream is to build a system that goes from the data gathered through their training, evaluation, and inference processes and gets deployed on the cars. As people drive, more data will be collected and added back to the training set. As this process repeats, Tesla\u2019s ML engineers can all go on vacation :) The picture above (from the famous Google paper \u201c Machine Learning: The High-Interest Credit Card of Technical Debt \u201d) shows that the ML code portion in a real-world ML system is a lot smaller than the infrastructure needed for its support . As ML projects move from small-scale research experiments to large-scale industry deployments, your organization most likely will require a massive amount of infrastructure to support large inferences, distributed training, data processing pipelines, reproducible experiments, model monitoring, etc.","title":"1 - Dream vs. Reality for ML Practitioners"},{"location":"spring2021/lecture-6/#2-three-buckets-of-tooling-landscape","text":"We can break down the landscape of all this necessary infrastructure into three buckets: data, training/evaluation, and deployment. The data bucket includes the data sources, data lakes/warehouses, data processing, data exploration, data versioning, and data labeling. The training/evaluation bucket includes compute sources, resource management, software engineering, frameworks and distributed training libraries, experiment management, and hyper-parameter tuning. The deployment bucket includes continuous integration and testing, edge deployment, web deployment, monitoring, and feature store. There are also several vendors offering \u201call-in-one\u201d MLOps solutions that cover all three buckets. This lecture focuses on the training/evaluation bucket.","title":"2 - Three Buckets of Tooling Landscape"},{"location":"spring2021/lecture-6/#3-software-engineering","text":"When it comes to writing deep learning code, Python is the clear programming language of choice . As a general-purpose language, Python is easy to learn and easily accessible, enabling you to find skilled developers on a faster basis. It has various scientific libraries for data wrangling and machine learning (Pandas, NumPy, Scikit-Learn, etc.). Regardless of whether your engineering colleagues write code in a lower-level language like C, C++, or Java, it is generally neat to join different components with a Python wrapper. When choosing your IDEs, there are many options out there (Vim, Emacs, Sublime Text, Jupyter, VS Code, PyCharm, Atom, etc.). Each of these has its uses in any application, and you\u2019re better to switch between them to remain agile without relying heavily on shortcuts and packages. It also helps teams work better if they can jump into different IDEs and comment/collaborate with other colleagues. In particular, Visual Studio Code makes for a very nice Python experience, where you have access to built-in git staging and diffing, peek at documentation, linter code as you write, and open projects remotely. Jupyter Notebooks have rapidly grown in popularity among data scientists to become the standard for quick prototyping and exploratory analysis. For example, Netflix based all of their machine learning workflows on them , effectively building a whole notebook infrastructure to leverage them as a unifying layer for scheduling workflows. Jeremy Howard develops his fast.ai codebase entirely with notebooks and introduces a project called nbdev that shows people how to develop well-tested code in a notebook environment. However, there are many problems with using notebooks as a last resort when working in teams that aim to build machine/deep learning products . Alexander Mueller's blog post outlines the five reasons why they suck: It is challenging to enable good code versioning because notebooks are big JSON files that cannot be merged automatically. Notebook \u201cIDE\u201d is primitive , as they have no integration, no lifting, and no code-style correction. Data scientists are not software engineers, and thus, tools that govern their code quality and help improve it are very important. It is very hard to structure code reasonably, put code into functions, and develop tests while working in notebooks. You better develop Python scripts based on test-driven development principles as soon as you want to reproduce some experiments and run notebooks frequently. Notebooks have out-of-order execution artifacts , meaning that you can easily destroy your current working state when jumping between cells of notebooks. It is also difficult to run long or distributed tasks . If you want to handle big datasets, better pull your code out of notebooks, start a Python folder, create fixtures, write tests, and then deploy your application to a cluster. Recently, a new application framework called Streamlit was introduced. The creators of the framework wanted machine learning engineers to be able to create beautiful apps without needing a tools team; in other words, these internal tools should arise as a natural byproduct of the machine learning workflow. According to the launch blog post , here are the core principles of Streamlit: Embrace Python scripting : Streamlit apps are just scripts that run from top to bottom. There\u2019s no hidden state. You can factor your code with function calls. If you know how to write Python scripts, you can write Streamlit apps. Treat widgets as variables : There are no callbacks in Streamlit. Every interaction simply reruns the script from top to bottom. This approach leads to a clean codebase. Reuse data and computation : Streamlit introduces a cache primitive that behaves like a persistent, immutable-by-default data store that lets Streamlit apps safely and effortlessly reuse information. Right now, Streamlit is building features that enable sharing machine learning projects to be as easy as pushing a web app to Heroku. We recommend using conda to set up your Python and CUDA environments and pip-tools to separate mutually compatible versions of all requirements for our lab .","title":"3 - Software Engineering"},{"location":"spring2021/lecture-6/#4-compute-hardware","text":"We can break down the compute needs into an early-stage development step and a late-stage training/evaluation step. During the development stage, we write code, debug models, and look at the results. It\u2019d be nice to be able to compile and train models via an intuitive GUI quickly. During the training/evaluation stage, we design model architecture, search for hyper-parameters, and train large models. It\u2019d be nice to launch experiments and review results easily. Compute matters with each passing year due to the fact that the results came out of deep learning are using more and more compute (check out this 2018 report from OpenAI ). Looking at recent Transformer models, while OpenAI\u2019s GPT-3 has not been fully commercialized yet, Google already released the Switch Transformer with orders of magnitude larger in the number of parameters. So should you get your own hardware, go straight to the cloud, or use on-premise options?","title":"4 - Compute Hardware"},{"location":"spring2021/lecture-6/#gpu-basics","text":"This is basically an NVIDIA game, as they are the only provider of good deep learning GPUs. However, Google\u2019s TPUs are the fastest, which is available only on GCP. There is a new NVIDIA architecture every year: Kepler -> Pascal -> Volta -> Turing -> Ampere. NVIDIA often released the server version of the cards first, then the \u201centhusiast\u201d version, and finally the consumer version. If you use these cards for business purposes, then you suppose to use the server version. GPUs have a different amount of RAM. You can only compute on the data that is on the GPU memory. The more data you can fit on the GPU, the larger your batches are, the faster your training goes . For deep learning, you use 32-bit precision. In fact, starting with the Volta architecture, NVIDIA developed tensor cores that are specifically designed for deep learning operations (mixed-precision between 32 and 16 bit). Tensor Cores reduce the used cycles needed for calculating multiply and addition operations and the reliance on repetitive shared memory access, thus saving additional cycles for memory access. This is very useful for the convolutional/Transformer models that are prevalent nowadays. Let\u2019s go through different GPU architectures: Kepler / Maxwell : They are 2-4x slower than the Pascal/Volta ones below. You should not buy these old guards (K80). Pascal : They are in the 1080 Ti cards from 2017, which are still useful if bought used (especially for recurrent neural networks). P100 is the equivalent cloud offering. Volta / Turing : These are the preferred choices over the Kepler and Pascal because of their support for 16-bit mixed-precision via tensor cores. Hardware options are 2080 Ti and Titan RTX, while the cloud option is V100. Ampere : This architecture is available in the latest hardware (3090) and cloud (A100) offerings. They have the most tensor cores, leading to at least 30% speedup over Turing. You can check out this recent GPU benchmark from Lambda Labs and consult Tim Dettmers\u2019 advice on which GPUs to get .","title":"GPU Basics"},{"location":"spring2021/lecture-6/#cloud-options","text":"Amazon Web Services, Google Cloud Platform, and Microsoft Azure are the cloud heavyweights with largely similar functions and prices. There are also startups like Lambda Labs and Corewave that provide cloud GPUs.","title":"Cloud Options"},{"location":"spring2021/lecture-6/#on-prem-options","text":"You can either build your own or buy pre-built devices from vendors like Lambda Labs, NVIDIA, Supermicro, Cirrascale, etc.","title":"On-Prem Options"},{"location":"spring2021/lecture-6/#recommendations","text":"Even though the cloud is expensive, it\u2019s hard to make on-prem devices scale past a certain point. Furthermore, dev-ops things are easier to be done in the cloud than to be set up by yourself. And if your machine dies or requires maintenance, that will be a constant headache if you are responsible for managing it. Here are our recommendations for three profiles: Hobbyists: Build your own machine (maybe a 4x Turing or a 2x Ampere PC) during development. Either use the same PC or use cloud instances during training/evaluation. Startups: Buy a sizeable Lambda Labs machine for every ML scientist during development. Buy more shared server machines or use cloud instances during training/evaluation. Larger companies: Buy an even more powerful machine for every ML scientist during development. Use cloud with fast instances with proper provisioning and handling of failures during training/evaluation.","title":"Recommendations"},{"location":"spring2021/lecture-6/#5-resource-management","text":"With all the resources we have discussed (compute, dependencies, etc.), our challenge turns to manage them across the specific use cases we may have. Across all the resources, our goal is always to be able to easily experiment with the necessary resources to achieve the desired application of ML for our product. For this challenge of allocating resources to experimenting users, there are some common solutions: Script a solution ourselves : In theory, this is the simplest solution. We can check if a resource is free and then lock it if a particular user is using it or wants to. SLURM : If we don't want to write the script entirely ourselves, standard cluster job schedulers like SLURM can help us. The workflow is as follows: First, a script defines a job\u2019s requirements. Then, the SLURM queue runner analyzes this and then executes the jobs on the correct resource. Docker/Kubernetes : The above approach might still be too manual for your needs, in which case you can turn to Docker/Kubernetes. Docker packages the dependency stack into a lighter-than-VM package called a container (that excludes the OS). Kubernetes lets us run these Docker containers on a cluster. In particular, Kubeflow is an OSS project started by Google that allows you to spawn/manage Jupyter notebooks and manage multi-step workflows. It also has lots of plug-ins for extra processes like hyperparameter tuning and model deployment. However, Kubeflow can be a challenge to setup. Custom ML software : There\u2019s a lot of novel work and all-in-one solutions being developed to provision compute resources for ML development efficiently. Platforms like AWS Sagemaker , Paperspace Gradient , and Determined AI are advancing. Newer startups like Anyscale and Grid.AI (creators of PyTorch Lightning) are also tackling this. Their vision is around allowing you to seamlessly go from training models on your computer to running lots of training jobs in the cloud with a simple set of SDK commands.","title":"5 - Resource Management"},{"location":"spring2021/lecture-6/#6-frameworks-and-distributed-training","text":"","title":"6 - Frameworks and Distributed Training"},{"location":"spring2021/lecture-6/#deep-learning-frameworks","text":"If you\u2019ve built a deep learning model in the last few years, you\u2019ve probably used a deep learning framework. Frameworks like TensorFlow have crucially shaped the development of the deep learning revolution. The reality is that deep learning frameworks have existed for a while. Projects like Theano and Torch have been around for 10+ years. In contemporary use, there are three main frameworks we\u2019ll focus on - TensorFlow , Keras , and PyTorch . We evaluate frameworks based on their utility for production and development . When TensorFlow came out in 2015, it was billed heavily as a production-optimized DL framework with an underlying static optimized graph that could be deployed across compute environments. However, TF 1.0 had a pretty unpleasant development experience; in addition to developing your models, you had to consider the underlying execution graph you were describing. This kind of \u201cmeta-development\u201d posed a challenge for newcomers. The Keras project solved many of these issues by offering a simpler way to define models, and eventually became a part of TensorFlow. PyTorch, when it was introduced in 2017, offered a polar opposite to TensorFlow. It made development super easy by consisting almost exclusively of simple Python commands, but was not designed to be fast at scale. Using TF/Keras or PyTorch is the current recommended way to build deep learning models unless you have a powerful reason not to. Essentially, both have converged to pretty similar points that balance development and production. TensorFlow adopted eager execution by default and became a lot easier to develop quickly in. PyTorch subsumed Caffe2 and became much faster as a result, specifically by adding the ability to compile speedier model artifacts. Nowadays, PyTorch has a lot of momentum, likely due to its ease of development. Newer projects like fast.ai and PyTorch Lighting add best practices and additional functionality to PyTorch, making it even more popular. According to this 2018 article on The Gradient , more than 80% of submissions are in PyTorch in academic projects. All these frameworks may seem like excessive quibbling, especially since PyTorch and TensorFlow have converged in important ways. Why do we even require such extensive frameworks? It\u2019s theoretically possible to define entire models and their required matrix math (e.g., a CNN) in NumPy, the classic Python numerical computing library. However, we quickly run into two challenges: back-propagating errors through our model and running the code on GPUs, which are powerful computation accelerators. For these issues to be addressed, we need frameworks to help us with auto-differentiation , an efficient way of computing the gradients, and software compatibility with GPUs , specifically interfacing with CUDA. Frameworks allow us to abstract the work required to achieve both features, while also layering in valuable abstractions for all the latest layer designs, optimizers, losses, and much more. As you can imagine, the abstractions offered by frameworks save us valuable time on getting our model to run and allow us to focus on optimizing our model. New projects like JAX and HuggingFace offer different or simpler abstractions. JAX focuses primarily on fast numerical computation with autodiff and GPUs across machine learning use cases (not just deep learning). HuggingFace abstracts entire model architectures in the NLP realm. Instead of loading individual layers, HuggingFace lets you load the entirety of a contemporary mode (along with weights)l like BERT, tremendously speeding up development time. HuggingFace works on both PyTorch and TensorFlow.","title":"Deep Learning Frameworks"},{"location":"spring2021/lecture-6/#distributed-training","text":"Distributed training is a hot topic as the datasets and the models we train become too large to work on a single GPU. It\u2019s increasingly a must-do. The important thing to note is that distributed training is a process to conduct a single model training process ; don\u2019t confuse it with training multiple models on different GPUs. There are two approaches to distributed training: data parallelism and model parallelism.","title":"Distributed Training"},{"location":"spring2021/lecture-6/#data-parallelism","text":"Data parallelism is quite simple but powerful. If we have a batch size of X samples, which is too large for one GPU, we can split the X samples evenly across N GPUs. Each GPU calculates the gradients and passes them to a central node (either a GPU or a CPU), where the gradients are averaged and backpropagated through the distributed GPUs. This paradigm generally results in a linear speed-up time (e.g., two distributed GPUs results in a ~2X speed-up in training time). In modern frameworks like PyTorch, PyTorch Lightning, and even in schedulers like SLURM, data-parallel training can be achieved simply by specifying the number of GPUs or calling a data parallelism-enabling object (e.g., torch.nn.DataParallel ). Other tools like Horovod (from Uber) use non-framework-specific ways of enabling data parallelism (e.g., MPI, a standard multiprocessing framework). Ray , the original open-source project from the Anyscale team, was designed to enable general distributed computing applications in Python and can be similarly applied to data-parallel distributed training.","title":"Data Parallelism"},{"location":"spring2021/lecture-6/#model-parallelism","text":"Model parallelism is a lot more complicated. If you can\u2019t fit your entire model\u2019s weights on a single GPU, you can split the weights across GPUs and pass data through each to train the weights. This usually adds a lot of complexity and should be avoided unless absolutely necessary. A better solution is to pony up for the best GPU available, either locally or in the cloud. You can also use gradient checkpointing, a clever trick wherein you write some gradients to disk as you compute them and load them only as you need them for updates. New work is coming out to make this easier (e.g., research and framework maturity).","title":"Model Parallelism"},{"location":"spring2021/lecture-6/#7-experiment-management","text":"As you run numerous experiments to refine your model, it\u2019s easy to lose track of code, hyperparameters, and artifacts. Model iteration can lead to lots of complexity and messiness. For example, you could be monitoring the learning rate\u2019s impact on your model\u2019s performance metric. With multiple model runs, how will you monitor the impact of the hyperparameter? A low-tech way would be to manually track the results of all model runs in a spreadsheet. Without great attention to detail, this can quickly spiral into a messy or incomplete artifact. Dedicated experiment management platforms are a remedy to this issue. Let\u2019s cover a few of the most common ones: TensorBoard : This is the default experiment tracking platform that comes with TensorFlow. As a pro, it\u2019s easy to get started with. On the flip side, it\u2019s not very good for tracking and comparing multiple experiments. It\u2019s also not the best solution to store past work. MLFlow : An OSS project from Databricks, MLFlow is a complete platform for the ML lifecycle. They have great experiment and model run management at the core of their platform. Another open-source project, Keepsake , recently came out focused solely on experiment tracking. Paid platforms ( Comet.ml , Weights and Biases , Neptune ): Finally, outside vendors offer deep, thorough experiment management platforms, with tools like code diffs, report writing, data visualization, and model registering features. In our labs, we will use Weights and Biases.","title":"7 - Experiment Management"},{"location":"spring2021/lecture-6/#8-hyperparameter-tuning","text":"To finalize models, we need to ensure that we have the optimal hyperparameters. Since hyperparameter optimization (as this process is called) can be a particularly compute-intensive process, it\u2019s useful to have software that can help. Using specific software can help us kill underperforming model runs with bad hyperparameters early (to save on cost) or help us intelligently sweep ranges of hyperparameter values. Luckily, there\u2019s an increasing number of software providers that do precisely this: SigOpt offers an API focused exclusively on efficient, iterative hyperparameter optimization. Specify a range of values, get SigOpt\u2019s recommended hyperparameter settings, run the model and return the results to SigOpt, and repeat the process until you\u2019ve found the best parameters for your model. Rather than an API, Ray Tune offers a local software (part of the broader Ray ecosystem) that integrates hyperparameter optimization with compute resource allocation. Jobs are scheduled with specific hyperparameters according to state-of-the-art methods, and underperforming jobs are automatically killed. Weights and Biases also has this feature! With a YAML file specification, we can specify a hyperparameter optimization job and perform a \u201c sweep ,\u201d during which W&B sends parameter settings to individual \u201cagents\u201d (our machines) and compares performance.","title":"8 - Hyperparameter Tuning"},{"location":"spring2021/lecture-6/#9-all-in-one-solutions","text":"Some platforms integrate all the aspects of the applied ML stack we\u2019ve discussed (experiment tracking, optimization, training, etc.) and wrap them into a single experience. To support the \u201clifecycle,\u201d these platforms typically include: Labeling and data querying services Model training, especially though job scaling and scheduling Experiment tracking and model versioning Development environments, typically through notebook-style interfaces Model deployment (e.g., via REST APIs) and monitoring One of the earliest examples of such a system is Facebook\u2019s FBLearner (2016), which encompassed data and feature storage, training, inference, and continuous learning based on user interactions with the model\u2019s outputs. You can imagine how powerful having one hub for all this activity can be for ML application and development speed. As a result, cloud vendors (Google, AWS, Azure) have developed similar all-in-one platforms, like Google Cloud AI Platform and AWS SageMaker . Startups like Paperspace Gradient , Neptune , and FloydHub also offer all-in-one platforms focused on deep learning. Determined AI , which focuses exclusively on the model development and training part of the lifecycle, is the rare open-source platform in this space. Domino Data Lab is a traditional ML-focused startup with an extensive feature set worth looking at. It\u2019s natural to expect more MLOps (as this kind of tooling and infra is referred to) companies and vendors to build out their feature set and become platform-oriented; Weights and Biases is a good example of this. In conclusion, take a look at the below table to compare a select number of MLOps platform vendors. Pricing is quite variable. Staying up to date across all the tooling can be a real challenge, but check out FSDL\u2019s Tooling Tuesdays on Twitter as a starting point!","title":"9 - \u201cAll-In-One\u201d Solutions"},{"location":"spring2021/lecture-7/","text":"Lecture 7: Troubleshooting Deep Neural Networks Video Slides PDF Download Detailed Notes Notes were taken by James Le and Vishnu Rachakonda In traditional software engineering, a bug usually leads to the program crashing. While this is annoying for the user, it is critical for the developer to inspect the errors to understand why. With deep learning, we sometimes encounter errors, but all too often, the program crashes without a clear reason why. While these issues can be debugged manually, deep learning models most often fail because of poor output predictions. What\u2019s worse is that when the model performance is low, there is usually no signal about why or when the models failed. A common sentiment among practitioners is that they spend 80\u201390% of time debugging and tuning the models and only 10\u201320% of time deriving math equations and implementing things. This is confirmed by Andrej Kaparthy, as seen in this tweet . 1 - Why Is Deep Learning Troubleshooting Hard? Suppose you are trying to reproduce a research paper result for your work, but your results are worse. You might wonder why your model\u2019s performance is significantly worse than the paper that you\u2019re trying to reproduce? Many different things can cause this: It can be implementation bugs . Most bugs in deep learning are actually invisible. Hyper-parameter choices can also cause your performance to degrade. Deep learning models are very sensitive to hyper-parameters. Even very subtle choices of learning rate and weight initialization can make a big difference. Performance can also be worse just because of data/model fit . For example, you pre-train your model on ImageNet data and fit it on self-driving car images, which are harder to learn. Finally, poor model performance could be caused not by your model but your dataset construction . Typical issues here include not having enough examples, dealing with noisy labels and imbalanced classes, splitting train and test set with different distributions. 2 - Strategy to Debug Neural Networks The key idea of deep learning troubleshooting is: Since it is hard to disambiguate errors, it\u2019s best to start simple and gradually ramp up complexity. This lecture provides a decision tree for debugging deep learning models and improving performance . This guide assumes that you already have an initial test dataset, a single metric to improve, and target performance based on human-level performance, published results, previous baselines, etc. 3 - Start Simple The first step is the troubleshooting workflow is starting simple . Choose A Simple Architecture There are a few things to consider when you want to start simple. The first is how to choose a simple architecture . These are architectures that are easy to implement and are likely to get you part of the way towards solving your problem without introducing as many bugs. Architecture selection is one of the many intimidating parts of getting into deep learning because there are tons of papers coming out all-the-time and claiming to be state-of-the-art on some problems. They get very complicated fast. In the limit, if you\u2019re trying to get to maximal performance, then architecture selection is challenging. But when starting on a new problem, you can just solve a simple set of rules that will allow you to pick an architecture that enables you to do a decent job on the problem you\u2019re working on. If your data looks like images , start with a LeNet-like architecture and consider using something like ResNet as your codebase gets more mature. If your data looks like sequences , start with an LSTM with one hidden layer and/or temporal/classical convolutions. Then, when your problem gets more mature, you can move to an Attention-based model or a WaveNet-like model. For all other tasks , start with a fully-connected neural network with one hidden layer and use more advanced networks later, depending on the problem. In reality, many times, the input data contains multiple of those things above. So how to deal with multiple input modalities into a neural network? Here is the 3-step strategy that we recommend: First, map each of these modalities into a lower-dimensional feature space. In the example above, the images are passed through a ConvNet, and the words are passed through an LSTM. Then we flatten the outputs of those networks to get a single vector for each of the inputs that will go into the model. Then we concatenate those inputs. Finally, we pass them through some fully-connected layers to an output. Use Sensible Defaults After choosing a simple architecture, the next thing to do is to select sensible hyper-parameter defaults to start with. Here are the defaults that we recommend: Adam optimizer with a \u201cmagic\u201d learning rate value of 3e-4 . ReLU activation for fully-connected and convolutional models and Tanh activation for LSTM models. He initialization for ReLU activation function and Glorot initialization for Tanh activation function . No regularization and data normalization. Normalize Inputs The next step is to normalize the input data , subtracting the mean and dividing by the variance. Note that for images, it\u2019s fine to scale values to [0, 1] or [-0.5, 0.5] (for example, by dividing by 255). Simplify The Problem The final thing you should do is consider simplifying the problem itself. If you have a complicated problem with massive data and tons of classes to deal with, then you should consider: Working with a small training set around 10,000 examples. Using a fixed number of objects, classes, input size, etc. Creating a simpler synthetic training set like in research labs. This is important because (1) you will have reasonable confidence that your model should be able to solve, and (2) your iteration speed will increase. The diagram below neatly summarizes how to start simple: 4 - Implement and Debug To give you a preview, below are the five most common bugs in deep learning models that we recognize: Incorrect shapes for the network tensors : This bug is a common one and can fail silently. This happens many times because the automatic differentiation systems in the deep learning framework do silent broadcasting. Tensors become different shapes in the network and can cause a lot of problems. Pre-processing inputs incorrectly : For example, you forget to normalize your inputs or apply too much input pre-processing (over-normalization and excessive data augmentation). Incorrect input to the model\u2019s loss function : For example, you use softmax outputs to a loss that expects logits. Forgot to set up train mode for the network correctly : For example, toggling train/evaluation mode or controlling batch norm dependencies. Numerical instability : For example, you get `inf` or `NaN` as outputs. This bug often stems from using an exponent, a log, or a division operation somewhere in the code. Here are three pieces of general advice for implementing your model: Start with a lightweight implementation . You want minimum possible new lines of code for the 1st version of your model. The rule of thumb is less than 200 lines. This doesn\u2019t count tested infrastructure components or TensorFlow/PyTorch code. Use off-the-shelf components such as Keras if possible, since most of the stuff in Keras works well out-of-the-box. If you have to use TensorFlow, use the built-in functions, don\u2019t do the math yourself. This would help you avoid a lot of numerical instability issues. Build complicated data pipelines later . These are important for large-scale ML systems, but you should not start with them because data pipelines themselves can be a big source of bugs. Just start with a dataset that you can load into memory. Get Your Model To Run The first step of implementing bug-free deep learning models is getting your model to run at all . There are a few things that can prevent this from happening: Shape mismatch/casting issue : To address this type of problem, you should step through your model creation and inference step-by-step in a debugger, checking for correct shapes and data types of your tensors. Out-of-memory issues : This can be very difficult to debug. You can scale back your memory-intensive operations one-by-one. For example, if you create large matrices anywhere in your code, you can reduce the size of their dimensions or cut your batch size in half. Other issues : You can simply Google it. Stack Overflow would be great most of the time. Let\u2019s zoom in on the process of stepping through model creation in a debugger and talk about debuggers for deep learning code : In PyTorch, you can use ipdb \u200a\u2014\u200awhich exports functions to access the interactive IPython debugger. In TensorFlow, it\u2019s trickier. TensorFlow separates the process of creating the graph and executing operations in the graph. There are three options you can try: (1) step through the graph creation itself and inspect each tensor layer, (2) step into the training loop and evaluate the tensor layers, or (3) use TensorFlow Debugger (tfdb), which does option 1 and 2 automatically. Overfit A Single Batch After getting your model to run, the next thing you need to do is to overfit a single batch of data . This is a heuristic that can catch an absurd number of bugs. This really means that you want to drive your training error arbitrarily close to 0. There are a few things that can happen when you try to overfit a single batch and it fails: Error goes up : Commonly, this is due to a flip sign somewhere in the loss function/gradient. Error explodes : This is usually a numerical issue but can also be caused by a high learning rate. Error oscillates : You can lower the learning rate and inspect the data for shuffled labels or incorrect data augmentation. Error plateaus : You can increase the learning rate and get rid of regulation. Then you can inspect the loss function and the data pipeline for correctness. Compare To A Known Result Once your model overfits in a single batch, there can still be some other issues that cause bugs. The last step here is to compare your results to a known result . So what sort of known results are useful? The most useful results come from an official model implementation evaluated on a similar dataset to yours . You can step through the code in both models line-by-line and ensure your model has the same output. You want to ensure that your model performance is up to par with expectations. If you can\u2019t find an official implementation on a similar dataset, you can compare your approach to results from an official model implementation evaluated on a benchmark dataset . You most definitely want to walk through the code line-by-line and ensure you have the same output. If there is no official implementation of your approach, you can compare it to results from an unofficial model implementation . You can review the code the same as before but with lower confidence (because almost all the unofficial implementations on GitHub have bugs). Then, you can compare to results from a paper with no code (to ensure that your performance is up to par with expectations), results from your model on a benchmark dataset (to make sure your model performs well in a simpler setting), and results from a similar model on a similar dataset (to help you get a general sense of what kind of performance can be expected). An under-rated source of results comes from simple baselines (for example, the average of outputs or linear regression), which can help make sure that your model is learning anything at all. The diagram below neatly summarizes how to implement and debug deep neural networks: 5 - Evaluate Bias-Variance Decomposition To evaluate models and prioritize the next steps in model development, we will apply the bias-variance decomposition. The bias-variance decomposition is the fundamental model fitting tradeoff. In our application, let\u2019s talk more specifically about the formula for bias-variance tradeoff with respect to the test error; this will help us apply the concept more directly to our model\u2019s performance. There are four terms in the formula for test error: Test error = irreducible error + bias + variance + validation overfitting Irreducible error is the baseline error you don\u2019t expect your model to do better. It can be estimated through strong baselines, like human performance. Avoidable bias , a measure of underfitting, is the difference between our train error and irreducible error. Variance , a measure of overfitting, is the difference between validation error and training error. Validation set overfitting is the difference between test error and validation error. Consider the chart of learning curves and errors below. Using the test error formula for bias and variance, we can calculate each component of test error and make decisions based on the value. For example, our avoidable bias is rather low (only 2 points), while the variance is much higher (5 points). With this knowledge, we should prioritize methods of preventing overfitting, like regularization. Distribution Shift Clearly, the application of the bias-variance decomposition to the test error has already helped prioritize our next steps for model development. However, until now, we\u2019ve assumed that the samples (training, validation, testing) all come from the same distribution. What if this isn\u2019t the case? In practical ML situations, this distribution shift often cars. In building self-driving cars, a frequent occurrence might be training with samples from one distribution (e.g., daytime driving video) but testing or inferring on samples from a totally different distribution (e.g., night time driving). A simple way of handling this wrinkle in our assumption is to create two validation sets: one from the training distribution and one from the test distribution. This can be helpful even with a very small testing set. If we apply this, we can actually estimate our distribution shift, which is the difference between testing validation error and testing error. This is really useful for practical applications of ML! With this new term, let\u2019s update our test error formula of bias and variance: Test error = irreducible error + bias + variance + distribution shift + validation overfitting 6 - Improve Model and Data Using the updated formula from the last section, we\u2019ll be able to decide on and prioritize the right next steps for each iteration of a model. In particular, we\u2019ll follow a specific process (shown below). Step 1: Address Underfitting We\u2019ll start by addressing underfitting (i.e., reducing bias). The first thing to try in this case is to make your model bigger (e.g., add layers, more units per layer). Next, consider regularization, which can prevent a tight fit to your data. Other options are error analysis, choosing a different model architecture (e.g., something more state of the art), tuning hyperparameters, or adding features. Some notes: Choosing different architectures, especially a SOTA one, can be very helpful but is also risky. Bugs are easily introduced in the implementation process. Adding features is uncommon in the deep learning paradigm (vs. traditional machine learning). We usually want the network to learn features of its own accord. If all else fails, it can be beneficial in a practical setting. Step 2: Address Overfitting After addressing underfitting, move on to solving overfitting. Similarly, there\u2019s a recommended series of methods to try in order. Starting with collecting training data (if possible) is the soundest way to address overfitting, though it can be challenging in certain applications. Next, tactical improvements like normalization, data augmentation, and regularization can help. Following these steps, traditional defaults like tuning hyperparameters, choosing a different architecture, or error analysis are useful. Finally, if overfitting is rather intractable, there\u2019s a series of less recommended steps, such as early stopping, removing features, and reducing model size. Early stopping is a personal choice; the fast.ai community is a strong proponent. Step 3: Address Distribution Shift After addressing underfitting and overfitting, If there\u2019s a difference between the error on our training validation set vs. our test validation set, we need to address the error caused by the distribution shift. This is a harder problem to solve, so there\u2019s less in our toolkit to apply. Start by looking manually at the errors in the test-validation set. Compare the potential logic behind these errors to the performance in the train-validation set, and use the errors to guide further data collection. Essentially, reason about why your model may be suffering from distribution shift error. This is the most principled way to deal with distribution shift, though it\u2019s the most challenging way practically. If collecting more data to address these errors isn\u2019t possible, try synthesizing data. Additionally, you can try domain adaptation . Error Analysis Manually evaluating errors to understand model performance is generally a high-yield way of figuring out how to improve the model. Systematically performing this error analysis process and decomposing the error from different error types can help prioritize model improvements. For example, in a self-driving car use case with error types like hard-to-see pedestrians, reflections, and nighttime scenes, decomposing the error contribution of each and where it occurs (train-val vs. test-val) can give rise to a clear set of prioritized action items. See the table for an example of how this error analysis can be effectively structured. Domain Adaptation Domain adaptation is a class of techniques that train on a \u201csource\u201d distribution and generalize to another \u201ctarget\u201d using only unlabeled data or limited labeled data. You should use domain adaptation when access to labeled data from the test distribution is limited, but access to relatively similar data is plentiful. There are a few different types of domain adaptation: Supervised domain adaptation : In this case, we have limited data from the target domain to adapt to. Some example applications of the concept include fine-tuning a pre-trained model or adding target data to a training set. Unsupervised domain adaptation : In this case, we have lots of unlabeled data from the target domain. Some techniques you might see are CORAL, domain confusion, and CycleGAN. Practically speaking, supervised domain adaptation can work really well! Unsupervised domain adaptation has a little bit further to go. Step 4: Rebalance datasets If the test-validation set performance starts to look considerably better than the test performance, you may have overfit the validation set. This commonly occurs with small validation sets or lots of hyperparameter training. If this occurs, resample the validation set from the test distribution and get a fresh estimate of the performance. 7 - Tune Hyperparameters One of the core challenges in hyperparameter optimization is very basic: which hyperparameters should you tune? As we consider this fundamental question, let\u2019s keep the following in mind: Models are more sensitive to some hyperparameters than others. This means we should focus our efforts on the more impactful hyperparameters. However, which hyperparameters are most important depends heavily on our choice of model. Certain rules of thumbs can help guide our initial thinking. Sensitivity is always relative to default values; if you use good defaults, you might start in a good place! See the following table for a ranked list of hyperparameters and their impact on the model: Techniques for Tuning Hyperparameter Optimization Now that we know which hyperparameters make the most sense to tune (using rules of thumb), let\u2019s consider the various methods of actually tuning them: Manual Hyperparameter Optimization . Colloquially referred to as Graduate Student Descent, this method works by taking a manual, detailed look at your algorithm, building intuition, and considering which hyperparameters would make the most difference. After figuring out these parameters, you train, evaluate, and guess a better hyperparameter value using your intuition for the algorithm and intelligence. While it may seem archaic, this method combines well with other methods (e.g., setting a range of values for hyperparameters) and has the main benefit of reducing computation time and cost if used skillfully. It can be time-consuming and challenging, but it can be a good starting point. Grid Search . Imagine each of your parameters plotted against each other on a grid, from which you uniformly sample values to test. For each point, you run a training run and evaluate performance. The advantages are that it\u2019s very simple and can often produce good results. However, it\u2019s quite inefficient, as you must run every combination of hyperparameters. It also often requires prior knowledge about the hyperparameters since we must manually set the range of values. Random Search : This method is recommended over grid search. Rather than sampling from the grid of values for the hyperparameter evenly, we\u2019ll choose n points sampled randomly across the grid. Empirically, this method produces better results than grid search. However, the results can be somewhat uninterpretable, with unexpected values in certain hyperparameters returned. Coarse-to-fine Search : Rather than running entirely random runs, we can gradually narrow in on the best hyperparameters through this method. Initially, start by defining a very large range to run a randomized search on. Within the pool of results, you can find N best results and hone in on the hyperparameter values used to generate those samples. As you iteratively perform this method, you can get excellent performance. This doesn\u2019t remove the manual component, as you have to select which range to continuously narrow your search to, but it\u2019s perhaps the most popular method available. Bayesian Hyperparameter Optimization : This is a reasonably sophisticated method, which you can read more about here and here . At a high level, start with a prior estimate of parameter distributions. Subsequently, maintain a probabilistic model of the relationship between hyperparameter values and model performance. As you maintain this model, you toggle between training with hyperparameter values that maximize the expected improvement (per the model) and use training results to update the initial probabilistic model and its expectations. This is a great, hands-off, efficient method to choose hyperparameters. However, these techniques can be quite challenging to implement from scratch. As libraries and infrastructure mature, the integration of these methods into training will become easier. In summary, you should probably start with coarse-to-fine random searches and move to Bayesian methods as your codebase matures and you\u2019re more certain of your model. 8 - Conclusion To wrap up this lecture, deep learning troubleshooting and debugging is really hard. It\u2019s difficult to tell if you have a bug because there are many possible sources for the same degradation in performance. Furthermore, the results can be sensitive to small changes in hyper-parameters and dataset makeup. To train bug-free deep learning models, we need to treat building them as an iterative process. If you skipped to the end, the following steps can make this process easier and catch errors as early as possible: Start Simple : Choose the simplest model and data possible. Implement and Debug : Once the model runs, overfit a single batch and reproduce a known result. Evaluate : Apply the bias-variance decomposition to decide what to do next. Tune Hyper-parameters : Use coarse-to-fine random searches to tune the model\u2019s hyper-parameters. Improve Model and Data : Make your model bigger if your model under-fits and add more data and/or regularization if your model over-fits. Here are additional resources that you can go to learn more: Andrew Ng\u2019s \u201c Machine Learning Yearning \u201d book. This Twitter thread from Andrej Karpathy. BYU\u2019s \u201c Practical Advice for Building Deep Neural Networks \u201d blog post.","title":"Lecture 7: Troubleshooting Deep Neural Networks"},{"location":"spring2021/lecture-7/#lecture-7-troubleshooting-deep-neural-networks","text":"","title":"Lecture 7: Troubleshooting Deep Neural Networks"},{"location":"spring2021/lecture-7/#video","text":"","title":"Video"},{"location":"spring2021/lecture-7/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lecture-7/#detailed-notes","text":"Notes were taken by James Le and Vishnu Rachakonda In traditional software engineering, a bug usually leads to the program crashing. While this is annoying for the user, it is critical for the developer to inspect the errors to understand why. With deep learning, we sometimes encounter errors, but all too often, the program crashes without a clear reason why. While these issues can be debugged manually, deep learning models most often fail because of poor output predictions. What\u2019s worse is that when the model performance is low, there is usually no signal about why or when the models failed. A common sentiment among practitioners is that they spend 80\u201390% of time debugging and tuning the models and only 10\u201320% of time deriving math equations and implementing things. This is confirmed by Andrej Kaparthy, as seen in this tweet .","title":"Detailed Notes"},{"location":"spring2021/lecture-7/#1-why-is-deep-learning-troubleshooting-hard","text":"Suppose you are trying to reproduce a research paper result for your work, but your results are worse. You might wonder why your model\u2019s performance is significantly worse than the paper that you\u2019re trying to reproduce? Many different things can cause this: It can be implementation bugs . Most bugs in deep learning are actually invisible. Hyper-parameter choices can also cause your performance to degrade. Deep learning models are very sensitive to hyper-parameters. Even very subtle choices of learning rate and weight initialization can make a big difference. Performance can also be worse just because of data/model fit . For example, you pre-train your model on ImageNet data and fit it on self-driving car images, which are harder to learn. Finally, poor model performance could be caused not by your model but your dataset construction . Typical issues here include not having enough examples, dealing with noisy labels and imbalanced classes, splitting train and test set with different distributions.","title":"1 - Why Is Deep Learning Troubleshooting Hard?"},{"location":"spring2021/lecture-7/#2-strategy-to-debug-neural-networks","text":"The key idea of deep learning troubleshooting is: Since it is hard to disambiguate errors, it\u2019s best to start simple and gradually ramp up complexity. This lecture provides a decision tree for debugging deep learning models and improving performance . This guide assumes that you already have an initial test dataset, a single metric to improve, and target performance based on human-level performance, published results, previous baselines, etc.","title":"2 - Strategy to Debug Neural Networks"},{"location":"spring2021/lecture-7/#3-start-simple","text":"The first step is the troubleshooting workflow is starting simple .","title":"3 - Start Simple"},{"location":"spring2021/lecture-7/#choose-a-simple-architecture","text":"There are a few things to consider when you want to start simple. The first is how to choose a simple architecture . These are architectures that are easy to implement and are likely to get you part of the way towards solving your problem without introducing as many bugs. Architecture selection is one of the many intimidating parts of getting into deep learning because there are tons of papers coming out all-the-time and claiming to be state-of-the-art on some problems. They get very complicated fast. In the limit, if you\u2019re trying to get to maximal performance, then architecture selection is challenging. But when starting on a new problem, you can just solve a simple set of rules that will allow you to pick an architecture that enables you to do a decent job on the problem you\u2019re working on. If your data looks like images , start with a LeNet-like architecture and consider using something like ResNet as your codebase gets more mature. If your data looks like sequences , start with an LSTM with one hidden layer and/or temporal/classical convolutions. Then, when your problem gets more mature, you can move to an Attention-based model or a WaveNet-like model. For all other tasks , start with a fully-connected neural network with one hidden layer and use more advanced networks later, depending on the problem. In reality, many times, the input data contains multiple of those things above. So how to deal with multiple input modalities into a neural network? Here is the 3-step strategy that we recommend: First, map each of these modalities into a lower-dimensional feature space. In the example above, the images are passed through a ConvNet, and the words are passed through an LSTM. Then we flatten the outputs of those networks to get a single vector for each of the inputs that will go into the model. Then we concatenate those inputs. Finally, we pass them through some fully-connected layers to an output.","title":"Choose A Simple Architecture"},{"location":"spring2021/lecture-7/#use-sensible-defaults","text":"After choosing a simple architecture, the next thing to do is to select sensible hyper-parameter defaults to start with. Here are the defaults that we recommend: Adam optimizer with a \u201cmagic\u201d learning rate value of 3e-4 . ReLU activation for fully-connected and convolutional models and Tanh activation for LSTM models. He initialization for ReLU activation function and Glorot initialization for Tanh activation function . No regularization and data normalization.","title":"Use Sensible Defaults"},{"location":"spring2021/lecture-7/#normalize-inputs","text":"The next step is to normalize the input data , subtracting the mean and dividing by the variance. Note that for images, it\u2019s fine to scale values to [0, 1] or [-0.5, 0.5] (for example, by dividing by 255).","title":"Normalize Inputs"},{"location":"spring2021/lecture-7/#simplify-the-problem","text":"The final thing you should do is consider simplifying the problem itself. If you have a complicated problem with massive data and tons of classes to deal with, then you should consider: Working with a small training set around 10,000 examples. Using a fixed number of objects, classes, input size, etc. Creating a simpler synthetic training set like in research labs. This is important because (1) you will have reasonable confidence that your model should be able to solve, and (2) your iteration speed will increase. The diagram below neatly summarizes how to start simple:","title":"Simplify The Problem"},{"location":"spring2021/lecture-7/#4-implement-and-debug","text":"To give you a preview, below are the five most common bugs in deep learning models that we recognize: Incorrect shapes for the network tensors : This bug is a common one and can fail silently. This happens many times because the automatic differentiation systems in the deep learning framework do silent broadcasting. Tensors become different shapes in the network and can cause a lot of problems. Pre-processing inputs incorrectly : For example, you forget to normalize your inputs or apply too much input pre-processing (over-normalization and excessive data augmentation). Incorrect input to the model\u2019s loss function : For example, you use softmax outputs to a loss that expects logits. Forgot to set up train mode for the network correctly : For example, toggling train/evaluation mode or controlling batch norm dependencies. Numerical instability : For example, you get `inf` or `NaN` as outputs. This bug often stems from using an exponent, a log, or a division operation somewhere in the code. Here are three pieces of general advice for implementing your model: Start with a lightweight implementation . You want minimum possible new lines of code for the 1st version of your model. The rule of thumb is less than 200 lines. This doesn\u2019t count tested infrastructure components or TensorFlow/PyTorch code. Use off-the-shelf components such as Keras if possible, since most of the stuff in Keras works well out-of-the-box. If you have to use TensorFlow, use the built-in functions, don\u2019t do the math yourself. This would help you avoid a lot of numerical instability issues. Build complicated data pipelines later . These are important for large-scale ML systems, but you should not start with them because data pipelines themselves can be a big source of bugs. Just start with a dataset that you can load into memory.","title":"4 - Implement and Debug"},{"location":"spring2021/lecture-7/#get-your-model-to-run","text":"The first step of implementing bug-free deep learning models is getting your model to run at all . There are a few things that can prevent this from happening: Shape mismatch/casting issue : To address this type of problem, you should step through your model creation and inference step-by-step in a debugger, checking for correct shapes and data types of your tensors. Out-of-memory issues : This can be very difficult to debug. You can scale back your memory-intensive operations one-by-one. For example, if you create large matrices anywhere in your code, you can reduce the size of their dimensions or cut your batch size in half. Other issues : You can simply Google it. Stack Overflow would be great most of the time. Let\u2019s zoom in on the process of stepping through model creation in a debugger and talk about debuggers for deep learning code : In PyTorch, you can use ipdb \u200a\u2014\u200awhich exports functions to access the interactive IPython debugger. In TensorFlow, it\u2019s trickier. TensorFlow separates the process of creating the graph and executing operations in the graph. There are three options you can try: (1) step through the graph creation itself and inspect each tensor layer, (2) step into the training loop and evaluate the tensor layers, or (3) use TensorFlow Debugger (tfdb), which does option 1 and 2 automatically.","title":"Get Your Model To Run"},{"location":"spring2021/lecture-7/#overfit-a-single-batch","text":"After getting your model to run, the next thing you need to do is to overfit a single batch of data . This is a heuristic that can catch an absurd number of bugs. This really means that you want to drive your training error arbitrarily close to 0. There are a few things that can happen when you try to overfit a single batch and it fails: Error goes up : Commonly, this is due to a flip sign somewhere in the loss function/gradient. Error explodes : This is usually a numerical issue but can also be caused by a high learning rate. Error oscillates : You can lower the learning rate and inspect the data for shuffled labels or incorrect data augmentation. Error plateaus : You can increase the learning rate and get rid of regulation. Then you can inspect the loss function and the data pipeline for correctness.","title":"Overfit A Single Batch"},{"location":"spring2021/lecture-7/#compare-to-a-known-result","text":"Once your model overfits in a single batch, there can still be some other issues that cause bugs. The last step here is to compare your results to a known result . So what sort of known results are useful? The most useful results come from an official model implementation evaluated on a similar dataset to yours . You can step through the code in both models line-by-line and ensure your model has the same output. You want to ensure that your model performance is up to par with expectations. If you can\u2019t find an official implementation on a similar dataset, you can compare your approach to results from an official model implementation evaluated on a benchmark dataset . You most definitely want to walk through the code line-by-line and ensure you have the same output. If there is no official implementation of your approach, you can compare it to results from an unofficial model implementation . You can review the code the same as before but with lower confidence (because almost all the unofficial implementations on GitHub have bugs). Then, you can compare to results from a paper with no code (to ensure that your performance is up to par with expectations), results from your model on a benchmark dataset (to make sure your model performs well in a simpler setting), and results from a similar model on a similar dataset (to help you get a general sense of what kind of performance can be expected). An under-rated source of results comes from simple baselines (for example, the average of outputs or linear regression), which can help make sure that your model is learning anything at all. The diagram below neatly summarizes how to implement and debug deep neural networks:","title":"Compare To A Known Result"},{"location":"spring2021/lecture-7/#5-evaluate","text":"","title":"5 - Evaluate"},{"location":"spring2021/lecture-7/#bias-variance-decomposition","text":"To evaluate models and prioritize the next steps in model development, we will apply the bias-variance decomposition. The bias-variance decomposition is the fundamental model fitting tradeoff. In our application, let\u2019s talk more specifically about the formula for bias-variance tradeoff with respect to the test error; this will help us apply the concept more directly to our model\u2019s performance. There are four terms in the formula for test error: Test error = irreducible error + bias + variance + validation overfitting Irreducible error is the baseline error you don\u2019t expect your model to do better. It can be estimated through strong baselines, like human performance. Avoidable bias , a measure of underfitting, is the difference between our train error and irreducible error. Variance , a measure of overfitting, is the difference between validation error and training error. Validation set overfitting is the difference between test error and validation error. Consider the chart of learning curves and errors below. Using the test error formula for bias and variance, we can calculate each component of test error and make decisions based on the value. For example, our avoidable bias is rather low (only 2 points), while the variance is much higher (5 points). With this knowledge, we should prioritize methods of preventing overfitting, like regularization.","title":"Bias-Variance Decomposition"},{"location":"spring2021/lecture-7/#distribution-shift","text":"Clearly, the application of the bias-variance decomposition to the test error has already helped prioritize our next steps for model development. However, until now, we\u2019ve assumed that the samples (training, validation, testing) all come from the same distribution. What if this isn\u2019t the case? In practical ML situations, this distribution shift often cars. In building self-driving cars, a frequent occurrence might be training with samples from one distribution (e.g., daytime driving video) but testing or inferring on samples from a totally different distribution (e.g., night time driving). A simple way of handling this wrinkle in our assumption is to create two validation sets: one from the training distribution and one from the test distribution. This can be helpful even with a very small testing set. If we apply this, we can actually estimate our distribution shift, which is the difference between testing validation error and testing error. This is really useful for practical applications of ML! With this new term, let\u2019s update our test error formula of bias and variance: Test error = irreducible error + bias + variance + distribution shift + validation overfitting","title":"Distribution Shift"},{"location":"spring2021/lecture-7/#6-improve-model-and-data","text":"Using the updated formula from the last section, we\u2019ll be able to decide on and prioritize the right next steps for each iteration of a model. In particular, we\u2019ll follow a specific process (shown below).","title":"6 - Improve Model and Data"},{"location":"spring2021/lecture-7/#step-1-address-underfitting","text":"We\u2019ll start by addressing underfitting (i.e., reducing bias). The first thing to try in this case is to make your model bigger (e.g., add layers, more units per layer). Next, consider regularization, which can prevent a tight fit to your data. Other options are error analysis, choosing a different model architecture (e.g., something more state of the art), tuning hyperparameters, or adding features. Some notes: Choosing different architectures, especially a SOTA one, can be very helpful but is also risky. Bugs are easily introduced in the implementation process. Adding features is uncommon in the deep learning paradigm (vs. traditional machine learning). We usually want the network to learn features of its own accord. If all else fails, it can be beneficial in a practical setting.","title":"Step 1: Address Underfitting"},{"location":"spring2021/lecture-7/#step-2-address-overfitting","text":"After addressing underfitting, move on to solving overfitting. Similarly, there\u2019s a recommended series of methods to try in order. Starting with collecting training data (if possible) is the soundest way to address overfitting, though it can be challenging in certain applications. Next, tactical improvements like normalization, data augmentation, and regularization can help. Following these steps, traditional defaults like tuning hyperparameters, choosing a different architecture, or error analysis are useful. Finally, if overfitting is rather intractable, there\u2019s a series of less recommended steps, such as early stopping, removing features, and reducing model size. Early stopping is a personal choice; the fast.ai community is a strong proponent.","title":"Step 2: Address Overfitting"},{"location":"spring2021/lecture-7/#step-3-address-distribution-shift","text":"After addressing underfitting and overfitting, If there\u2019s a difference between the error on our training validation set vs. our test validation set, we need to address the error caused by the distribution shift. This is a harder problem to solve, so there\u2019s less in our toolkit to apply. Start by looking manually at the errors in the test-validation set. Compare the potential logic behind these errors to the performance in the train-validation set, and use the errors to guide further data collection. Essentially, reason about why your model may be suffering from distribution shift error. This is the most principled way to deal with distribution shift, though it\u2019s the most challenging way practically. If collecting more data to address these errors isn\u2019t possible, try synthesizing data. Additionally, you can try domain adaptation .","title":"Step 3: Address Distribution Shift"},{"location":"spring2021/lecture-7/#error-analysis","text":"Manually evaluating errors to understand model performance is generally a high-yield way of figuring out how to improve the model. Systematically performing this error analysis process and decomposing the error from different error types can help prioritize model improvements. For example, in a self-driving car use case with error types like hard-to-see pedestrians, reflections, and nighttime scenes, decomposing the error contribution of each and where it occurs (train-val vs. test-val) can give rise to a clear set of prioritized action items. See the table for an example of how this error analysis can be effectively structured.","title":"Error Analysis"},{"location":"spring2021/lecture-7/#domain-adaptation","text":"Domain adaptation is a class of techniques that train on a \u201csource\u201d distribution and generalize to another \u201ctarget\u201d using only unlabeled data or limited labeled data. You should use domain adaptation when access to labeled data from the test distribution is limited, but access to relatively similar data is plentiful. There are a few different types of domain adaptation: Supervised domain adaptation : In this case, we have limited data from the target domain to adapt to. Some example applications of the concept include fine-tuning a pre-trained model or adding target data to a training set. Unsupervised domain adaptation : In this case, we have lots of unlabeled data from the target domain. Some techniques you might see are CORAL, domain confusion, and CycleGAN. Practically speaking, supervised domain adaptation can work really well! Unsupervised domain adaptation has a little bit further to go.","title":"Domain Adaptation"},{"location":"spring2021/lecture-7/#step-4-rebalance-datasets","text":"If the test-validation set performance starts to look considerably better than the test performance, you may have overfit the validation set. This commonly occurs with small validation sets or lots of hyperparameter training. If this occurs, resample the validation set from the test distribution and get a fresh estimate of the performance.","title":"Step 4: Rebalance datasets"},{"location":"spring2021/lecture-7/#7-tune-hyperparameters","text":"One of the core challenges in hyperparameter optimization is very basic: which hyperparameters should you tune? As we consider this fundamental question, let\u2019s keep the following in mind: Models are more sensitive to some hyperparameters than others. This means we should focus our efforts on the more impactful hyperparameters. However, which hyperparameters are most important depends heavily on our choice of model. Certain rules of thumbs can help guide our initial thinking. Sensitivity is always relative to default values; if you use good defaults, you might start in a good place! See the following table for a ranked list of hyperparameters and their impact on the model:","title":"7 - Tune Hyperparameters"},{"location":"spring2021/lecture-7/#techniques-for-tuning-hyperparameter-optimization","text":"Now that we know which hyperparameters make the most sense to tune (using rules of thumb), let\u2019s consider the various methods of actually tuning them: Manual Hyperparameter Optimization . Colloquially referred to as Graduate Student Descent, this method works by taking a manual, detailed look at your algorithm, building intuition, and considering which hyperparameters would make the most difference. After figuring out these parameters, you train, evaluate, and guess a better hyperparameter value using your intuition for the algorithm and intelligence. While it may seem archaic, this method combines well with other methods (e.g., setting a range of values for hyperparameters) and has the main benefit of reducing computation time and cost if used skillfully. It can be time-consuming and challenging, but it can be a good starting point. Grid Search . Imagine each of your parameters plotted against each other on a grid, from which you uniformly sample values to test. For each point, you run a training run and evaluate performance. The advantages are that it\u2019s very simple and can often produce good results. However, it\u2019s quite inefficient, as you must run every combination of hyperparameters. It also often requires prior knowledge about the hyperparameters since we must manually set the range of values. Random Search : This method is recommended over grid search. Rather than sampling from the grid of values for the hyperparameter evenly, we\u2019ll choose n points sampled randomly across the grid. Empirically, this method produces better results than grid search. However, the results can be somewhat uninterpretable, with unexpected values in certain hyperparameters returned. Coarse-to-fine Search : Rather than running entirely random runs, we can gradually narrow in on the best hyperparameters through this method. Initially, start by defining a very large range to run a randomized search on. Within the pool of results, you can find N best results and hone in on the hyperparameter values used to generate those samples. As you iteratively perform this method, you can get excellent performance. This doesn\u2019t remove the manual component, as you have to select which range to continuously narrow your search to, but it\u2019s perhaps the most popular method available. Bayesian Hyperparameter Optimization : This is a reasonably sophisticated method, which you can read more about here and here . At a high level, start with a prior estimate of parameter distributions. Subsequently, maintain a probabilistic model of the relationship between hyperparameter values and model performance. As you maintain this model, you toggle between training with hyperparameter values that maximize the expected improvement (per the model) and use training results to update the initial probabilistic model and its expectations. This is a great, hands-off, efficient method to choose hyperparameters. However, these techniques can be quite challenging to implement from scratch. As libraries and infrastructure mature, the integration of these methods into training will become easier. In summary, you should probably start with coarse-to-fine random searches and move to Bayesian methods as your codebase matures and you\u2019re more certain of your model.","title":"Techniques for Tuning Hyperparameter Optimization"},{"location":"spring2021/lecture-7/#8-conclusion","text":"To wrap up this lecture, deep learning troubleshooting and debugging is really hard. It\u2019s difficult to tell if you have a bug because there are many possible sources for the same degradation in performance. Furthermore, the results can be sensitive to small changes in hyper-parameters and dataset makeup. To train bug-free deep learning models, we need to treat building them as an iterative process. If you skipped to the end, the following steps can make this process easier and catch errors as early as possible: Start Simple : Choose the simplest model and data possible. Implement and Debug : Once the model runs, overfit a single batch and reproduce a known result. Evaluate : Apply the bias-variance decomposition to decide what to do next. Tune Hyper-parameters : Use coarse-to-fine random searches to tune the model\u2019s hyper-parameters. Improve Model and Data : Make your model bigger if your model under-fits and add more data and/or regularization if your model over-fits. Here are additional resources that you can go to learn more: Andrew Ng\u2019s \u201c Machine Learning Yearning \u201d book. This Twitter thread from Andrej Karpathy. BYU\u2019s \u201c Practical Advice for Building Deep Neural Networks \u201d blog post.","title":"8 - Conclusion"},{"location":"spring2021/lecture-8/","text":"Lecture 8: Data Management Video Slides PDF Download Detailed Notes Notes were taken by James Le and Vishnu Rachakonda One of the best data science articles written in 2019 is \u201c Data science is different now \u201d by Vicki Boykis . Part of the article is a collection of tweets from other data science and machine learning practitioners. 1 - Data Management Overview When we think about what data management for deep learning entails, there might be many different data sources: images on S3, text files on a file system, logs spread across different machines, and maybe even records in a database. At some point, you need to get all of that data over to a local filesystem next to GPUs. The way you will get data over to that trainable format is different for every project and every company. For instance: Maybe you train your images on ImageNet, and all the images are just S3 URLs. Then, all you have to do is download them over to the local filesystem. Maybe you have a bunch of text files that you crawled yourself somewhere. You want to use Spark to process them on a cluster and Pandas data frame to analyze/select subsets that will be used in the local filesystem. Maybe you collect logs and records from your database into a data lake/warehouse (like Snowflake). Then, you process that output and convert them into a trainable format. There are countless possibilities that we are not going to cover completely in this lecture, but here are the key points to remember: Let the data flow through you : You should spend 10x as much time as you want to on exploring the dataset. Data is the best way to improve your overall ML project performance : Instead of trying new architectures or kicking off the hyper-parameter search, adding more data and augmenting the existing dataset will often be the best bang to your buck. Keep It Simple Stupid: We will discuss complex pipelines and new terms, but it\u2019s important to not over-complicate things and make data management a rocket science. 2 - Data Sources So, where do the training data come from? Most deep learning applications require lots of labeled data (with exceptions in applications of reinforcement learning, GANs, and GPT-3). There are publicly available datasets that can serve as a starting point, but there is no competitive advantage of using them. In fact, most companies usually spend a lot of money and time labeling their own data. Data Flywheel Data flywheel is an exciting concept: if you can get your models in front of the users, you can build your products in a mechanism that your users contribute good data back to you and improve the model predictions. This can enable rapid improvement after you get that v1 model out into the real world. Semi-Supervised Learning Semi-supervised learning is a relatively recent learning technique where the training data is autonomously (or automatically) labeled. It is still supervised learning, but the datasets do not need to be manually labeled by a human; but they can be labeled by finding and exploiting the relations (or correlations) between different input signals (that is, input coming from different sensor modalities). A natural advantage and consequence of semi-supervised learning are that this technique can be performed in an online fashion (given that data can be gathered and labeled without human intervention) more easily (with respect to, e.g., supervised learning), where models can be updated or trained entirely from scratch. Therefore, semi-supervised learning should also be well suited for changing environments, changing data, and, in general, changing requirements. For a text example, you can predict the future words from the past words, predict the beginning of a sentence from the end of a sentence, or predict the middle word of a sentence from the words surrounding it. You can even examine whether two sentences occur in the same paragraph in the same corpus of your training data. These are different ways to formulate the problem, where you don\u2019t need to label anything and simply use the data to supervise itself. This technique also applies to vision. Facebook AI recently released a model called SEER trained on 1 billion random images from the Internet. Yet, SEER achieved state-of-the-art accuracy on the ImageNet top-1 prediction task. If you\u2019re interested in learning more about semi-supervised learning, check out: Lilian Weng's \"Self-Supervised Learning\" post Facebook AI\u2019s \u201c Self-Supervised Learning: The Dark Matter Of Intelligence \u201d post Facebook AI\u2019s VISSL library for the SEER algorithm Data Augmentation Recent advances in deep learning models have been largely attributed to the quantity and diversity of data gathered in recent years. Data augmentation is a strategy that enables practitioners to significantly increase the diversity of data available for training models without actually collecting new data. Data augmentation techniques such as cropping, padding, and horizontal flipping are commonly used to train large neural networks. In fact, they are mostly required for training computer vision models. Both Keras and fast.ai provide functions that do this. Data augmentation also applies to other types of data. For tabular data, you can delete some cells to simulate missing data. For text, there are no well-established techniques, but you can replace words with synonyms and change the order of things. For speech and video, you can change speed, insert a pause, mix different sequences, and more. If you\u2019re interested in learning more about data augmentation, check out: Berkeley AI\u2019s \u201c 1000x Faster Data Augmentation \u201d post Edward Ma\u2019s \u201c nlpaug \u201d repository Synthetic Data Related to the concept of data augmentation is synthetic data, an underrated idea that is almost always worth starting with. Synthetic data is data that\u2019s generated programmatically. For example, photorealistic images of objects in arbitrary scenes can be rendered using video game engines or audio generated by a speech synthesis model from the known text. It\u2019s not unlike traditional data augmentation, where crops, flips, rotations, and distortions are used to increase the variety of data that models have to learn from. Synthetically generated data takes those same concepts even further. Most of today\u2019s synthetic data is visual. Tools and techniques developed to create photorealistic graphics in movies and computer games are repurposed to create the training data needed for machine learning. Not only can these rendering engines produce arbitrary numbers of images, but they can also produce annotations too. Bounding boxes, segmentation masks, depth maps, and any other metadata is output right alongside pictures, making it simple to build pipelines that produce their own data. Because samples are generated programmatically along with annotations, synthetic datasets are far cheaper to produce than traditional ones. That means we can create more data and iterate more often to produce better results. Need to add another class to your model? No problem. Need to add another key point to the annotation? Done. This is especially useful for applications in driving and robotics. If you\u2019re interested in learning more about synthetic data, check out: Dropbox\u2019s \u201c Creating A Modern OCR Pipeline Using Computer Vision and Deep Learning \u201d post Andrew Moffat\u2019s \u201c metabrite-receipt-tests \u201d repository Microsoft\u2019s AirSim simulator OpenAI\u2019s \u201c Ingredients For Robotics Research \u201d post 3 - Data Storage Data storage requirements for AI vary widely according to the application and the source material. Datasets in intelligence, defense, medical, science, and geology frequently combine petabyte-scale storage volumes with individual file sizes in the gigabyte range. By contrast, data used in areas such as supply chain analytics and fraud detection are much smaller. There are four building blocks in a data storage system: The filesystem The object storage The database The data lake or data warehouse Filesystem The filesystem is the foundational layer of storage. Its fundamental unit is a \u201cfile\u201d\u200a\u2014\u200awhich can be text or binary, is not versioned, and is easily overwritten. A file system can be as simple as a locally mounted disk containing all the files you need. More advanced options include networked filesystems ( NFS ), which are accessible over the network by multiple machines, and distributed file systems ( HDFS ) which are stored and accessed over multiple machines. The plots above display hard-drive speeds for SATA hard drive, SATA SSD, and NVMe SSD. The left plot shows the sustained throughput in MBps (how much information to copy a file): The latest iteration of hard drive technology (NVMe) is 6-10x more powerful than older iterations. The right plot shows the seek time in milliseconds (how long it takes to go to a file on disk): The NVMe is 25-30x faster than the old-school ones. What format should we store data in? For binary data (images, audios, videos), just files are enough. In Tensorflow, you have the TFRecord format to batch binary files, which does not seem to be necessary with the NVMe hard drives. For large tabular and text data, you have two choices: HDF5 is powerful but bloated and declining. Parquet is widespread and recommended. Feather is an up-and-coming open-source option powered by Apache Arrow. Both Tensorflow and PyTorch provide their native dataset class interfaces ( tf.data and PyTorch DataLoader ). Object Storage Object storage is an API over the filesystem that allows users to use a command on files (GET, PUT, DELETE) to a service without worrying where they are actually stored. Its fundamental unit is an \u201cobject,\u201d\u200a \u200awhich is usually binary (images, sound files\u2026). Object storage can be built with data versioning and data redundancy into the API. It is not as fast as local files but fast enough within the cloud. Database A database is a persistent, fast, scalable storage and retrieval of structured data. Its fundamental unit is a \u201crow\u201d (unique IDs, references to other rows, values in columns). Databases are also known for online transaction processing (OLTP). The mental model here is that everything is actually in memory, but the software ensures that everything is logged to disk and never lost. Databases are not built for binary data, so you must store the references (i.e., S3 URLs) instead. Here are our recommendations: PostgreSQL is the right choice most of the time, thanks to the support of unstructured JSON. SQLite is perfectly good for small projects. \u201cNoSQL\u201d was a big craze in the 2010s (like MongoDB ). However, they are not as fast as the relational database and also have consistency issues frequently. Redis is handy when you need a simple key-value store. Data Warehouse A data warehouse is a structured aggregation of data for analysis, known as online analytical processing (OLAP). Another acronym that you might have heard of is ETL ( Extract, Transform, Load ). The idea here is to extract data from data sources, transform the data into a common schema, and load the schema into the data warehouse. You can load the subset from the warehouse that you need and generate reports or run analytical queries. Well-known enterprise options in the market are Google BigQuery , Amazon Redshift , and Snowflake . SQL and DataFrames Most data solutions use SQL as the interface to the data, except for some (like Databricks) that use DataFrames . SQL is the standard interface for structured data. But in the Python ecosystem, Pandas is the main DataFrame. Our advice is to become fluent in both. Data Lake A data lake is the unstructured aggregation of data from multiple sources (databases, logs, expensive data transformations). It operates under the concept of ELT ( Extract, Load, Transform ) by dumping everything in the lake and transforming the data for specific needs later. Data \u201cLakehouse\u201d The current trend in the field is to combine data warehouses and data lakes in the same suite. The Databricks Lakehouse Platform is both a warehouse and a lake, operated as an open-source project called Delta Lake . You can store both structured and unstructured data in the platform and use them for analytics workloads and machine learning engines. What Goes Where? Binary data (images, sound files, compressed texts) are stored as objects . Metadata (labels, user activity) is stored in a database . If we need features that are not obtainable from the database ( logs ), we would want to set up a data lake and a process to aggregate the data required. At training time , we need to copy the necessary data to the filesystem on a fast drive. A lot is going on within the data management tooling and infrastructure. We recommend looking at a16z\u2019s \u201c Emerging Architectures For Modern Data Infrastructure \u201d article to get a broad look into this ecosystem. A highly recommended resource is Martin Kleppmann\u2019s book \u201c Designing Data-Intensive Applications ,\u201d\u200a \u200awhich provides excellent coverage of tools and approaches to build reliable, scalable, and maintainable data storage systems. 4 - Data Processing Data Dependencies Let\u2019s look at a motivational example of training a photo popularity predictor every night. For each photo, the training data must include these components: Metadata (such as posting time, title, location) that is in the database. Some features of the user (such as how many times they logged in today) that need to be computed from logs. Outputs of photo classifiers (such as content, style) that can be obtained after running the classifiers. The idea is that we have different sources of data, and they have different dependencies. The big hurdle here is that some tasks can\u2019t be started until other tasks are finished. Finishing a task should \u201ckick-off\u201d its dependencies. The simplest thing we can do is a \u201cMakefile\u201d to specify what action(s) depend on. But here are some limitations to this approach: What if re-computation needs to depend on content, not on a date? What if the dependencies are not files but disparate programs and databases? What if the work needs to be spread over multiple machines? What if many dependency graphs are executing all at once, with shared dependencies? MapReduce The old-school big data solutions to this are Hadoop and Apache Spark . These are MapReduce implementations, where you launch different tasks that each take a bit of the data (Map) and reduce their outputs into a single output (Reduce). Both Hadoop and Spark can run data processing operations and simple ML models on commodity hardware, with tricks to speed things up. In the modern environment, you can\u2019t run an ML model (in PyTorch or TensorFlow) as part of running a Spark job (unless that model itself is programmed in Spark). That\u2019s when you need a workflow management system like Apache Airflow . DAG In Airflow, a workflow is defined as a collection of tasks with directional dependencies, basically a directed acyclic graph (DAG). Each node in the graph is a task, and the edges define dependencies among the tasks. Tasks belong to two categories: (1) operators that execute some operation and (2) sensors that check for the state of a process or a data structure. The main components of Airflow include: (1) a metadata database that stores the state of tasks and workflows, (2) a scheduler that uses the DAGs definitions together with the state of tasks in the metadata database to decide what needs to be executed, and (3) an executor that determines which worker will execute each task. Besides Airflow, here are other notable solutions: Apache Beam : The TensorFlow team uses Apache Beam to generate big datasets and run those processing steps on Google Cloud Dataflow (a cloud orchestrator). Prefect : A similar idea to Airflow, Prefect is a Python framework that makes it easy to combine tasks into workflows, then deploy, schedule, and monitor their execution through the Prefect UI or API. dbt : dbt provides this data processing ability in SQL (called \u201canalytics engineering.\u201d) Dagster : Dagster is another data orchestrator for ML, analytics, and ETL. You can test locally and run anywhere with a unified view of data pipelines and assets. 5 - Feature Store Feature stores were first popularized by the ML team at Uber as part of their Michelangelo platform. Traditionally, ML systems are divided into two portions, offline processing and online processing. For the initial work of modeling, data that is generally static, perhaps stored in a data lake. Using some preprocessing methods (usually in SQL or Spark), data, which could be logfiles, requests, etc., are converted into features used to develop and train the model. The end result of this process is a model trained on a static sample of the data. This is an offline process . In contrast, the process of performing inference (e.g., Uber\u2019s need to return ride prices in real-time) often works with real-time data in an online process fashion. From a technology standpoint, whereas the offline use case might involve a data lake and Spark/SQL, the online processing use case involves technologies like Kafka and Cassandra that support speedier processing of creating or accessing the features required to perform inference. This difference in how features need to be created and accessed is a natural place for bugs to crop up. Harmonization of the online and offline processes would reduce bugs, so the Uber team, amongst others, introduced the concept of features stores to do just that. Members of the Uber team developed Tecton , a feature store company, which is one option to implement this system. An open-source alternative is Feast . To summarize, Tecton offers a handy definition of what a feature store is: \u201c an ML-specific data system that runs data pipelines that transform raw data into feature values, stores and manages the feature data itself, and serves feature data consistently for training and inference purposes.\u201d A word of caution: don\u2019t over-engineer your system according to what others are doing. It\u2019s easy to wrap yourself up in adopting many tools and systems that aren\u2019t as optimal as their publicity may make them seem. Work with the tools you have first! For an interesting example of this, look at how \u201c command-line tools can be 235x faster than your Hadoop cluster \u201d. 6 - Data Exploration The objective of data exploration is to understand and visualize the nature of the data you\u2019re modeling. Pandas is the Python workhorse of data visualization. It\u2019s highly recommended to be familiar with it. Dask is an alternative that can speed up data processing for large datasets that Pandas cannot handle through parallelization. Similarly, RAPIDS speeds up large dataset processing, though it does through the use of GPUs. 7 - Data Labeling Effective data labeling is a core ingredient of production machine learning systems. Most data labeling platforms have a standard set of features: the ability to generate bounding boxes, segmentations, key points, class assignments, etc. The crucial objective is agreeing on what makes a good annotation and training annotators accordingly. To avoid annotator error cropping up, write clear guidelines that clarify rules for edge cases and high-quality annotations. One way to acquire the material needed to write such a guide is to start by annotating yourself. As you generate labels, ensure the quality of the annotations holds up across the annotator base. Some participants will be more reliable than others. To develop an annotator base, there are a few options. Sources of Labor One option is to hire your own annotators , which can help with the speed and quality of annotations. This, however, can be expensive and difficult to scale. Another option is to crowdsource labels via a platform like Amazon Mechanical Turk, which is fast and cheap to set up, but for which the quality can be poorer. \u2026or full-service data labeling companies . Service Companies There are entire service companies that focus on data labeling that you can hire. Hiring such a company makes a great deal of sense, considering the time, labor, and software investment needed to label well at scale. To figure out the best data labeling company, start by annotating some gold standard data yourself. Then, contact and evaluate several companies on their value and a sample labeling task. Some companies in this space are FigureEight , Scale.ai , Labelbox , and Supervisely . Software If the costs of a full-service data labeling company are prohibitive, pure-play labeling software can be an option. Label Studio is a friendly open-source platform for this. New concepts to make labeling more strategic and efficient are coming to the fore. Aquarium helps you explore your data extensively and map the appropriate labeling strategy for classes that may be less prevalent or performant. Snorkel.ai offers a platform that incorporates weak supervision, which automatically labels data points based on heuristics and human feedback. In summary, if you can afford not to label, don\u2019t; get a full-service company to take care of it. Failing that, try to use existing software and a part-time annotator base work (in lieu of a crowdsourced workforce). 8 - Data Versioning Data versioning is important because machine learning models are part code and part data. If the data isn\u2019t versioned, the system isn\u2019t fully versioned! There are four levels to data versioning, which is similar to code versioning: Level 0: No versioning . All data lives on a filesystem, in S3, and/or in a database. The problem arises most acutely in this paradigm, as deployed ML systems (whose code may be versioned) can quickly become divorced from their corresponding data. Furthermore, reverting to older versions will be challenging. Level 1: Storing a snapshot of everything at training time . This works and can help you revert, but it\u2019s very hacky. Rather than doing this entire process manually, let\u2019s try to version automatically. Level 2: Versioned as a mix of assets and code . You store the large files with unique IDs in S3, with corresponding reference JSON versioned with code. You should avoid storing the data directly in the repository, as the metadata itself can get pretty large. Using git-lfs lets you store them just as easily as code. The git signature + of the raw data file fully defines a model\u2019s data and code. Level 3: Specialized solutions for version data . You should avoid them until you can identify their unique value add to your project. Some options here are DVC are Pachyderm . DVC has a Git-like workflow worth taking a closer look at. Dolt versions databases, if that\u2019s your need. 9 - Data Privacy Increasingly, unfettered access to data for machine learning is less desirable and prevalent. This is especially true in regulated industries like healthcare and finance. To address such challenges, researchers are developing new data privacy techniques. Federated learning trains a global model on several local devices without ever acquiring global access to the data. Federated learning is still research-use only due to these issues: (1) sending model updates can be expensive, (2) the depth of anonymization is not clear, and (3) system heterogeneity when it comes to training is unacceptably high. Another research area is differential privacy , which tries to aggregate data in ways that prevent identification. Finally, learning on encrypted data has potential. Most data privacy efforts are research-focused, as the tooling is not yet mature.","title":"Lecture 8: Data Management"},{"location":"spring2021/lecture-8/#lecture-8-data-management","text":"","title":"Lecture 8: Data Management"},{"location":"spring2021/lecture-8/#video","text":"","title":"Video"},{"location":"spring2021/lecture-8/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lecture-8/#detailed-notes","text":"Notes were taken by James Le and Vishnu Rachakonda One of the best data science articles written in 2019 is \u201c Data science is different now \u201d by Vicki Boykis . Part of the article is a collection of tweets from other data science and machine learning practitioners.","title":"Detailed Notes"},{"location":"spring2021/lecture-8/#1-data-management-overview","text":"When we think about what data management for deep learning entails, there might be many different data sources: images on S3, text files on a file system, logs spread across different machines, and maybe even records in a database. At some point, you need to get all of that data over to a local filesystem next to GPUs. The way you will get data over to that trainable format is different for every project and every company. For instance: Maybe you train your images on ImageNet, and all the images are just S3 URLs. Then, all you have to do is download them over to the local filesystem. Maybe you have a bunch of text files that you crawled yourself somewhere. You want to use Spark to process them on a cluster and Pandas data frame to analyze/select subsets that will be used in the local filesystem. Maybe you collect logs and records from your database into a data lake/warehouse (like Snowflake). Then, you process that output and convert them into a trainable format. There are countless possibilities that we are not going to cover completely in this lecture, but here are the key points to remember: Let the data flow through you : You should spend 10x as much time as you want to on exploring the dataset. Data is the best way to improve your overall ML project performance : Instead of trying new architectures or kicking off the hyper-parameter search, adding more data and augmenting the existing dataset will often be the best bang to your buck. Keep It Simple Stupid: We will discuss complex pipelines and new terms, but it\u2019s important to not over-complicate things and make data management a rocket science.","title":"1 - Data Management Overview"},{"location":"spring2021/lecture-8/#2-data-sources","text":"So, where do the training data come from? Most deep learning applications require lots of labeled data (with exceptions in applications of reinforcement learning, GANs, and GPT-3). There are publicly available datasets that can serve as a starting point, but there is no competitive advantage of using them. In fact, most companies usually spend a lot of money and time labeling their own data.","title":"2 - Data Sources"},{"location":"spring2021/lecture-8/#data-flywheel","text":"Data flywheel is an exciting concept: if you can get your models in front of the users, you can build your products in a mechanism that your users contribute good data back to you and improve the model predictions. This can enable rapid improvement after you get that v1 model out into the real world.","title":"Data Flywheel"},{"location":"spring2021/lecture-8/#semi-supervised-learning","text":"Semi-supervised learning is a relatively recent learning technique where the training data is autonomously (or automatically) labeled. It is still supervised learning, but the datasets do not need to be manually labeled by a human; but they can be labeled by finding and exploiting the relations (or correlations) between different input signals (that is, input coming from different sensor modalities). A natural advantage and consequence of semi-supervised learning are that this technique can be performed in an online fashion (given that data can be gathered and labeled without human intervention) more easily (with respect to, e.g., supervised learning), where models can be updated or trained entirely from scratch. Therefore, semi-supervised learning should also be well suited for changing environments, changing data, and, in general, changing requirements. For a text example, you can predict the future words from the past words, predict the beginning of a sentence from the end of a sentence, or predict the middle word of a sentence from the words surrounding it. You can even examine whether two sentences occur in the same paragraph in the same corpus of your training data. These are different ways to formulate the problem, where you don\u2019t need to label anything and simply use the data to supervise itself. This technique also applies to vision. Facebook AI recently released a model called SEER trained on 1 billion random images from the Internet. Yet, SEER achieved state-of-the-art accuracy on the ImageNet top-1 prediction task. If you\u2019re interested in learning more about semi-supervised learning, check out: Lilian Weng's \"Self-Supervised Learning\" post Facebook AI\u2019s \u201c Self-Supervised Learning: The Dark Matter Of Intelligence \u201d post Facebook AI\u2019s VISSL library for the SEER algorithm","title":"Semi-Supervised Learning"},{"location":"spring2021/lecture-8/#data-augmentation","text":"Recent advances in deep learning models have been largely attributed to the quantity and diversity of data gathered in recent years. Data augmentation is a strategy that enables practitioners to significantly increase the diversity of data available for training models without actually collecting new data. Data augmentation techniques such as cropping, padding, and horizontal flipping are commonly used to train large neural networks. In fact, they are mostly required for training computer vision models. Both Keras and fast.ai provide functions that do this. Data augmentation also applies to other types of data. For tabular data, you can delete some cells to simulate missing data. For text, there are no well-established techniques, but you can replace words with synonyms and change the order of things. For speech and video, you can change speed, insert a pause, mix different sequences, and more. If you\u2019re interested in learning more about data augmentation, check out: Berkeley AI\u2019s \u201c 1000x Faster Data Augmentation \u201d post Edward Ma\u2019s \u201c nlpaug \u201d repository","title":"Data Augmentation"},{"location":"spring2021/lecture-8/#synthetic-data","text":"Related to the concept of data augmentation is synthetic data, an underrated idea that is almost always worth starting with. Synthetic data is data that\u2019s generated programmatically. For example, photorealistic images of objects in arbitrary scenes can be rendered using video game engines or audio generated by a speech synthesis model from the known text. It\u2019s not unlike traditional data augmentation, where crops, flips, rotations, and distortions are used to increase the variety of data that models have to learn from. Synthetically generated data takes those same concepts even further. Most of today\u2019s synthetic data is visual. Tools and techniques developed to create photorealistic graphics in movies and computer games are repurposed to create the training data needed for machine learning. Not only can these rendering engines produce arbitrary numbers of images, but they can also produce annotations too. Bounding boxes, segmentation masks, depth maps, and any other metadata is output right alongside pictures, making it simple to build pipelines that produce their own data. Because samples are generated programmatically along with annotations, synthetic datasets are far cheaper to produce than traditional ones. That means we can create more data and iterate more often to produce better results. Need to add another class to your model? No problem. Need to add another key point to the annotation? Done. This is especially useful for applications in driving and robotics. If you\u2019re interested in learning more about synthetic data, check out: Dropbox\u2019s \u201c Creating A Modern OCR Pipeline Using Computer Vision and Deep Learning \u201d post Andrew Moffat\u2019s \u201c metabrite-receipt-tests \u201d repository Microsoft\u2019s AirSim simulator OpenAI\u2019s \u201c Ingredients For Robotics Research \u201d post","title":"Synthetic Data"},{"location":"spring2021/lecture-8/#3-data-storage","text":"Data storage requirements for AI vary widely according to the application and the source material. Datasets in intelligence, defense, medical, science, and geology frequently combine petabyte-scale storage volumes with individual file sizes in the gigabyte range. By contrast, data used in areas such as supply chain analytics and fraud detection are much smaller. There are four building blocks in a data storage system: The filesystem The object storage The database The data lake or data warehouse","title":"3 - Data Storage"},{"location":"spring2021/lecture-8/#filesystem","text":"The filesystem is the foundational layer of storage. Its fundamental unit is a \u201cfile\u201d\u200a\u2014\u200awhich can be text or binary, is not versioned, and is easily overwritten. A file system can be as simple as a locally mounted disk containing all the files you need. More advanced options include networked filesystems ( NFS ), which are accessible over the network by multiple machines, and distributed file systems ( HDFS ) which are stored and accessed over multiple machines. The plots above display hard-drive speeds for SATA hard drive, SATA SSD, and NVMe SSD. The left plot shows the sustained throughput in MBps (how much information to copy a file): The latest iteration of hard drive technology (NVMe) is 6-10x more powerful than older iterations. The right plot shows the seek time in milliseconds (how long it takes to go to a file on disk): The NVMe is 25-30x faster than the old-school ones. What format should we store data in? For binary data (images, audios, videos), just files are enough. In Tensorflow, you have the TFRecord format to batch binary files, which does not seem to be necessary with the NVMe hard drives. For large tabular and text data, you have two choices: HDF5 is powerful but bloated and declining. Parquet is widespread and recommended. Feather is an up-and-coming open-source option powered by Apache Arrow. Both Tensorflow and PyTorch provide their native dataset class interfaces ( tf.data and PyTorch DataLoader ).","title":"Filesystem"},{"location":"spring2021/lecture-8/#object-storage","text":"Object storage is an API over the filesystem that allows users to use a command on files (GET, PUT, DELETE) to a service without worrying where they are actually stored. Its fundamental unit is an \u201cobject,\u201d\u200a \u200awhich is usually binary (images, sound files\u2026). Object storage can be built with data versioning and data redundancy into the API. It is not as fast as local files but fast enough within the cloud.","title":"Object Storage"},{"location":"spring2021/lecture-8/#database","text":"A database is a persistent, fast, scalable storage and retrieval of structured data. Its fundamental unit is a \u201crow\u201d (unique IDs, references to other rows, values in columns). Databases are also known for online transaction processing (OLTP). The mental model here is that everything is actually in memory, but the software ensures that everything is logged to disk and never lost. Databases are not built for binary data, so you must store the references (i.e., S3 URLs) instead. Here are our recommendations: PostgreSQL is the right choice most of the time, thanks to the support of unstructured JSON. SQLite is perfectly good for small projects. \u201cNoSQL\u201d was a big craze in the 2010s (like MongoDB ). However, they are not as fast as the relational database and also have consistency issues frequently. Redis is handy when you need a simple key-value store.","title":"Database"},{"location":"spring2021/lecture-8/#data-warehouse","text":"A data warehouse is a structured aggregation of data for analysis, known as online analytical processing (OLAP). Another acronym that you might have heard of is ETL ( Extract, Transform, Load ). The idea here is to extract data from data sources, transform the data into a common schema, and load the schema into the data warehouse. You can load the subset from the warehouse that you need and generate reports or run analytical queries. Well-known enterprise options in the market are Google BigQuery , Amazon Redshift , and Snowflake .","title":"Data Warehouse"},{"location":"spring2021/lecture-8/#sql-and-dataframes","text":"Most data solutions use SQL as the interface to the data, except for some (like Databricks) that use DataFrames . SQL is the standard interface for structured data. But in the Python ecosystem, Pandas is the main DataFrame. Our advice is to become fluent in both.","title":"SQL and DataFrames"},{"location":"spring2021/lecture-8/#data-lake","text":"A data lake is the unstructured aggregation of data from multiple sources (databases, logs, expensive data transformations). It operates under the concept of ELT ( Extract, Load, Transform ) by dumping everything in the lake and transforming the data for specific needs later.","title":"Data Lake"},{"location":"spring2021/lecture-8/#data-lakehouse","text":"The current trend in the field is to combine data warehouses and data lakes in the same suite. The Databricks Lakehouse Platform is both a warehouse and a lake, operated as an open-source project called Delta Lake . You can store both structured and unstructured data in the platform and use them for analytics workloads and machine learning engines.","title":"Data \u201cLakehouse\u201d"},{"location":"spring2021/lecture-8/#what-goes-where","text":"Binary data (images, sound files, compressed texts) are stored as objects . Metadata (labels, user activity) is stored in a database . If we need features that are not obtainable from the database ( logs ), we would want to set up a data lake and a process to aggregate the data required. At training time , we need to copy the necessary data to the filesystem on a fast drive. A lot is going on within the data management tooling and infrastructure. We recommend looking at a16z\u2019s \u201c Emerging Architectures For Modern Data Infrastructure \u201d article to get a broad look into this ecosystem. A highly recommended resource is Martin Kleppmann\u2019s book \u201c Designing Data-Intensive Applications ,\u201d\u200a \u200awhich provides excellent coverage of tools and approaches to build reliable, scalable, and maintainable data storage systems.","title":"What Goes Where?"},{"location":"spring2021/lecture-8/#4-data-processing","text":"","title":"4 - Data Processing"},{"location":"spring2021/lecture-8/#data-dependencies","text":"Let\u2019s look at a motivational example of training a photo popularity predictor every night. For each photo, the training data must include these components: Metadata (such as posting time, title, location) that is in the database. Some features of the user (such as how many times they logged in today) that need to be computed from logs. Outputs of photo classifiers (such as content, style) that can be obtained after running the classifiers. The idea is that we have different sources of data, and they have different dependencies. The big hurdle here is that some tasks can\u2019t be started until other tasks are finished. Finishing a task should \u201ckick-off\u201d its dependencies. The simplest thing we can do is a \u201cMakefile\u201d to specify what action(s) depend on. But here are some limitations to this approach: What if re-computation needs to depend on content, not on a date? What if the dependencies are not files but disparate programs and databases? What if the work needs to be spread over multiple machines? What if many dependency graphs are executing all at once, with shared dependencies?","title":"Data Dependencies"},{"location":"spring2021/lecture-8/#mapreduce","text":"The old-school big data solutions to this are Hadoop and Apache Spark . These are MapReduce implementations, where you launch different tasks that each take a bit of the data (Map) and reduce their outputs into a single output (Reduce). Both Hadoop and Spark can run data processing operations and simple ML models on commodity hardware, with tricks to speed things up. In the modern environment, you can\u2019t run an ML model (in PyTorch or TensorFlow) as part of running a Spark job (unless that model itself is programmed in Spark). That\u2019s when you need a workflow management system like Apache Airflow .","title":"MapReduce"},{"location":"spring2021/lecture-8/#dag","text":"In Airflow, a workflow is defined as a collection of tasks with directional dependencies, basically a directed acyclic graph (DAG). Each node in the graph is a task, and the edges define dependencies among the tasks. Tasks belong to two categories: (1) operators that execute some operation and (2) sensors that check for the state of a process or a data structure. The main components of Airflow include: (1) a metadata database that stores the state of tasks and workflows, (2) a scheduler that uses the DAGs definitions together with the state of tasks in the metadata database to decide what needs to be executed, and (3) an executor that determines which worker will execute each task. Besides Airflow, here are other notable solutions: Apache Beam : The TensorFlow team uses Apache Beam to generate big datasets and run those processing steps on Google Cloud Dataflow (a cloud orchestrator). Prefect : A similar idea to Airflow, Prefect is a Python framework that makes it easy to combine tasks into workflows, then deploy, schedule, and monitor their execution through the Prefect UI or API. dbt : dbt provides this data processing ability in SQL (called \u201canalytics engineering.\u201d) Dagster : Dagster is another data orchestrator for ML, analytics, and ETL. You can test locally and run anywhere with a unified view of data pipelines and assets.","title":"DAG"},{"location":"spring2021/lecture-8/#5-feature-store","text":"Feature stores were first popularized by the ML team at Uber as part of their Michelangelo platform. Traditionally, ML systems are divided into two portions, offline processing and online processing. For the initial work of modeling, data that is generally static, perhaps stored in a data lake. Using some preprocessing methods (usually in SQL or Spark), data, which could be logfiles, requests, etc., are converted into features used to develop and train the model. The end result of this process is a model trained on a static sample of the data. This is an offline process . In contrast, the process of performing inference (e.g., Uber\u2019s need to return ride prices in real-time) often works with real-time data in an online process fashion. From a technology standpoint, whereas the offline use case might involve a data lake and Spark/SQL, the online processing use case involves technologies like Kafka and Cassandra that support speedier processing of creating or accessing the features required to perform inference. This difference in how features need to be created and accessed is a natural place for bugs to crop up. Harmonization of the online and offline processes would reduce bugs, so the Uber team, amongst others, introduced the concept of features stores to do just that. Members of the Uber team developed Tecton , a feature store company, which is one option to implement this system. An open-source alternative is Feast . To summarize, Tecton offers a handy definition of what a feature store is: \u201c an ML-specific data system that runs data pipelines that transform raw data into feature values, stores and manages the feature data itself, and serves feature data consistently for training and inference purposes.\u201d A word of caution: don\u2019t over-engineer your system according to what others are doing. It\u2019s easy to wrap yourself up in adopting many tools and systems that aren\u2019t as optimal as their publicity may make them seem. Work with the tools you have first! For an interesting example of this, look at how \u201c command-line tools can be 235x faster than your Hadoop cluster \u201d.","title":"5 - Feature Store"},{"location":"spring2021/lecture-8/#6-data-exploration","text":"The objective of data exploration is to understand and visualize the nature of the data you\u2019re modeling. Pandas is the Python workhorse of data visualization. It\u2019s highly recommended to be familiar with it. Dask is an alternative that can speed up data processing for large datasets that Pandas cannot handle through parallelization. Similarly, RAPIDS speeds up large dataset processing, though it does through the use of GPUs.","title":"6 - Data Exploration"},{"location":"spring2021/lecture-8/#7-data-labeling","text":"Effective data labeling is a core ingredient of production machine learning systems. Most data labeling platforms have a standard set of features: the ability to generate bounding boxes, segmentations, key points, class assignments, etc. The crucial objective is agreeing on what makes a good annotation and training annotators accordingly. To avoid annotator error cropping up, write clear guidelines that clarify rules for edge cases and high-quality annotations. One way to acquire the material needed to write such a guide is to start by annotating yourself. As you generate labels, ensure the quality of the annotations holds up across the annotator base. Some participants will be more reliable than others. To develop an annotator base, there are a few options.","title":"7 - Data Labeling"},{"location":"spring2021/lecture-8/#sources-of-labor","text":"One option is to hire your own annotators , which can help with the speed and quality of annotations. This, however, can be expensive and difficult to scale. Another option is to crowdsource labels via a platform like Amazon Mechanical Turk, which is fast and cheap to set up, but for which the quality can be poorer. \u2026or full-service data labeling companies .","title":"Sources of Labor"},{"location":"spring2021/lecture-8/#service-companies","text":"There are entire service companies that focus on data labeling that you can hire. Hiring such a company makes a great deal of sense, considering the time, labor, and software investment needed to label well at scale. To figure out the best data labeling company, start by annotating some gold standard data yourself. Then, contact and evaluate several companies on their value and a sample labeling task. Some companies in this space are FigureEight , Scale.ai , Labelbox , and Supervisely .","title":"Service Companies"},{"location":"spring2021/lecture-8/#software","text":"If the costs of a full-service data labeling company are prohibitive, pure-play labeling software can be an option. Label Studio is a friendly open-source platform for this. New concepts to make labeling more strategic and efficient are coming to the fore. Aquarium helps you explore your data extensively and map the appropriate labeling strategy for classes that may be less prevalent or performant. Snorkel.ai offers a platform that incorporates weak supervision, which automatically labels data points based on heuristics and human feedback. In summary, if you can afford not to label, don\u2019t; get a full-service company to take care of it. Failing that, try to use existing software and a part-time annotator base work (in lieu of a crowdsourced workforce).","title":"Software"},{"location":"spring2021/lecture-8/#8-data-versioning","text":"Data versioning is important because machine learning models are part code and part data. If the data isn\u2019t versioned, the system isn\u2019t fully versioned! There are four levels to data versioning, which is similar to code versioning: Level 0: No versioning . All data lives on a filesystem, in S3, and/or in a database. The problem arises most acutely in this paradigm, as deployed ML systems (whose code may be versioned) can quickly become divorced from their corresponding data. Furthermore, reverting to older versions will be challenging. Level 1: Storing a snapshot of everything at training time . This works and can help you revert, but it\u2019s very hacky. Rather than doing this entire process manually, let\u2019s try to version automatically. Level 2: Versioned as a mix of assets and code . You store the large files with unique IDs in S3, with corresponding reference JSON versioned with code. You should avoid storing the data directly in the repository, as the metadata itself can get pretty large. Using git-lfs lets you store them just as easily as code. The git signature + of the raw data file fully defines a model\u2019s data and code. Level 3: Specialized solutions for version data . You should avoid them until you can identify their unique value add to your project. Some options here are DVC are Pachyderm . DVC has a Git-like workflow worth taking a closer look at. Dolt versions databases, if that\u2019s your need.","title":"8 - Data Versioning"},{"location":"spring2021/lecture-8/#9-data-privacy","text":"Increasingly, unfettered access to data for machine learning is less desirable and prevalent. This is especially true in regulated industries like healthcare and finance. To address such challenges, researchers are developing new data privacy techniques. Federated learning trains a global model on several local devices without ever acquiring global access to the data. Federated learning is still research-use only due to these issues: (1) sending model updates can be expensive, (2) the depth of anonymization is not clear, and (3) system heterogeneity when it comes to training is unacceptably high. Another research area is differential privacy , which tries to aggregate data in ways that prevent identification. Finally, learning on encrypted data has potential. Most data privacy efforts are research-focused, as the tooling is not yet mature.","title":"9 - Data Privacy"},{"location":"spring2021/lecture-9/","text":"Lecture 9: AI Ethics Video Slides PDF Download Detailed Notes Notes were taken by James Le and Vishnu Rachakonda A preamble : Ethics is a vast subject spanning many disciplines and addressing many real different problems. As ML practitioners, we need to have a student mindset and do not assume we have the answers because these are not easy problems. 1 - What is Ethics? Let\u2019s start with the definition of ethics: Ethics are not feelings because your feelings might mislead you. Ethics are not laws because ethics can supersede laws. Ethics are not societal beliefs because even an immoral society has its set of ethics. Ethical Theories Kevin Binz put together a tour of ethical theories , including: The divine command theory states that a behavior is moral if the divine commands it. This theory might be accurate, but philosophy doesn\u2019t engage with it. The virtue ethics theory states that a behavior is moral if it upholds a person\u2019s virtues (bravery, generosity, love, etc.). This theory is apparently robust to philosophical inquiry, but there is increasing evidence that virtues are not persistent across a person\u2019s life and somewhat illusory. The deontology (duty-based) theory states that a behavior is moral if it satisfies the categorical imperative (i.e., don\u2019t lie, don\u2019t kill). This theory might lead to counter-intuitive moral decisions in many situations and has unacceptable inflexibility to many people. The utilitarian theory states that a behavior is moral if it brings the most good to the most people. But of course, how do we measure utility? There does not seem to be a clear winner among professional philosophers. From this survey , there appears to be an even split between virtue, deontology, and utilitarianism. The Trolley Problem The \u201ctrolley problem\u201d is often used to gain intuition about a person\u2019s ethics by presenting to him/her a moral dilemma. The classic dilemma is that: You see a trolley that is about to run over five people. But you can divert it to run over only one person. Would you do it? It actually leads to a lot of good memes. \ud83e\udd23 Another prominent ethical theory is John Rawl\u2019s theory of justice . Rawls argued that equal distribution of resources should be the desirable state of nature instead of following utilitarian philosophies. A Theory of Justice holds that every individual has an equal right to basic liberties. They should have the right to opportunities and an equal chance as other individuals of similar ability. When ethics are applied to technology, it\u2019s essential to understand that they are not static and change with technological progress . Some examples include the industrial revolution, the right to Internet access, birth control, surrogate pregnancy, embryo selection, artificial womb, lab-grown meat, and much more. An excellent book to explore is Juan Enriquez\u2019s \u201c Right/Wrong: How Technology Transforms Our Ethics .\u201d 2 - Long-Term AI Ethical Problems Autonomous Weapons The first example that came to a lot of people\u2019s minds is autonomous weapons. It might be tempting to dismiss it as far-fetched and unrealistic at this time. But as the saying goes, \u201cthe future is already here, just not evenly distributed\u201d: Israel apparently has autonomous \u2018robo-snipers\u2019 on its borders today. NYPD has been deploying Boston Dynamics robots in crime situations. Lost Human Labor Replacing human labor is another concern that has been creeping upon us. With the pandemic, you probably saw many articles saying that millions of people have lost jobs and probably will never get them back (replaced by AI). This could be both good and bad. \ud83e\udd14 It\u2019s bad if there are no social safety net and no other jobs for the unemployed. It\u2019s good because there is a megatrend of the demographic inversion. As the world\u2019s population tops out and baby booms vary across regions, the economy can\u2019t function as currently designed. Therefore, we need labor from somewhere. Rodney Brooks, a roboticist from MIT and the founder of iRobot, advocates for having robots in order to have a functioning economy in the next few decades. An interesting spin on this worry is that AI is not necessarily replacing human labor but controlling human labor. This article from The Verge provides more details about working in conditions in warehouses, call centers, and other sectors. If you want to go down the rabbit hole, check out this series \u201c Manna - Two Views of Humanity\u2019s Future \u201d from Marshall Brain. Human Extinction The final worry is that if AI is superintelligent, then it is capable of replacing humans entirely . The Alignment Problem What\u2019s common in all these long-term problems is the alignment problem. This notion is often expressed by the parable of the \u201c paperclip maximizer \u201d - given the goal of producing paperclips, an AGI will eventually turn every atom space into paperclips. This is an old lesson about how to establish and communicate our goals and values to technologies precisely. The guiding principle to build safe AI is that the AI systems we build need to be aligned with our goals and values. This is a deep topic and active research area in many places (including CHAI at Berkeley ). As a matter of fact, this alignment lens is useful for near-term problems as well, as discussed in the rest of the lecture. 3 - Hiring Let\u2019s say we are building an ML model to predict hiring decisions given a resume (inspired by this Reuters article about Amazon\u2019s hiring algorithm). What should the data contain? Should it be the hiring decision that was made? Or should it be the eventual job performance given the person that was hired? The data comes from the world, which is known to be biased in many ways: the hiring pipeline (not enough women educated for a software engineering job), the hiring decisions (employers intentionally or unintentionally select people that match some prejudice), the performance ratings (people get promoted not because they are good of their job, but because they match other expectations of the promoter). Because the world is biased, the data will be biased no matter how we structure the data. Therefore, the model trained on that data will be biased . The model will be used to aid or make an action: sourcing candidates, double-checking human decisions, or making the actual hiring decisions? In the last case, that action will amplify existing biases. Amplifying existing biases is not aligned with our goals and values! \ud83d\ude20 4 - Fairness COMPAS Let\u2019s look at a case study about COMPAS - Correctional Offender Management Profiling for Alternative Sanctions system to discuss fairness. The goal of this system is to predict recidivism (committing another crime), such that judges can consult a 1-10 score in pre-trial sentencing decisions. The motivation of this system is to be less biased than humans because the criminal justice system is notoriously biased against certain races. The solution of this system is to (1) gather relevant data, (2) exclude protected class attributes (race, gender, age, etc.), (3) train the model by ensuring that the model\u2019s score corresponds to the same probability of recidivism across all demographic groups. And yet, this famous ProPublica report exposes the bias of this system against blacks. Fairness Definitions (From Aravind Narayanan\u2019s Lecture ) There are a bunch of fairness definitions. The first one concerns bias. We often mean statistical bias in machine learning - the difference between the model\u2019s expected value and the true value. In this sense, the COMPAS scores are not biased with respect to re-arrest. This is an important caveat; because we only have data for arrests, not crimes committed. There may well be bias in arrests (the data-generating process). Even if COMPAS is free of statistical bias, is it an adequate fairness criterion? Is this criterion aligned with human values? Taking a step back and look at the classic binary classification problem setup, we have the confusion matrix as seen above. The interesting question to ask is what do different stakeholders want from the classifier? The decision-maker (the judge or the prosecutor) asks: \u201cOf those that I have labeled high risk, how many recidivated?\u201d This corresponds to the model\u2019s predictive value = TP / (TP + FP). The defendant asks: \u201cWhat is the probability I\u2019ll be incorrectly classified as high risk?\u201d This corresponds to the model\u2019s false positive rate = FP / (FP + FN). The society at large might care about: \u201cIs the selected set demographically balanced?\u201d This could be demographic parity, which leads to the definition of group fairness (\u201cDo outcomes differ between groups, which we have no reason to believe are actually different?\u201d). A lot of these group fairness metrics have natural motivations, so there\u2019s not a single correct fairness definition. They depend on the politics of the situation. Let\u2019s forget about demographic parity and only pick the two most important metrics (false-positive rate and false-negative rate) while allowing the model to use protected class attributes. We fail the individual fairness definition, which uses a single threshold for the sentencing decision or the pre-sentencing release decision. Even if we pick one metric to optimize for, we still sacrifice some utility (providing public safety or releasing too few defendants). To build more intuition, you should play around with this interactive demo on attacking discrimination with smarter ML from Google Research. Finally, ML can be very good at finding patterns that maybe humans can\u2019t find. For instance, your ZIP code and age might be highly correlated with your race. That means the model can always pick up from a protected class attribute from other attributes. Read this paper on Equality of Opportunity in Supervised Learning for more detail. Tradeoffs There are tradeoffs between different measures of group fairness, between the definitions of group fairness and individual fairness, and between the notions of fairness and utility. In fact, these tradeoffs are not specific to machine learning. They apply to human decision making too. There is also a tension between disparate treatment and disparate impact, which is another deep subject. Seeing The Water In order to see the water, it would be noteworthy to think about the differences between environmental equity and environmental justice : Equality: The assumption is that everyone benefits from the same supports. This is equal treatment. Equity: Everyone gets the support they need (\u201caffirmative action\u201d), thus producing equity. Justice: All parties are supported equally because the cause of the inequity was addressed. The systematic barrier has been removed. The justice mindset is valuable to have. As computer scientists, we have very literal minds and argue for the rationality of our choices. But taking a step back and seeing the whole situation would be even more crucial. 5 - Representation The Problem Watch this simple video : a hand sanitizer dispenser that doesn\u2019t recognize racially diverse hands. It\u2019s a small example but illustrates a big problem: a lack of attention to diverse representation in the development of technology products . This occurs across fields, such as drug development, photography, etc. As pointed out by Timnit Gebru in this New York Times article , the exclusion of people from certain backgrounds poses a serious long-term threat to the viability of ML systems. One way to address this challenge head-on is to focus on the inclusion of people from all backgrounds . Groups like Black in AI , Women in Machine Learning , and Latinx in AI play a big role in building communities of underrepresented people and inviting them into the AI/ML industry. Another is to deliberately ensure products reflect inclusive values . For example, Google Images now yields a diverse set of images for the search term \u201cCEO\u201d whereas it used to return entirely white, middle-aged men. Word Embeddings A particularly relevant example of bias in machine learning is the underlying bias in the Word2Vec model. Word2Vec introduced vector math for word embeddings and is frequently used for NLP applications. The original model was trained on a large corpus, and the weights were open-sourced. As these weights were examined, underlying bias in the word logic was discovered. Terms like \u201cdoctor\u201d and \u201cprogrammer\u201d were associated with men, while \u201chomemaker\u201d and \u201cnurse\u201d were associated with women. Translating our existing biases like these into the ML domain is undesirable, to say the least! \ud83d\ude29 One potential solution to address this problem is to de-bias at training time with carefully screened data . With newer models like GPT-3 that are trained on massive swathes of data, this can be hard to do in practice. Bender and Gebru advise in a 2021 paper to reduce the dependence on large, unfiltered datasets and more carefully document the data-generating process. Alternatively, you can alert the user proactively of potential bias. Addressing this problem of bias in language models is an open problem. Seeing The Water Part of the challenge lies in agreeing on whether the model should learn about the world as it is in the data or learn about the world in a more idealistic manner . This is application-specific. A model recognizing hate speech on Facebook should probably learn about the world as it is, or a model interacting with humans\u2019 conversations should adhere to proper ideals. Of course, this begs the question of who decides what ideals are desirable and suitable for a model to follow. Consider these questions as you build models for various applications. Ultimately, these challenges in machine learning systems development are rooted in ethics. Face recognition is a boundary-breaking area that has been grappling with ethical concerns . Importantly, face recognition illustrates how technology can impact ethics and change standards. Is the loss of privacy associated with face recognition desirable? Relatedly, are face recognition systems performing well across groups? The question of performance should generally follow ethics to avoid distracting from the fundamental ethical issues (e.g., civil rights, privacy, etc.). 6 - Best Practices A recent survey of ML practitioners found these to be the top challenges in ensuring fairness that they face: Receiving support in fairness-aware data collection and curation Overcoming team\u2019s blind spots Implementing more proactive fairness auditing processes Auditing complex ML systems Deciding how to address particular instances of unfairness Addressing biases in the humans embedded throughout the ML development pipeline Suggestions Rachel Thomas, the co-creator of Fast.ai, has some great ideas on how to confront fairness issues proactively : Perform ethical risk sweeping . Akin to cybersecurity penetration testing, where engineers intentionally try to find faults, you can try to engage in regular fairness checks on behalf of different stakeholders. Expand the ethical circle. Try to consider different perspectives than yours regularly, and invite such people into your decision-making \u201ccircle\u201d to ensure that systems do not lead to unfair outcomes. Think about worst-case scenarios. What incentives may crop up for people to engage in unethical behavior? For example, the upvote-downvote system and recommendations on Reddit can cause toxic behavior. Think about such incentives and requisite safeguards in advance. Close the loop! You have to put in place a process to keep improving, as fairness is not a static test (just like raw performance). One powerful tool, proposed by Gebru and Mitchell in 2018, is adopting \u201c model cards .\u201d For every ML model, make a simple page that discusses the expectations (i.e., input/output), tradeoffs, performance, and known limitations. Engaging in this documentation exercise allows for teams to confront fairness issues head-on more effectively. The objective here is to get everyone on the same page about what the model can and cannot do from a fairness perspective . We believe everyone should do this, considering how easy it is. Other methods like bias audits are also useful, as the Aequitas team at UChicago shows. A Code of Ethics? AI is a reflection of society. It\u2019s impossible to expect AI to be completely unbiased when humans still struggle with the problem. However, we can try our best to ensure that these biases are not amplified by AI and mitigate any such damage. Making fairness and ethics a routine part of AI development by professionals and teams is crucial to addressing the challenge. Perhaps an AI code of ethics (akin to the Hippocratic Oath) would make sense! 7 - Where To Learn More Here are some links to learn more: https://ethics.fast.ai/ : a course by the fast.ai team on practical data ethics consisting of 6 lectures. CS 294: Fairness in Machine Learning : A graduate course (similar to FSDL) taught at Berkeley in 2017 about AI ethics. Fair ML Book : A book being written by the instructor of the aforementioned course on fair ML. KDD Tutorial on Fair ML : Taught by folks from CMU, this is a workshop addressing some of the topics in this lecture. The Alignment Problem : a book that confronts present-day issues in AI alignment. Weapons of Math Destruction : a popular book about current issues like Facebook\u2019s News Feed.","title":"Lecture 9: AI Ethics"},{"location":"spring2021/lecture-9/#lecture-9-ai-ethics","text":"","title":"Lecture 9: AI Ethics"},{"location":"spring2021/lecture-9/#video","text":"","title":"Video"},{"location":"spring2021/lecture-9/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lecture-9/#detailed-notes","text":"Notes were taken by James Le and Vishnu Rachakonda A preamble : Ethics is a vast subject spanning many disciplines and addressing many real different problems. As ML practitioners, we need to have a student mindset and do not assume we have the answers because these are not easy problems.","title":"Detailed Notes"},{"location":"spring2021/lecture-9/#1-what-is-ethics","text":"Let\u2019s start with the definition of ethics: Ethics are not feelings because your feelings might mislead you. Ethics are not laws because ethics can supersede laws. Ethics are not societal beliefs because even an immoral society has its set of ethics.","title":"1 - What is Ethics?"},{"location":"spring2021/lecture-9/#ethical-theories","text":"Kevin Binz put together a tour of ethical theories , including: The divine command theory states that a behavior is moral if the divine commands it. This theory might be accurate, but philosophy doesn\u2019t engage with it. The virtue ethics theory states that a behavior is moral if it upholds a person\u2019s virtues (bravery, generosity, love, etc.). This theory is apparently robust to philosophical inquiry, but there is increasing evidence that virtues are not persistent across a person\u2019s life and somewhat illusory. The deontology (duty-based) theory states that a behavior is moral if it satisfies the categorical imperative (i.e., don\u2019t lie, don\u2019t kill). This theory might lead to counter-intuitive moral decisions in many situations and has unacceptable inflexibility to many people. The utilitarian theory states that a behavior is moral if it brings the most good to the most people. But of course, how do we measure utility? There does not seem to be a clear winner among professional philosophers. From this survey , there appears to be an even split between virtue, deontology, and utilitarianism.","title":"Ethical Theories"},{"location":"spring2021/lecture-9/#the-trolley-problem","text":"The \u201ctrolley problem\u201d is often used to gain intuition about a person\u2019s ethics by presenting to him/her a moral dilemma. The classic dilemma is that: You see a trolley that is about to run over five people. But you can divert it to run over only one person. Would you do it? It actually leads to a lot of good memes. \ud83e\udd23 Another prominent ethical theory is John Rawl\u2019s theory of justice . Rawls argued that equal distribution of resources should be the desirable state of nature instead of following utilitarian philosophies. A Theory of Justice holds that every individual has an equal right to basic liberties. They should have the right to opportunities and an equal chance as other individuals of similar ability. When ethics are applied to technology, it\u2019s essential to understand that they are not static and change with technological progress . Some examples include the industrial revolution, the right to Internet access, birth control, surrogate pregnancy, embryo selection, artificial womb, lab-grown meat, and much more. An excellent book to explore is Juan Enriquez\u2019s \u201c Right/Wrong: How Technology Transforms Our Ethics .\u201d","title":"The Trolley Problem"},{"location":"spring2021/lecture-9/#2-long-term-ai-ethical-problems","text":"","title":"2 - Long-Term AI Ethical Problems"},{"location":"spring2021/lecture-9/#autonomous-weapons","text":"The first example that came to a lot of people\u2019s minds is autonomous weapons. It might be tempting to dismiss it as far-fetched and unrealistic at this time. But as the saying goes, \u201cthe future is already here, just not evenly distributed\u201d: Israel apparently has autonomous \u2018robo-snipers\u2019 on its borders today. NYPD has been deploying Boston Dynamics robots in crime situations.","title":"Autonomous Weapons"},{"location":"spring2021/lecture-9/#lost-human-labor","text":"Replacing human labor is another concern that has been creeping upon us. With the pandemic, you probably saw many articles saying that millions of people have lost jobs and probably will never get them back (replaced by AI). This could be both good and bad. \ud83e\udd14 It\u2019s bad if there are no social safety net and no other jobs for the unemployed. It\u2019s good because there is a megatrend of the demographic inversion. As the world\u2019s population tops out and baby booms vary across regions, the economy can\u2019t function as currently designed. Therefore, we need labor from somewhere. Rodney Brooks, a roboticist from MIT and the founder of iRobot, advocates for having robots in order to have a functioning economy in the next few decades. An interesting spin on this worry is that AI is not necessarily replacing human labor but controlling human labor. This article from The Verge provides more details about working in conditions in warehouses, call centers, and other sectors. If you want to go down the rabbit hole, check out this series \u201c Manna - Two Views of Humanity\u2019s Future \u201d from Marshall Brain.","title":"Lost Human Labor"},{"location":"spring2021/lecture-9/#human-extinction","text":"The final worry is that if AI is superintelligent, then it is capable of replacing humans entirely .","title":"Human Extinction"},{"location":"spring2021/lecture-9/#the-alignment-problem","text":"What\u2019s common in all these long-term problems is the alignment problem. This notion is often expressed by the parable of the \u201c paperclip maximizer \u201d - given the goal of producing paperclips, an AGI will eventually turn every atom space into paperclips. This is an old lesson about how to establish and communicate our goals and values to technologies precisely. The guiding principle to build safe AI is that the AI systems we build need to be aligned with our goals and values. This is a deep topic and active research area in many places (including CHAI at Berkeley ). As a matter of fact, this alignment lens is useful for near-term problems as well, as discussed in the rest of the lecture.","title":"The Alignment Problem"},{"location":"spring2021/lecture-9/#3-hiring","text":"Let\u2019s say we are building an ML model to predict hiring decisions given a resume (inspired by this Reuters article about Amazon\u2019s hiring algorithm). What should the data contain? Should it be the hiring decision that was made? Or should it be the eventual job performance given the person that was hired? The data comes from the world, which is known to be biased in many ways: the hiring pipeline (not enough women educated for a software engineering job), the hiring decisions (employers intentionally or unintentionally select people that match some prejudice), the performance ratings (people get promoted not because they are good of their job, but because they match other expectations of the promoter). Because the world is biased, the data will be biased no matter how we structure the data. Therefore, the model trained on that data will be biased . The model will be used to aid or make an action: sourcing candidates, double-checking human decisions, or making the actual hiring decisions? In the last case, that action will amplify existing biases. Amplifying existing biases is not aligned with our goals and values! \ud83d\ude20","title":"3 - Hiring"},{"location":"spring2021/lecture-9/#4-fairness","text":"","title":"4 - Fairness"},{"location":"spring2021/lecture-9/#compas","text":"Let\u2019s look at a case study about COMPAS - Correctional Offender Management Profiling for Alternative Sanctions system to discuss fairness. The goal of this system is to predict recidivism (committing another crime), such that judges can consult a 1-10 score in pre-trial sentencing decisions. The motivation of this system is to be less biased than humans because the criminal justice system is notoriously biased against certain races. The solution of this system is to (1) gather relevant data, (2) exclude protected class attributes (race, gender, age, etc.), (3) train the model by ensuring that the model\u2019s score corresponds to the same probability of recidivism across all demographic groups. And yet, this famous ProPublica report exposes the bias of this system against blacks.","title":"COMPAS"},{"location":"spring2021/lecture-9/#fairness-definitions-from-aravind-narayanans-lecture","text":"There are a bunch of fairness definitions. The first one concerns bias. We often mean statistical bias in machine learning - the difference between the model\u2019s expected value and the true value. In this sense, the COMPAS scores are not biased with respect to re-arrest. This is an important caveat; because we only have data for arrests, not crimes committed. There may well be bias in arrests (the data-generating process). Even if COMPAS is free of statistical bias, is it an adequate fairness criterion? Is this criterion aligned with human values? Taking a step back and look at the classic binary classification problem setup, we have the confusion matrix as seen above. The interesting question to ask is what do different stakeholders want from the classifier? The decision-maker (the judge or the prosecutor) asks: \u201cOf those that I have labeled high risk, how many recidivated?\u201d This corresponds to the model\u2019s predictive value = TP / (TP + FP). The defendant asks: \u201cWhat is the probability I\u2019ll be incorrectly classified as high risk?\u201d This corresponds to the model\u2019s false positive rate = FP / (FP + FN). The society at large might care about: \u201cIs the selected set demographically balanced?\u201d This could be demographic parity, which leads to the definition of group fairness (\u201cDo outcomes differ between groups, which we have no reason to believe are actually different?\u201d). A lot of these group fairness metrics have natural motivations, so there\u2019s not a single correct fairness definition. They depend on the politics of the situation. Let\u2019s forget about demographic parity and only pick the two most important metrics (false-positive rate and false-negative rate) while allowing the model to use protected class attributes. We fail the individual fairness definition, which uses a single threshold for the sentencing decision or the pre-sentencing release decision. Even if we pick one metric to optimize for, we still sacrifice some utility (providing public safety or releasing too few defendants). To build more intuition, you should play around with this interactive demo on attacking discrimination with smarter ML from Google Research. Finally, ML can be very good at finding patterns that maybe humans can\u2019t find. For instance, your ZIP code and age might be highly correlated with your race. That means the model can always pick up from a protected class attribute from other attributes. Read this paper on Equality of Opportunity in Supervised Learning for more detail.","title":"Fairness Definitions (From Aravind Narayanan\u2019s Lecture)"},{"location":"spring2021/lecture-9/#tradeoffs","text":"There are tradeoffs between different measures of group fairness, between the definitions of group fairness and individual fairness, and between the notions of fairness and utility. In fact, these tradeoffs are not specific to machine learning. They apply to human decision making too. There is also a tension between disparate treatment and disparate impact, which is another deep subject.","title":"Tradeoffs"},{"location":"spring2021/lecture-9/#seeing-the-water","text":"In order to see the water, it would be noteworthy to think about the differences between environmental equity and environmental justice : Equality: The assumption is that everyone benefits from the same supports. This is equal treatment. Equity: Everyone gets the support they need (\u201caffirmative action\u201d), thus producing equity. Justice: All parties are supported equally because the cause of the inequity was addressed. The systematic barrier has been removed. The justice mindset is valuable to have. As computer scientists, we have very literal minds and argue for the rationality of our choices. But taking a step back and seeing the whole situation would be even more crucial.","title":"Seeing The Water"},{"location":"spring2021/lecture-9/#5-representation","text":"","title":"5 - Representation"},{"location":"spring2021/lecture-9/#the-problem","text":"Watch this simple video : a hand sanitizer dispenser that doesn\u2019t recognize racially diverse hands. It\u2019s a small example but illustrates a big problem: a lack of attention to diverse representation in the development of technology products . This occurs across fields, such as drug development, photography, etc. As pointed out by Timnit Gebru in this New York Times article , the exclusion of people from certain backgrounds poses a serious long-term threat to the viability of ML systems. One way to address this challenge head-on is to focus on the inclusion of people from all backgrounds . Groups like Black in AI , Women in Machine Learning , and Latinx in AI play a big role in building communities of underrepresented people and inviting them into the AI/ML industry. Another is to deliberately ensure products reflect inclusive values . For example, Google Images now yields a diverse set of images for the search term \u201cCEO\u201d whereas it used to return entirely white, middle-aged men.","title":"The Problem"},{"location":"spring2021/lecture-9/#word-embeddings","text":"A particularly relevant example of bias in machine learning is the underlying bias in the Word2Vec model. Word2Vec introduced vector math for word embeddings and is frequently used for NLP applications. The original model was trained on a large corpus, and the weights were open-sourced. As these weights were examined, underlying bias in the word logic was discovered. Terms like \u201cdoctor\u201d and \u201cprogrammer\u201d were associated with men, while \u201chomemaker\u201d and \u201cnurse\u201d were associated with women. Translating our existing biases like these into the ML domain is undesirable, to say the least! \ud83d\ude29 One potential solution to address this problem is to de-bias at training time with carefully screened data . With newer models like GPT-3 that are trained on massive swathes of data, this can be hard to do in practice. Bender and Gebru advise in a 2021 paper to reduce the dependence on large, unfiltered datasets and more carefully document the data-generating process. Alternatively, you can alert the user proactively of potential bias. Addressing this problem of bias in language models is an open problem.","title":"Word Embeddings"},{"location":"spring2021/lecture-9/#seeing-the-water_1","text":"Part of the challenge lies in agreeing on whether the model should learn about the world as it is in the data or learn about the world in a more idealistic manner . This is application-specific. A model recognizing hate speech on Facebook should probably learn about the world as it is, or a model interacting with humans\u2019 conversations should adhere to proper ideals. Of course, this begs the question of who decides what ideals are desirable and suitable for a model to follow. Consider these questions as you build models for various applications. Ultimately, these challenges in machine learning systems development are rooted in ethics. Face recognition is a boundary-breaking area that has been grappling with ethical concerns . Importantly, face recognition illustrates how technology can impact ethics and change standards. Is the loss of privacy associated with face recognition desirable? Relatedly, are face recognition systems performing well across groups? The question of performance should generally follow ethics to avoid distracting from the fundamental ethical issues (e.g., civil rights, privacy, etc.).","title":"Seeing The Water"},{"location":"spring2021/lecture-9/#6-best-practices","text":"A recent survey of ML practitioners found these to be the top challenges in ensuring fairness that they face: Receiving support in fairness-aware data collection and curation Overcoming team\u2019s blind spots Implementing more proactive fairness auditing processes Auditing complex ML systems Deciding how to address particular instances of unfairness Addressing biases in the humans embedded throughout the ML development pipeline","title":"6 - Best Practices"},{"location":"spring2021/lecture-9/#suggestions","text":"Rachel Thomas, the co-creator of Fast.ai, has some great ideas on how to confront fairness issues proactively : Perform ethical risk sweeping . Akin to cybersecurity penetration testing, where engineers intentionally try to find faults, you can try to engage in regular fairness checks on behalf of different stakeholders. Expand the ethical circle. Try to consider different perspectives than yours regularly, and invite such people into your decision-making \u201ccircle\u201d to ensure that systems do not lead to unfair outcomes. Think about worst-case scenarios. What incentives may crop up for people to engage in unethical behavior? For example, the upvote-downvote system and recommendations on Reddit can cause toxic behavior. Think about such incentives and requisite safeguards in advance. Close the loop! You have to put in place a process to keep improving, as fairness is not a static test (just like raw performance). One powerful tool, proposed by Gebru and Mitchell in 2018, is adopting \u201c model cards .\u201d For every ML model, make a simple page that discusses the expectations (i.e., input/output), tradeoffs, performance, and known limitations. Engaging in this documentation exercise allows for teams to confront fairness issues head-on more effectively. The objective here is to get everyone on the same page about what the model can and cannot do from a fairness perspective . We believe everyone should do this, considering how easy it is. Other methods like bias audits are also useful, as the Aequitas team at UChicago shows.","title":"Suggestions"},{"location":"spring2021/lecture-9/#a-code-of-ethics","text":"AI is a reflection of society. It\u2019s impossible to expect AI to be completely unbiased when humans still struggle with the problem. However, we can try our best to ensure that these biases are not amplified by AI and mitigate any such damage. Making fairness and ethics a routine part of AI development by professionals and teams is crucial to addressing the challenge. Perhaps an AI code of ethics (akin to the Hippocratic Oath) would make sense!","title":"A Code of Ethics?"},{"location":"spring2021/lecture-9/#7-where-to-learn-more","text":"Here are some links to learn more: https://ethics.fast.ai/ : a course by the fast.ai team on practical data ethics consisting of 6 lectures. CS 294: Fairness in Machine Learning : A graduate course (similar to FSDL) taught at Berkeley in 2017 about AI ethics. Fair ML Book : A book being written by the instructor of the aforementioned course on fair ML. KDD Tutorial on Fair ML : Taught by folks from CMU, this is a workshop addressing some of the topics in this lecture. The Alignment Problem : a book that confronts present-day issues in AI alignment. Weapons of Math Destruction : a popular book about current issues like Facebook\u2019s News Feed.","title":"7 - Where To Learn More"},{"location":"spring2021/notebook-1/","text":"Notebook: Coding a neural net Video In this video, we code a neural network from scratch. You'll get familiar with the Google Colab environment, create a simple linear regression model using only Numpy, and build a multi-layer perception regression model using NumPy, PyTorch, and Keras. 0:30\u200b - Colab Notebook 101 5:30\u200b - Numerical computing via NumPy 10:15\u200b - Plotting via Matplotlib 11:33\u200b - Basic regression with a linear model 24:30\u200b - Basic regression with a multi-layer perceptron Follow Along Google Colab","title":"Notebook: Coding a neural net"},{"location":"spring2021/notebook-1/#notebook-coding-a-neural-net","text":"","title":"Notebook: Coding a neural net"},{"location":"spring2021/notebook-1/#video","text":"In this video, we code a neural network from scratch. You'll get familiar with the Google Colab environment, create a simple linear regression model using only Numpy, and build a multi-layer perception regression model using NumPy, PyTorch, and Keras. 0:30\u200b - Colab Notebook 101 5:30\u200b - Numerical computing via NumPy 10:15\u200b - Plotting via Matplotlib 11:33\u200b - Basic regression with a linear model 24:30\u200b - Basic regression with a multi-layer perceptron","title":"Video"},{"location":"spring2021/notebook-1/#follow-along","text":"Google Colab","title":"Follow Along"},{"location":"spring2021/panel/","text":"Panel Discussion: Do I need a PhD to work in ML? We gathered a few people to offer different perspectives on whether grad school is required to work on interesting ML problems. The panelists: Pieter Abbeel - Professor at UC Berkeley Georgia Gkioxari - Research Scientist at Facebook, formerly PhD Berkeley Peter Gao - Co-founder and CEO of Aquarium Learning, formerly Cruise Automation Anil Jason - Co-foudner and CTO of Quillbot Video","title":"Panel Discussion: Do I need a PhD to work in ML?"},{"location":"spring2021/panel/#panel-discussion-do-i-need-a-phd-to-work-in-ml","text":"We gathered a few people to offer different perspectives on whether grad school is required to work on interesting ML problems. The panelists: Pieter Abbeel - Professor at UC Berkeley Georgia Gkioxari - Research Scientist at Facebook, formerly PhD Berkeley Peter Gao - Co-founder and CEO of Aquarium Learning, formerly Cruise Automation Anil Jason - Co-foudner and CTO of Quillbot","title":"Panel Discussion: Do I need a PhD to work in ML?"},{"location":"spring2021/panel/#video","text":"","title":"Video"},{"location":"spring2021/projects/","text":"","title":"Projects"},{"location":"spring2021/synchronous/","text":"Synchronous Online Course For those of you who signed up, in addition to the lecture and lab materials released publicly, there are some major extras: Slack workspace for learners, instructors, and teaching assistants Weekly graded assignment Capstone project reviewed by peers and staff Certificate of completion How do I know if I am in this course? If you registered and received an email receipt from Stripe, you're in, and should have been added to our Slack workspace on February 1. Please email us if you have a Stripe receipt but aren't in our Slack. Teaching Assistants This course is only possible with the support of our amazing TAs: Head TA James Le runs Data Relations for Superb AI and contributes to Data Journalism for Snorkel AI, after getting an MS in Recommendation Systems at RIT. Daniel Cooper is a machine learning engineer at QuantumWork, SaaS for recruiters. Han Lee is a Senior Data Scientist at WalletHub. Prior to that, he worked on various DS, MLE, and quant roles. Previously, he co-managed TEFQX . Nadia Ahmed is a machine learning researcher with The Frontier Development Lab and Trillium Technologies in remote sensing for severe weather and flood events. Andrew Mendez is a Senior Machine Learning Engineer at Clarifai, developing large scale computer vision and machine learning systems for the public sector. Previously he was a ML Engineer at CACI. Vishnu Rachakonda is a Machine Learning Engineer at Tesseract Health, a retinal imaging company, where he builds machine learning models for workflow augmentation and diagnostics in on-device and cloud use cases. Chester Chen is the Director of Data Science Engineering at GoPro. He also founded the SF Big Analytics Meetup. Schedule While we post lectures once a week starting February 1, the first four weeks are review lectures -- stuff you should already know from other courses. On March 1, we get to the Full Stack content, and you will begin doing weekly assignments, discussing in Slack, and thinking about their course project. Logistics All learners, instructors, and TAs will be part of a Slack workspace. The Slack community is a crucial part of the course: a place to meet each other, post helpful links, share experiences, ask and answer questions. On Monday, we post the lecture and lab videos for you to watch. Post questions, ideas, articles in Slack as you view the materials. On Thursday, we go live on Zoom to discuss the posted questions and ideas. We have two 30-min slots: 9am and 6pm Pacific Time. We will send everyone a Google Calendar invite with the Zoom meeting information. You have until Friday night to finish the assignment via Gradescope, which will be graded by next Tuesday, so that you have prompt feedback. Labs are not graded and can be done on your own. Projects The final project is the most important as well as the most fun part of the course. You can pair up or work individually. The project can involve any part of the full stack of deep learning, and should take you roughly 40 hours per person, over 5 weeks. Projects will be presented as five-minute videos and associated reports, and open sourcing the code is highly encouraged. All projects will be posted for peer and staff review. The best projects will be awarded and publicized by Full Stack Deep Learning. If you want to find a partner, please post in the #spring2021-projects Slack channel with your idea or just that you're available to pair up. Project proposals are due on Gradescope a few weeks into the course. Please read more information about the projects . Certificate Those who complete the assignments and project will receive a certificate that can, for example, be displayed on LinkedIn. Time Commitment On average, expect to spend 5-10 hours per week on the course.","title":"Synchronous Online Course"},{"location":"spring2021/synchronous/#synchronous-online-course","text":"For those of you who signed up, in addition to the lecture and lab materials released publicly, there are some major extras: Slack workspace for learners, instructors, and teaching assistants Weekly graded assignment Capstone project reviewed by peers and staff Certificate of completion","title":"Synchronous Online Course"},{"location":"spring2021/synchronous/#how-do-i-know-if-i-am-in-this-course","text":"If you registered and received an email receipt from Stripe, you're in, and should have been added to our Slack workspace on February 1. Please email us if you have a Stripe receipt but aren't in our Slack.","title":"How do I know if I am in this course?"},{"location":"spring2021/synchronous/#teaching-assistants","text":"This course is only possible with the support of our amazing TAs: Head TA James Le runs Data Relations for Superb AI and contributes to Data Journalism for Snorkel AI, after getting an MS in Recommendation Systems at RIT. Daniel Cooper is a machine learning engineer at QuantumWork, SaaS for recruiters. Han Lee is a Senior Data Scientist at WalletHub. Prior to that, he worked on various DS, MLE, and quant roles. Previously, he co-managed TEFQX . Nadia Ahmed is a machine learning researcher with The Frontier Development Lab and Trillium Technologies in remote sensing for severe weather and flood events. Andrew Mendez is a Senior Machine Learning Engineer at Clarifai, developing large scale computer vision and machine learning systems for the public sector. Previously he was a ML Engineer at CACI. Vishnu Rachakonda is a Machine Learning Engineer at Tesseract Health, a retinal imaging company, where he builds machine learning models for workflow augmentation and diagnostics in on-device and cloud use cases. Chester Chen is the Director of Data Science Engineering at GoPro. He also founded the SF Big Analytics Meetup.","title":"Teaching Assistants"},{"location":"spring2021/synchronous/#schedule","text":"While we post lectures once a week starting February 1, the first four weeks are review lectures -- stuff you should already know from other courses. On March 1, we get to the Full Stack content, and you will begin doing weekly assignments, discussing in Slack, and thinking about their course project.","title":"Schedule"},{"location":"spring2021/synchronous/#logistics","text":"All learners, instructors, and TAs will be part of a Slack workspace. The Slack community is a crucial part of the course: a place to meet each other, post helpful links, share experiences, ask and answer questions. On Monday, we post the lecture and lab videos for you to watch. Post questions, ideas, articles in Slack as you view the materials. On Thursday, we go live on Zoom to discuss the posted questions and ideas. We have two 30-min slots: 9am and 6pm Pacific Time. We will send everyone a Google Calendar invite with the Zoom meeting information. You have until Friday night to finish the assignment via Gradescope, which will be graded by next Tuesday, so that you have prompt feedback. Labs are not graded and can be done on your own.","title":"Logistics"},{"location":"spring2021/synchronous/#projects","text":"The final project is the most important as well as the most fun part of the course. You can pair up or work individually. The project can involve any part of the full stack of deep learning, and should take you roughly 40 hours per person, over 5 weeks. Projects will be presented as five-minute videos and associated reports, and open sourcing the code is highly encouraged. All projects will be posted for peer and staff review. The best projects will be awarded and publicized by Full Stack Deep Learning. If you want to find a partner, please post in the #spring2021-projects Slack channel with your idea or just that you're available to pair up. Project proposals are due on Gradescope a few weeks into the course. Please read more information about the projects .","title":"Projects"},{"location":"spring2021/synchronous/#certificate","text":"Those who complete the assignments and project will receive a certificate that can, for example, be displayed on LinkedIn.","title":"Certificate"},{"location":"spring2021/synchronous/#time-commitment","text":"On average, expect to spend 5-10 hours per week on the course.","title":"Time Commitment"}]}